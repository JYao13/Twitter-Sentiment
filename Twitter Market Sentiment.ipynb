{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from tweepy import Stream, OAuthHandler, API\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "json_tw = 'twitter.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter credentials and tokens\n",
    "\n",
    "with open(json_tw) as f:\n",
    "    json_creds = json.load(f)\n",
    "    \n",
    "ckey = json_creds['API Keys']['ckey']\n",
    "csecret = json_creds['API Keys']['csecret']\n",
    "\n",
    "atoken = json_creds['Access']['atoken']\n",
    "asecret = json_creds['Access']['asecret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHxPZJfRrmc5L84n0uEpqgKg3\n",
      "bMxnOFoccXDEGNUGdMSmLdZRB3i9uB5sRooe8UaRuiaQuvx8dE\n",
      "1247632081-kdOaYMBdciern0yjCLyMtOyzvU2J02wmQpOlj5H\n",
      "Xnaclpwqpl0cy8A62Ve9HYZDq4PWAtWpYxNqCx9qAtuQk\n"
     ]
    }
   ],
   "source": [
    "print(ckey)\n",
    "print(csecret)\n",
    "print(atoken)\n",
    "print(asecret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWantReadError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1839\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1840\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36m_raise_ssl_error\u001b[1;34m(self, ssl, result)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_ERROR_WANT_READ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1646\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mWantReadError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1647\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_ERROR_WANT_WRITE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWantReadError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-222-cd2852c71a03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtwitterStream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlistener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtwitterStream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"s&p\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, follow, track, is_async, locations, stall_warnings, languages, encoding, filter_level)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filter_level'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_level\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'delimited'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'length'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_async\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     def sitestream(self, follow, stall_warnings=False,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_start\u001b[1;34m(self, is_async)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                 \u001b[1;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_read_loop\u001b[1;34m(self, resp)\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m                 \u001b[0mstripped_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mline\u001b[0m \u001b[1;31m# line is sometimes None so we need to check here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstripped_line\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mread_line\u001b[1;34m(self, sep)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m                 if (\n\u001b[0;32m    509\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readinto_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_readinto_chunked\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_chunk_left\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal_bytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_get_chunk_left\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    552\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# toss the CRLF at the end of the chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m                 \u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_next_chunk_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_next_chunk_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_next_chunk_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;31m# Read the next chunk size from the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"chunk size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWantReadError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The read operation timed out\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36mwait_for_read\u001b[1;34m(sock, timeout)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msocket\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreadable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \"\"\"\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mwait_for_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36mselect_wait_for_socket\u001b[1;34m(sock, read, write, timeout)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# thing.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwcheck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0mrready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_retry_on_intr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrready\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mwready\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mxready\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36m_retry_on_intr\u001b[1;34m(fn, timeout)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Modern Python, that retries syscalls by default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_retry_on_intr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# listener class to stream tweets\n",
    "\n",
    "class TweetListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print(data.text)\n",
    "        return(True)\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        if status == 420:\n",
    "            return False\n",
    "\n",
    "auth = OAuthHandler(ckey, csecret)\n",
    "auth.set_access_token(atoken, asecret)\n",
    "\n",
    "tw_api = API(auth)\n",
    "\n",
    "twitterStream = Stream(auth=api.auth, listener=TweetListener())\n",
    "twitterStream.filter(track=[\"s&p\", \"equity\", \"markets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-06 16:51:54.205177\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import wordninja\n",
    "import re\n",
    "\n",
    "import logging\n",
    "import requests\n",
    "import os\n",
    "import kaggle\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "# Set log\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "json_log = 'kaggle.json'\n",
    "json_tw = 'twitter.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading kaggle dataset with personal credentials\n",
    "\n",
    "with open(json_log) as f:\n",
    "    json_keys = json.load(f)\n",
    "\n",
    "kaggle_user = json_keys['username']\n",
    "kaggle_key = json_keys['key']\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_user\n",
    "\n",
    "os.environ['KAGGLE_KEY'] = kaggle_key\n",
    "\n",
    "# download kaggle dataset\n",
    "\n",
    "#kg download -u 'myUsername' -p 'myPassword' -d DigitRecognizer -f test.csv\n",
    "!kaggle datasets download -d kazanova/sentiment140 --unzip --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Kaggle dataset - 1.6m tweets and sentiment polarity, 2009 '''\n",
    "\n",
    "filepath = 'C:\\\\Users\\\\jymas\\\\jupyternotebook\\\\Projects\\\\sentiment140.zip'\n",
    "directory = 'C:\\\\Users\\\\jymas\\\\jupyternotebook\\\\Projects'\n",
    "\n",
    "with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory)\n",
    "\n",
    "encoder = \"ISO-8859-1\"\n",
    "columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "df_training = pd.read_csv(filepath, encoding=encoder, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599995</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599996</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599997</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599998</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599999</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training['flag'].unique() # we should drop this, every sample has the same flag\n",
    "\n",
    "df_training.drop('flag', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    }
   ],
   "source": [
    "# splitting dates\n",
    "\n",
    "df_training['date'] = pd.to_datetime(df_training['date'])\n",
    "\n",
    "for i in ['Year', 'month', 'day']:\n",
    "    form = '%' + str(i[0])\n",
    "    df_training[i] = df_training['date'].dt.strftime(form).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "          ..\n",
       "1599995    4\n",
       "1599996    4\n",
       "1599997    4\n",
       "1599998    4\n",
       "1599999    4\n",
       "Name: target, Length: 1600000, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    800000\n",
      "0    800000\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nDistribution of positive and negative sentiments \\n\\n0 - negative sentiment\\n4 - positive sentiment\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# no tweets were labeled as neutral in our training set, analyzing distribution of pos/neg\n",
    "print(df_training.target.value_counts())\n",
    "\n",
    "target_count = Counter(df_training['target'])\n",
    "\n",
    "plt.bar(target_count.keys(), target_count.values())\n",
    "plt.title(\"Dataset Sentiment Distribution\")\n",
    "\n",
    "'''\n",
    "\n",
    "Distribution of positive and negative sentiments \n",
    "\n",
    "0 - negative sentiment\n",
    "4 - positive sentiment\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>Year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>target</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.261601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.163692</td>\n",
       "      <td>-0.086910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id</td>\n",
       "      <td>-0.261601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.846066</td>\n",
       "      <td>0.110995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Year</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>month</td>\n",
       "      <td>-0.163692</td>\n",
       "      <td>0.846066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.425614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>day</td>\n",
       "      <td>-0.086910</td>\n",
       "      <td>0.110995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.425614</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target        id  Year     month       day\n",
       "target  1.000000 -0.261601   NaN -0.163692 -0.086910\n",
       "id     -0.261601  1.000000   NaN  0.846066  0.110995\n",
       "Year         NaN       NaN   NaN       NaN       NaN\n",
       "month  -0.163692  0.846066   NaN  1.000000 -0.425614\n",
       "day    -0.086910  0.110995   NaN -0.425614  1.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Notes:\n",
    "\n",
    "- tweet date slightly negatively correlated with target sentiment, not useful enough for training \n",
    "- drop id and user for now, not strong enough correlation \n",
    "\n",
    "'''\n",
    "\n",
    "df_training.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\pandas\\core\\frame.py:4223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# train only on the text information\n",
    "\n",
    "df1 = df_training[['target', 'text']]\n",
    "df1.rename(columns={'target':'label', 'text':'feature'}, inplace=True) # rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599995</td>\n",
       "      <td>4</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599996</td>\n",
       "      <td>4</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599997</td>\n",
       "      <td>4</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599998</td>\n",
       "      <td>4</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599999</td>\n",
       "      <td>4</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            feature\n",
       "0            0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1            0  is upset that he can't update his Facebook by ...\n",
       "2            0  @Kenichan I dived many times for the ball. Man...\n",
       "3            0    my whole body feels itchy and like its on fire \n",
       "4            0  @nationwideclass no, it's not behaving at all....\n",
       "...        ...                                                ...\n",
       "1599995      4  Just woke up. Having no school is the best fee...\n",
       "1599996      4  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997      4  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998      4  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999      4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Using regex to parse and clean text features\n",
    "\n",
    "HASH:\n",
    "- extract all hashtags as a column: df1[hash]\n",
    "- individual hashtags sometimes used to express overall emotion of message:\n",
    "    - ex. #wow, #amazing, #fail\n",
    "\n",
    "CLEAN1:\n",
    "- dropping all retweets -> @****\n",
    "- dropping all links -> http:\\\\****\n",
    "- hashtag words are kept, but hashtag symbols are removed by CLEAN2\n",
    "    - hashtag words sometimes used as part of sentence\n",
    "        - ex. \n",
    "\n",
    "CLEAN2:\n",
    "- keep all letters \n",
    "\n",
    "'''\n",
    "\n",
    "HASH = '#(\\w+)' # hashtags\n",
    "CLEAN1 = '@(\\w+)' # retweets\n",
    "CLEAN2 = 'https?:\\S+|http?:\\S|www\\.\\S+' # links\n",
    "CLEAN3 = '[^A-Za-z0-9\\']+' # symbols\n",
    "\n",
    "# create separate feature for hashtags only\n",
    "\n",
    "df1['hash'] = df1['feature'].apply(lambda text: re.findall(r\"#(\\w+)\", text)) # extract hashtags first\n",
    "\n",
    "# function that replaces hashtag words with split words, in the feature column \n",
    "\n",
    "def replacehash(text):\n",
    "    hash_words = re.findall(r\"#(\\w+)\", text)\n",
    "    for word in hash_words:\n",
    "        text = text.replace(word, ' '.join(wordninja.split(word)))\n",
    "    \n",
    "    return text\n",
    "\n",
    "df1['feature'] = df1['feature'].apply(lambda text: replacehash(text))\n",
    "\n",
    "# applying CLEAN1 and CLEAN2 to remove all hashtags, retweets, links and symbols\n",
    "\n",
    "df1['feature'] = df1['feature'].apply(lambda text: re.sub(CLEAN1, ' ', text.lower()).strip())\n",
    "df1['feature'] = df1['feature'].apply(lambda text: re.sub(CLEAN2, ' ', text.lower()).strip())\n",
    "df1['feature'] = df1['feature'].apply(lambda text: re.sub(CLEAN3, ' ', text.lower()).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>awww that's a bummer you shoulda got david car...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>no it's not behaving at all i'm mad why am i h...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599995</td>\n",
       "      <td>4</td>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599996</td>\n",
       "      <td>4</td>\n",
       "      <td>thewdb com very cool to hear old walt interviews</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599997</td>\n",
       "      <td>4</td>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599998</td>\n",
       "      <td>4</td>\n",
       "      <td>happy 38th birthday to my boo of alll time tup...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599999</td>\n",
       "      <td>4</td>\n",
       "      <td>happy charity tuesday</td>\n",
       "      <td>[charitytuesday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            feature  \\\n",
       "0            0  awww that's a bummer you shoulda got david car...   \n",
       "1            0  is upset that he can't update his facebook by ...   \n",
       "2            0  i dived many times for the ball managed to sav...   \n",
       "3            0     my whole body feels itchy and like its on fire   \n",
       "4            0  no it's not behaving at all i'm mad why am i h...   \n",
       "...        ...                                                ...   \n",
       "1599995      4  just woke up having no school is the best feel...   \n",
       "1599996      4   thewdb com very cool to hear old walt interviews   \n",
       "1599997      4  are you ready for your mojo makeover ask me fo...   \n",
       "1599998      4  happy 38th birthday to my boo of alll time tup...   \n",
       "1599999      4                              happy charity tuesday   \n",
       "\n",
       "                     hash  \n",
       "0                      []  \n",
       "1                      []  \n",
       "2                      []  \n",
       "3                      []  \n",
       "4                      []  \n",
       "...                   ...  \n",
       "1599995                []  \n",
       "1599996                []  \n",
       "1599997                []  \n",
       "1599998                []  \n",
       "1599999  [charitytuesday]  \n",
       "\n",
       "[1600000 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using wordninja to split hashtag joined words\n",
    "ex. '#charitytuesday' -> ['charity', 'tuesday']\n",
    "\n",
    "- next steps, implementing wordninja in-house\n",
    "\n",
    "'''\n",
    "\n",
    "# splitwords takes a list of hashtag texts, uses wordninja to split each text into words, and joins all into a list\n",
    "\n",
    "def splitwords(row):\n",
    "    _list = []\n",
    "    if len(row) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        for i in row:\n",
    "            _list.append(wordninja.split(i))\n",
    "    return [item for sublist in _list for item in sublist] \n",
    "\n",
    "# applying splitwords onto our hash column\n",
    "\n",
    "df1['hash'] = df1['hash'].apply(lambda row: splitwords(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here', 'before', 'oprah']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.iloc[15818]['hash']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet, reader\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find features and create a featureset\n",
    "\n",
    "#top_words = list(all_words.keys())[:5000]\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# mapper from nltk.pos_tag tags to wordnet tags\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Find features takes a tweet and finds if each word in tweet can be found in top_words (top 5000 words from our training set)\n",
    "# returns both the token list of words and the frequency of each token as dict\n",
    "\n",
    "def preprocess(tweet):\n",
    "    \n",
    "    ''' for hash tweets (input is a list) '''\n",
    "    if type(tweet) == list:\n",
    "        tweet = ' '.join(tweet)\n",
    "        \n",
    "    ''' text lemmatization '''    \n",
    "    words = word_tokenize(tweet)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = pos_tag(words)\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in words]\n",
    "    \n",
    "#     freq = {}\n",
    "#     for w in words:\n",
    "#         freq[w] = (w in top_words)\n",
    "    \n",
    "    return words#, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# featuresets = [(find_features(rev), category) for (rev, category) in documents] # creating featuresets for aggregated all reviews\n",
    "\n",
    "df1['tokens'] = df1['feature'].apply(lambda row: preprocess(row)) \n",
    "#df1['hash_tokens'] = df1['hash'].apply(lambda row: preprocess(row))\n",
    "\n",
    "# df1['tokens'], df1['freq'] = features.apply(lambda row: row[0]), features.apply(lambda row: row[1])\n",
    "# df1['hash_tokens'], df1['hash_freq'] = hash_features.apply(lambda row: row[0]), hash_features.apply(lambda row: row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling our training set to save\n",
    "\n",
    "import pickle\n",
    "\n",
    "save_set = open('df1.pickle', 'wb') # wb - write as bytes\n",
    "pickle.dump(df1, save_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280000\n",
      "320000\n"
     ]
    }
   ],
   "source": [
    "# loading a pickled training set for use\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "load_set = open('df1.pickle', 'rb')\n",
    "df1 = pickle.load(load_set)\n",
    "load_set.close()\n",
    "\n",
    "# create training and testing sets\n",
    "\n",
    "train_size = 0.8 # 80% allocated to training\n",
    "\n",
    "df_train, df_test = train_test_split(df1, test_size = 1-train_size, random_state=42)\n",
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "\n",
    "# turn df_train['tokens'] into a list of list format\n",
    "\n",
    "train_set = [row for row in df_train['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ya',\n",
       "  'quot',\n",
       "  \"'d\",\n",
       "  'like',\n",
       "  'palm',\n",
       "  'pre',\n",
       "  'touchstone',\n",
       "  'charger',\n",
       "  'readynow',\n",
       "  'yes',\n",
       "  'sound',\n",
       "  'good',\n",
       "  'beer',\n",
       "  'ready',\n",
       "  \"'\",\n",
       "  'pre',\n",
       "  'launch'],\n",
       " ['felt', 'earthquake', 'afternoon', 'seem', 'epicenter'],\n",
       " ['ruffle', 'shirt', 'like', 'likey'],\n",
       " ['pretty',\n",
       "  'bad',\n",
       "  'night',\n",
       "  'crappy',\n",
       "  'morning',\n",
       "  'fml',\n",
       "  'buttface',\n",
       "  'didnt',\n",
       "  'say',\n",
       "  'could',\n",
       "  'go',\n",
       "  'work',\n",
       "  'today'],\n",
       " ['yeah', 'clear', 'view'],\n",
       " ['one', 'time', 'follow', 'fam', 'ff', 'welcome', 'friday', 'follow', '2'],\n",
       " ['u', 'rock', 'thanks', 'add', 'next', 'robcrotch', 'tm', 'video'],\n",
       " ['yes',\n",
       "  'please',\n",
       "  'meet',\n",
       "  'fiancee',\n",
       "  'ian',\n",
       "  \"'s\",\n",
       "  'party',\n",
       "  'day',\n",
       "  'btw',\n",
       "  \"'s\",\n",
       "  'sweet',\n",
       "  'congrats',\n",
       "  'guy'],\n",
       " ['gawd',\n",
       "  \"'ve\",\n",
       "  'finish',\n",
       "  'whole',\n",
       "  'bowl',\n",
       "  'even',\n",
       "  'post',\n",
       "  'previous',\n",
       "  'tweet',\n",
       "  'make',\n",
       "  '2nd',\n",
       "  'bowl'],\n",
       " ['awesome',\n",
       "  'game',\n",
       "  'short',\n",
       "  \"'ll\",\n",
       "  'finish',\n",
       "  'crave',\n",
       "  'god',\n",
       "  'war',\n",
       "  'action'],\n",
       " ['happy',\n",
       "  'mother',\n",
       "  'day',\n",
       "  'grow',\n",
       "  'n',\n",
       "  'happy',\n",
       "  'amp',\n",
       "  'lil',\n",
       "  'prego',\n",
       "  'smut',\n",
       "  'lol',\n",
       "  'play',\n",
       "  'everyone',\n",
       "  'love',\n",
       "  'happy',\n",
       "  'mother',\n",
       "  'day',\n",
       "  'amp',\n",
       "  'god',\n",
       "  'bless'],\n",
       " ['back', 'eating', 'dinner'],\n",
       " ['could',\n",
       "  'follow',\n",
       "  'chat',\n",
       "  'room',\n",
       "  'could',\n",
       "  'please',\n",
       "  'follow',\n",
       "  \"n't\",\n",
       "  'many',\n",
       "  'lol'],\n",
       " ['heck', 'way', 'height', 'want', 'trade'],\n",
       " ['tired', 'say', 'goodbye', 'chicago', 'steal', 'half'],\n",
       " ['12', 'mai', 'tai', 'inbound', 'suddenly', 'trouble', 'relax'],\n",
       " ['lol',\n",
       "  \"'m\",\n",
       "  'still',\n",
       "  'even',\n",
       "  'closeee',\n",
       "  'figure',\n",
       "  'ahhh',\n",
       "  'want',\n",
       "  'help',\n",
       "  'lol'],\n",
       " ['thank', 'support', 'maternal', 'health'],\n",
       " ['hell', 'yeah', 'starbucks', 'rule', 'lol', 'know', 'work'],\n",
       " ['pay', 'doctor', '50'],\n",
       " ['hahahah',\n",
       "  'mah',\n",
       "  'freakin',\n",
       "  'bad',\n",
       "  'final',\n",
       "  'week',\n",
       "  'yes',\n",
       "  \"fo'sho\",\n",
       "  \"'ll\",\n",
       "  'visit',\n",
       "  'like',\n",
       "  'foreal',\n",
       "  'new',\n",
       "  'prospect'],\n",
       " ['make', 'greeting', 'card', 'say', 'eff'],\n",
       " ['maybe',\n",
       "  'five',\n",
       "  'month',\n",
       "  'nobody',\n",
       "  'die',\n",
       "  \"'s\",\n",
       "  'treatable',\n",
       "  'unlike',\n",
       "  'flu',\n",
       "  'guess',\n",
       "  \"n't\",\n",
       "  'mean',\n",
       "  \"n't\",\n",
       "  'care'],\n",
       " ['daughter', 'hook', 'sims', 'buy', '3', 'day', 'come'],\n",
       " ['love', 'boyfriend'],\n",
       " ['alone', 'friday', 'night'],\n",
       " ['temp', '102f'],\n",
       " ['dunno', 'dunno', 'god', 'love', 'shaun'],\n",
       " ['dont', 'time', 'shit', 'anymore'],\n",
       " ['spx', '920', 'floor'],\n",
       " ['thats', 'iv', \"'\", 'e', 'fun'],\n",
       " ['move', 'la', 'already', 'please'],\n",
       " ['tired', \"'m\", 'go', 'back', 'sleep', 'iv', 'get', 'work', 'tonite'],\n",
       " ['come',\n",
       "  'u',\n",
       "  'didnt',\n",
       "  'make',\n",
       "  'apperance',\n",
       "  'yesterday',\n",
       "  'webcast',\n",
       "  'tisk',\n",
       "  'tisk'],\n",
       " ['remember', 'suppose', 'wear', 'hotter', 'dress', 'bride'],\n",
       " ['congrats', 'andy', 'bet', \"'re\", 'great', 'day', 'worry', 'go'],\n",
       " ['love', 'song', 'sang', 'american', 'idol'],\n",
       " ['make',\n",
       "  '1st',\n",
       "  'break',\n",
       "  'fell',\n",
       "  'bit',\n",
       "  'near',\n",
       "  'end',\n",
       "  '2nd',\n",
       "  'level',\n",
       "  'bad',\n",
       "  'though',\n",
       "  '3550',\n",
       "  'chip'],\n",
       " ['actually',\n",
       "  \"'m\",\n",
       "  'hella',\n",
       "  'bore',\n",
       "  'friday',\n",
       "  \"'m\",\n",
       "  'totally',\n",
       "  'work',\n",
       "  'mode',\n",
       "  'let',\n",
       "  'know',\n",
       "  'check',\n",
       "  'schedule'],\n",
       " ['ethan', 'still', 'go', 'home'],\n",
       " ['piss', 'dont', 'want', 'get'],\n",
       " ['yea',\n",
       "  'bet',\n",
       "  'tell',\n",
       "  'u',\n",
       "  'want',\n",
       "  'shirt',\n",
       "  'holla',\n",
       "  'empress',\n",
       "  'empressccp',\n",
       "  'com'],\n",
       " ['watch',\n",
       "  'camp',\n",
       "  'rock',\n",
       "  'get',\n",
       "  'ta',\n",
       "  'find',\n",
       "  'joe',\n",
       "  'sang',\n",
       "  'last',\n",
       "  'night',\n",
       "  'sound',\n",
       "  'beautiful'],\n",
       " ['doesnt', 'friend', 'twitter'],\n",
       " ['back', 'work', 'today', 'bad', 'time'],\n",
       " [\"'s\",\n",
       "  'probably',\n",
       "  'gon',\n",
       "  'na',\n",
       "  'take',\n",
       "  '2',\n",
       "  'week',\n",
       "  'filter',\n",
       "  'daily',\n",
       "  'basis',\n",
       "  'lot',\n",
       "  'good',\n",
       "  'stuff',\n",
       "  '455',\n",
       "  'atm'],\n",
       " ['tweet', 'include', 'long', 'poem', 'world'],\n",
       " ['sry', 'yep', 'school', 'friend'],\n",
       " ['jonathan', 'donde', 'estas', 'bro', 'turn', 'twitter', 'junkie', 'likey'],\n",
       " ['need',\n",
       "  'scooby',\n",
       "  'snack',\n",
       "  \"'m\",\n",
       "  'sure',\n",
       "  'new',\n",
       "  'betablockers',\n",
       "  \"'m\",\n",
       "  'give',\n",
       "  'munchies'],\n",
       " ['back', 'work', 'tomorrow'],\n",
       " ['oh', 'dead', 'computer', \"'s\", 'good', 'poor', 'macbook', \"'m\", 'sad'],\n",
       " ['whats', 'everyone', 'holiday', 'rain', 'course'],\n",
       " ['wicked',\n",
       "  'long',\n",
       "  'weekend',\n",
       "  'miami',\n",
       "  \"n't\",\n",
       "  'sleep',\n",
       "  'head',\n",
       "  'airport',\n",
       "  'back',\n",
       "  'life',\n",
       "  'back',\n",
       "  'reality',\n",
       "  'thanks',\n",
       "  'bday',\n",
       "  'love'],\n",
       " ['saw', 'curragh', 'weekend', 'say', 'hello'],\n",
       " ['really', \"n't\", 'want', 'work', 'today'],\n",
       " ['awwwwwwwww', 'love', 'awesome', 'clown'],\n",
       " ['wtf',\n",
       "  'sarcastic',\n",
       "  'lol',\n",
       "  '771',\n",
       "  \"'s\",\n",
       "  'pretty',\n",
       "  'amazing',\n",
       "  'ill',\n",
       "  'catch',\n",
       "  'tonight',\n",
       "  '400',\n",
       "  'text',\n",
       "  'come'],\n",
       " ['good', 'morning', 'luvlies'],\n",
       " ['spent',\n",
       "  'non',\n",
       "  'productive',\n",
       "  'day',\n",
       "  'today',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'even',\n",
       "  'believe',\n",
       "  'ha',\n",
       "  'jus',\n",
       "  'bum',\n",
       "  'home',\n",
       "  'day',\n",
       "  'hope',\n",
       "  'dun',\n",
       "  'get',\n",
       "  'use'],\n",
       " ['twitter', 'slow', 'today', 'maintenance'],\n",
       " ['tgif', 'excite', 'see', 'everyone', \"'s\", 'grad', 'party', 'saturday'],\n",
       " ['finish',\n",
       "  'homework',\n",
       "  'move',\n",
       "  'onto',\n",
       "  'assignment',\n",
       "  'someone',\n",
       "  'please',\n",
       "  'tell',\n",
       "  \"'twibe\",\n",
       "  \"'\"],\n",
       " [\"'m\", 'feel', 'lonely', 'today'],\n",
       " ['heyy', 'wats', 'mite', 'comin', '2', 'florida', 'summer'],\n",
       " ['dance', 'puddle', 'like', 'little', 'kid', 'really', 'make', 'day', 'well'],\n",
       " ['baby', 'dog', 'ashlee'],\n",
       " ['24a3e9ee', \"'s\", 'bb', 'pin', 'anyone', 'want'],\n",
       " ['jodi', 'also', 'uncool', 'twitter', 'love'],\n",
       " ['sorry'],\n",
       " [\"'s\", 'occasion', 'leave', 'early', 'also', 'wan', 'na', 'leave'],\n",
       " ['somee', 'ppl', 'mean', 'sumtimes', 'im', 'best', 'time', 'ever', 'right'],\n",
       " ['record', 'audition'],\n",
       " ['make', 'think', \"'re\", 'easy', 'ha', 'ha', 'kidding'],\n",
       " [\"'m\", 'problem'],\n",
       " ['far', \"'m\", 'enjoy', 'new', 'alexisonfire', 'album'],\n",
       " ['sick',\n",
       "  'mac',\n",
       "  'cope',\n",
       "  'later',\n",
       "  'spending',\n",
       "  'time',\n",
       "  'faves',\n",
       "  'take',\n",
       "  'precedence'],\n",
       " [\"c'mon\",\n",
       "  'jon',\n",
       "  'u',\n",
       "  'cant',\n",
       "  'tell',\n",
       "  'u',\n",
       "  'stuff',\n",
       "  'like',\n",
       "  \"'possibly\",\n",
       "  'last',\n",
       "  'show',\n",
       "  \"'\",\n",
       "  'bwwwaaa'],\n",
       " ['yeah', 'send', 'info', \"n't\", 'mind', \"'re\", 'tweetup', 'monday'],\n",
       " ['sure',\n",
       "  'thing',\n",
       "  \"'ll\",\n",
       "  'make',\n",
       "  'sure',\n",
       "  'bring',\n",
       "  'extra',\n",
       "  'load',\n",
       "  'vegemite',\n",
       "  'haha',\n",
       "  'well',\n",
       "  'see',\n",
       "  'soon'],\n",
       " ['yea', 'im', 'tower', 'internet', 'anyways', 'r', 'u'],\n",
       " ['get',\n",
       "  'school',\n",
       "  'work',\n",
       "  'do',\n",
       "  'semester',\n",
       "  'ridiculously',\n",
       "  'short',\n",
       "  'oh',\n",
       "  'no',\n",
       "  'final'],\n",
       " ['sorry', 'feel', 'like', 'poop'],\n",
       " ['sick',\n",
       "  'today',\n",
       "  'plan',\n",
       "  'episode',\n",
       "  'radio',\n",
       "  'show',\n",
       "  'talkshoe',\n",
       "  'tomorrow',\n",
       "  '8',\n",
       "  'eastern',\n",
       "  'time'],\n",
       " ['cleaninggg', 'hit'],\n",
       " ['glad', 'one', 'true', 'great', 'wonderful', 'weekend'],\n",
       " ['happy',\n",
       "  'monday',\n",
       "  \"'m\",\n",
       "  'excited',\n",
       "  'week',\n",
       "  'get',\n",
       "  'warm',\n",
       "  'day',\n",
       "  'get',\n",
       "  'long'],\n",
       " ['miss', 'nana', 'grandad', 'l', 'urgh', 'listen', 'song', 'make', 'sad'],\n",
       " ['woof', 'woof', \"'s\", 'colder', 'today', 'walky'],\n",
       " ['thrift', 'shop', 'find', 'bracelet', 'watch'],\n",
       " ['lol', 'wrong', 'ahah', 'suppose', 'wrap', 'nasty', 'goo', 'thing'],\n",
       " ['wait',\n",
       "  'new',\n",
       "  'tnt',\n",
       "  'triathletes',\n",
       "  'get',\n",
       "  'store',\n",
       "  'brick',\n",
       "  'gon',\n",
       "  'na',\n",
       "  'teach',\n",
       "  'em',\n",
       "  'nutrition',\n",
       "  'value',\n",
       "  'endurolytes',\n",
       "  'fl'],\n",
       " ['que'],\n",
       " ['think', 'fail', 'exam', 'ugh', 'fml'],\n",
       " ['barn',\n",
       "  'shawn',\n",
       "  'throw',\n",
       "  'shoe',\n",
       "  'great',\n",
       "  'lesson',\n",
       "  'little',\n",
       "  'chilly',\n",
       "  'wear',\n",
       "  'jacket'],\n",
       " ['follow', 'twitter', 'still', 'happy', 'leave', 'facebook'],\n",
       " ['lindsey', 'know', 'cubby', 'hole', 'bed', 'open', 'get', 'home'],\n",
       " ['omg',\n",
       "  'rumble',\n",
       "  'thunder',\n",
       "  'make',\n",
       "  'ground',\n",
       "  'shake',\n",
       "  'freak',\n",
       "  'whole',\n",
       "  'house',\n",
       "  'shook'],\n",
       " ['want', 'give', 'phil', 'labonte', 'hug'],\n",
       " ['get',\n",
       "  '1k',\n",
       "  'last',\n",
       "  'week',\n",
       "  'get',\n",
       "  'list',\n",
       "  'top',\n",
       "  '25',\n",
       "  'ppl',\n",
       "  'follow',\n",
       "  'twitter'],\n",
       " [\"'re\", 'go', 'risk', 'car', 'size', 'pothole', 'let', 'adventure', 'begin'],\n",
       " ['playiing',\n",
       "  'card',\n",
       "  'wit',\n",
       "  'bebes',\n",
       "  'fam',\n",
       "  'libby',\n",
       "  'bday',\n",
       "  'party',\n",
       "  'yay',\n",
       "  'shes',\n",
       "  'three',\n",
       "  'muah',\n",
       "  'baby',\n",
       "  'come',\n",
       "  'back'],\n",
       " ['disappointed',\n",
       "  'find',\n",
       "  'triple',\n",
       "  'mocha',\n",
       "  'appear',\n",
       "  'actual',\n",
       "  'mocha',\n",
       "  'part'],\n",
       " ['haha',\n",
       "  'call',\n",
       "  'soda',\n",
       "  'pop',\n",
       "  'well',\n",
       "  \"'m\",\n",
       "  'actually',\n",
       "  'catch',\n",
       "  'call',\n",
       "  'soda'],\n",
       " ['aww',\n",
       "  \"'s\",\n",
       "  'unfair',\n",
       "  'well',\n",
       "  'always',\n",
       "  'offer',\n",
       "  'male',\n",
       "  'stripper',\n",
       "  'think',\n",
       "  \"'ll\",\n",
       "  'hide',\n",
       "  'gun',\n",
       "  'first'],\n",
       " ['square',\n",
       "  'space',\n",
       "  \"'m\",\n",
       "  'crazy',\n",
       "  'bout',\n",
       "  'precious',\n",
       "  'phone',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'afford'],\n",
       " [\"'ve\", 'work', '24', 'hour', 'weekend', 'simply', 'run', 'time'],\n",
       " ['dansk', 'folkeparti', 'get', 'approximately', '15', 'vote'],\n",
       " ['wheel', 'keep', 'turn'],\n",
       " [\"'re\",\n",
       "  'start',\n",
       "  'line',\n",
       "  '10',\n",
       "  'select',\n",
       "  'store',\n",
       "  'hopefully',\n",
       "  'somewhere',\n",
       "  'ger',\n",
       "  'list',\n",
       "  'sure',\n",
       "  'norfolk'],\n",
       " ['also',\n",
       "  'realize',\n",
       "  'digital',\n",
       "  'voice',\n",
       "  'recorder',\n",
       "  \"n't\",\n",
       "  'usb',\n",
       "  'port',\n",
       "  'think',\n",
       "  'pick',\n",
       "  'wrong',\n",
       "  'model'],\n",
       " ['thoroughly',\n",
       "  'feed',\n",
       "  'even',\n",
       "  'really',\n",
       "  'f',\n",
       "  'e',\n",
       "  'u',\n",
       "  'p',\n",
       "  'wish',\n",
       "  'stress',\n",
       "  'would',\n",
       "  'vanish',\n",
       "  'life'],\n",
       " ['rainy', 'day', 'well', 'today', 'tomorrow', 'though'],\n",
       " ['boyfriend',\n",
       "  'go',\n",
       "  'one',\n",
       "  'night',\n",
       "  'amp',\n",
       "  'get',\n",
       "  'call',\n",
       "  'ashton',\n",
       "  'cut',\n",
       "  'cher',\n",
       "  'dem',\n",
       "  'moore',\n",
       "  \"'s\",\n",
       "  'say',\n",
       "  'hilarious'],\n",
       " ['note',\n",
       "  'self',\n",
       "  'never',\n",
       "  'use',\n",
       "  \"'smooth\",\n",
       "  'away',\n",
       "  \"'\",\n",
       "  'unless',\n",
       "  \"'m\",\n",
       "  'sandpaper',\n",
       "  'buffing',\n",
       "  'look'],\n",
       " ['ok',\n",
       "  'last',\n",
       "  'time',\n",
       "  'twitter',\n",
       "  'leave',\n",
       "  'mate',\n",
       "  'house',\n",
       "  'c',\n",
       "  'ya',\n",
       "  'world',\n",
       "  'tweet',\n",
       "  'last',\n",
       "  'exam',\n",
       "  '2mozzeh',\n",
       "  'xo'],\n",
       " ['thanks', 'go', 'really', 'well', 'proud', 'self'],\n",
       " ['babysitting', 'baby', \"'s\", 'asleep', 'fun'],\n",
       " ['check', 'engine', 'light', 'since', 'drive', 'glendale', 'yesterday'],\n",
       " ['yuvraj',\n",
       "  'hit',\n",
       "  'hard',\n",
       "  'india',\n",
       "  'look',\n",
       "  'preety',\n",
       "  'good',\n",
       "  'think',\n",
       "  'need',\n",
       "  'worry',\n",
       "  'india',\n",
       "  'win'],\n",
       " ['fun', 'tweetup', 'epic'],\n",
       " ['proud', 'manny', 'pacquiao', 'amaze', 'fight'],\n",
       " ['place', 'blow', 'ubertwitter', \"n't\", 'function', 'switch', 'tb'],\n",
       " [],\n",
       " ['mighty',\n",
       "  'boosh',\n",
       "  'luv',\n",
       "  'um',\n",
       "  'haha',\n",
       "  'loave',\n",
       "  'load',\n",
       "  'diffrent',\n",
       "  'one',\n",
       "  'aswell'],\n",
       " ['ba', 'one', 'last', 'time', '2', 'update', 'status'],\n",
       " ['many', 'eft', 'practitioner', 'anyway'],\n",
       " ['last', 'day', 'holiday', 'wan', 'na', 'cry'],\n",
       " ['yay',\n",
       "  'im',\n",
       "  'bed',\n",
       "  'kinda',\n",
       "  'call',\n",
       "  'u',\n",
       "  'want',\n",
       "  'nite',\n",
       "  'ntie',\n",
       "  'sweet',\n",
       "  'dream',\n",
       "  'lt',\n",
       "  '33333',\n",
       "  'meant',\n",
       "  'say',\n",
       "  'earlier'],\n",
       " [\"'s\", 'bad', 'cycle', 'rain'],\n",
       " ['watch', 'crossroad', 'jessi'],\n",
       " ['growl',\n",
       "  'kind',\n",
       "  'notification',\n",
       "  'thing',\n",
       "  'someone',\n",
       "  'start',\n",
       "  'chat',\n",
       "  'notification',\n",
       "  'pop',\n",
       "  'screen',\n",
       "  \"'s\",\n",
       "  'fine',\n",
       "  'get'],\n",
       " ['usually', \"'s\", 'bad', 'would', \"n't\", 'bad', 'could', 'log', 'pc'],\n",
       " ['middle',\n",
       "  'finger',\n",
       "  'right',\n",
       "  'hand',\n",
       "  'hurt',\n",
       "  'right',\n",
       "  'small',\n",
       "  'accident',\n",
       "  'nail',\n",
       "  'clipper',\n",
       "  'lol',\n",
       "  'sister',\n",
       "  \"'s\",\n",
       "  'birthday',\n",
       "  'today'],\n",
       " ['congrats', 'ur', 'car'],\n",
       " ['ca',\n",
       "  \"n't\",\n",
       "  'believe',\n",
       "  'detroit',\n",
       "  'lose',\n",
       "  'series',\n",
       "  'tie',\n",
       "  'lose',\n",
       "  'actually',\n",
       "  'cry',\n",
       "  'well',\n",
       "  'wo',\n",
       "  \"n't\",\n",
       "  'know'],\n",
       " ['may',\n",
       "  'want',\n",
       "  'monitor',\n",
       "  'content',\n",
       "  'closely',\n",
       "  'go',\n",
       "  'full',\n",
       "  'feed',\n",
       "  'people',\n",
       "  'easy',\n",
       "  'access',\n",
       "  'steal'],\n",
       " ['iron', 'hubby', 'laundry', 'nunpung', 'langit', 'molor'],\n",
       " [\"'m\",\n",
       "  'tired',\n",
       "  'time',\n",
       "  'sleep',\n",
       "  'today',\n",
       "  \"'s\",\n",
       "  'cookbook',\n",
       "  \"'s\",\n",
       "  'b',\n",
       "  'day',\n",
       "  'wish',\n",
       "  'foo',\n",
       "  'happy',\n",
       "  'one'],\n",
       " ['home', 'wish', 'guy', 'friend', 'would', 'come', 'around', 'talk'],\n",
       " ['love'],\n",
       " ['top',\n",
       "  'miss',\n",
       "  'viejitos',\n",
       "  'wish',\n",
       "  'still',\n",
       "  'around',\n",
       "  'may',\n",
       "  'continue',\n",
       "  'r',\n",
       "  'p'],\n",
       " [\"'d\", 'leave', 'never', 'mind', 'wo', \"n't\", 'go'],\n",
       " ['neil',\n",
       "  'patrick',\n",
       "  'harris',\n",
       "  'gay',\n",
       "  'n',\n",
       "  \"n't\",\n",
       "  'anybody',\n",
       "  'tell',\n",
       "  'plaintive',\n",
       "  'sigh',\n",
       "  'hmm',\n",
       "  \"n't\",\n",
       "  'many',\n",
       "  'openly',\n",
       "  'gay',\n",
       "  'actor'],\n",
       " ['emacs',\n",
       "  'make',\n",
       "  'computer',\n",
       "  'slow',\n",
       "  'esc',\n",
       "  'meta',\n",
       "  'alt',\n",
       "  'ctrl',\n",
       "  'shift',\n",
       "  'never',\n",
       "  'recall'],\n",
       " ['stay', 'home', 'nothing', 'really', 'waste', 'time', 'miss', 'notnot'],\n",
       " ['ugh',\n",
       "  'im',\n",
       "  'jealous',\n",
       "  '3',\n",
       "  'day',\n",
       "  'weekend',\n",
       "  'next',\n",
       "  'week',\n",
       "  'get',\n",
       "  'take',\n",
       "  'away',\n",
       "  'since',\n",
       "  'go',\n",
       "  'funeral',\n",
       "  'much'],\n",
       " ['hate', 'time', 'month', \"'m\", 'uncomfortable', 'uncomfortable'],\n",
       " ['look',\n",
       "  'like',\n",
       "  'miss',\n",
       "  \"'ll\",\n",
       "  'catch',\n",
       "  'finish',\n",
       "  'mean',\n",
       "  'keep',\n",
       "  '4',\n",
       "  '5hr',\n",
       "  'pace'],\n",
       " ['haha', 'thanks', 'mean', 'great', 'show'],\n",
       " ['city',\n",
       "  'ridiculous',\n",
       "  'lol',\n",
       "  'jk',\n",
       "  'sewer',\n",
       "  'downtown',\n",
       "  'smell',\n",
       "  'like',\n",
       "  'death',\n",
       "  'cuz',\n",
       "  'heat',\n",
       "  'tho',\n",
       "  'hope',\n",
       "  'youve',\n",
       "  'well'],\n",
       " [\"'d\",\n",
       "  'like',\n",
       "  'use',\n",
       "  'clothes',\n",
       "  'photo',\n",
       "  'shoot',\n",
       "  'help',\n",
       "  'would',\n",
       "  'appreciate'],\n",
       " ['aaahhhhh', 'burn', 'face', 'cook', 'really', 'hurt'],\n",
       " ['sprinkle',\n",
       "  'maple',\n",
       "  'bar',\n",
       "  'donut',\n",
       "  'take',\n",
       "  'picture',\n",
       "  'make',\n",
       "  'hungry',\n",
       "  'nom'],\n",
       " ['room',\n",
       "  'yet',\n",
       "  'finger',\n",
       "  'freeze',\n",
       "  'type',\n",
       "  'cold',\n",
       "  'cold',\n",
       "  'cold',\n",
       "  'might',\n",
       "  'work',\n",
       "  'later',\n",
       "  'think'],\n",
       " ['iyaaaa', 'shit', \"'re\", 'gon', 'na', 'see', 'like', '2', 'week'],\n",
       " ['btw', 'love', 'hair', 'fork', 'pic'],\n",
       " ['hey', 'r', 'u', \"'s\", 'miss', 'u', '2'],\n",
       " ['thank', 'much'],\n",
       " ['thanks'],\n",
       " ['happy', 'tweet', 'mr', 'gruchacz', 'fuck', 'monkey'],\n",
       " ['jamba', 'juice', 'first', 'time', 'favorite', 'person'],\n",
       " ['rain', 'delhi', 'get', 'drench', 'last', 'moment'],\n",
       " ['left',\n",
       "  'sad',\n",
       "  'wait',\n",
       "  'mom',\n",
       "  'come',\n",
       "  'home',\n",
       "  'want',\n",
       "  'papa',\n",
       "  'john',\n",
       "  'dinner'],\n",
       " ['home', 'work', 'still', 'get', 'stupid', 'cold', 'cough'],\n",
       " ['screw', 'precal'],\n",
       " ['get', 'wisdom', 'teeth', 'minute'],\n",
       " ['watch', 'labyrinth'],\n",
       " [\"'s\", 'mean'],\n",
       " ['go', 'green', 'use', 'blacke'],\n",
       " ['oh', 'yes', 'blog', 'nottingham'],\n",
       " ['welcome'],\n",
       " ['shop',\n",
       "  'clean',\n",
       "  'bmfing',\n",
       "  'webcam',\n",
       "  'chat',\n",
       "  'nephew',\n",
       "  'nothing',\n",
       "  'spesh',\n",
       "  'good',\n",
       "  'bank',\n",
       "  'holiday',\n",
       "  'monday',\n",
       "  'nonetheless'],\n",
       " ['u',\n",
       "  'nevr',\n",
       "  'kno',\n",
       "  'h8',\n",
       "  'storm',\n",
       "  'amp',\n",
       "  'flyin',\n",
       "  'accident',\n",
       "  'happen',\n",
       "  'surprise',\n",
       "  'yer',\n",
       "  'hungry',\n",
       "  'lol'],\n",
       " ['bad', 'brain'],\n",
       " ['also', \"n't\", 'gotten', 'tweet', 'phone', \"'m\", 'confused'],\n",
       " ['boooo', \"n't\", 'call', \"n't\", 'write', \"n't\", 'send', 'flower'],\n",
       " ['finish', 'across', 'universe', 'beatles', 'style', 'playlist', 'song'],\n",
       " ['need',\n",
       "  'hoover',\n",
       "  'really',\n",
       "  'hurt',\n",
       "  'shoulder',\n",
       "  'wear',\n",
       "  'mr',\n",
       "  'messy',\n",
       "  'men',\n",
       "  \"'s\",\n",
       "  'shirt',\n",
       "  'today',\n",
       "  'design',\n",
       "  'man',\n",
       "  'whatever',\n",
       "  'mr',\n",
       "  'messy',\n",
       "  'pink'],\n",
       " [\"'s\", 'killer', 'whale', 'scene', 'happy', 'foot'],\n",
       " [\"n't\",\n",
       "  'worry',\n",
       "  \"'re\",\n",
       "  'alone',\n",
       "  'lot',\n",
       "  'u',\n",
       "  'one',\n",
       "  'point',\n",
       "  'live',\n",
       "  'pimple',\n",
       "  'scalp',\n",
       "  'cheer'],\n",
       " ['oh',\n",
       "  'god',\n",
       "  'sound',\n",
       "  'epic',\n",
       "  'sound',\n",
       "  'epic',\n",
       "  'think',\n",
       "  'name',\n",
       "  'replace',\n",
       "  'thermidor',\n",
       "  'restaurant'],\n",
       " ['best',\n",
       "  'friend',\n",
       "  'high',\n",
       "  'school',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'come',\n",
       "  'wed',\n",
       "  'bridal',\n",
       "  'party',\n",
       "  'anymore'],\n",
       " ['believe',\n",
       "  'everything',\n",
       "  'do',\n",
       "  'moderation',\n",
       "  'except',\n",
       "  'sex',\n",
       "  'drug',\n",
       "  'rock',\n",
       "  'roll',\n",
       "  'oh',\n",
       "  'ice',\n",
       "  'cream'],\n",
       " ['today',\n",
       "  'national',\n",
       "  'donut',\n",
       "  'day',\n",
       "  'wish',\n",
       "  \"'d\",\n",
       "  'know',\n",
       "  'earlier',\n",
       "  'could',\n",
       "  'grab',\n",
       "  'free',\n",
       "  'one'],\n",
       " ['good',\n",
       "  'morning',\n",
       "  'slowly',\n",
       "  'feel',\n",
       "  'like',\n",
       "  \"'m\",\n",
       "  'get',\n",
       "  'good',\n",
       "  'hope',\n",
       "  'today',\n",
       "  'continue',\n",
       "  'heal',\n",
       "  'trend'],\n",
       " ['hope', 'prgnosis', 'good'],\n",
       " ['ditch'],\n",
       " ['mention',\n",
       "  'really',\n",
       "  'painful',\n",
       "  'really',\n",
       "  \"'m\",\n",
       "  'pretty',\n",
       "  'sure',\n",
       "  \"'m\",\n",
       "  'sympathy',\n",
       "  'contraction'],\n",
       " ['hope', 'someday', 'come', 'tweetup', 'visit', 'saudi'],\n",
       " ['spammed', 'thanks'],\n",
       " ['miss', 'work', 'today'],\n",
       " ['nice',\n",
       "  'say',\n",
       "  'hello',\n",
       "  'unable',\n",
       "  'reply',\n",
       "  'via',\n",
       "  'blackberry',\n",
       "  'suck',\n",
       "  'maria',\n",
       "  'tony',\n",
       "  'jump',\n",
       "  'onboard'],\n",
       " ['boys', 'flower', 'love', 'oh', 'yes'],\n",
       " ['best',\n",
       "  'conversation',\n",
       "  'one',\n",
       "  'auto',\n",
       "  'rikshaw',\n",
       "  'driver',\n",
       "  'simple',\n",
       "  'joys',\n",
       "  'life'],\n",
       " ['eep', 'another', 'cold', 'day', 'sun', 'go', 'miss'],\n",
       " ['hate', 'older', 'graduate', 'sibling', 'finer', 'mom', 'bday', 'gift'],\n",
       " ['try',\n",
       "  'direct',\n",
       "  'message',\n",
       "  'trouble',\n",
       "  'name',\n",
       "  'catherine',\n",
       "  'di',\n",
       "  'cesare',\n",
       "  'pas',\n",
       "  'need',\n",
       "  'use',\n",
       "  'tomorrow'],\n",
       " ['head', 'home'],\n",
       " ['awwww'],\n",
       " ['exactly', 'one', 'really', 'need', 'skin', 'toe'],\n",
       " ['cant', 'make', 'tonight', 'maybe', 'weekend', 'sometime', \"'ll\", 'work'],\n",
       " ['become',\n",
       "  'obsess',\n",
       "  'strawberrys',\n",
       "  'sugar',\n",
       "  'also',\n",
       "  'school',\n",
       "  'day',\n",
       "  'however',\n",
       "  \"'m\",\n",
       "  'upset',\n",
       "  'school',\n",
       "  'day',\n",
       "  'dvd'],\n",
       " ['home', 'amp', 'homework'],\n",
       " ['feel',\n",
       "  'like',\n",
       "  \"'m\",\n",
       "  'give',\n",
       "  'silent',\n",
       "  'treatment',\n",
       "  \"'m\",\n",
       "  'even',\n",
       "  'quite',\n",
       "  'sure',\n",
       "  'wrong'],\n",
       " ['way',\n",
       "  'home',\n",
       "  'agree',\n",
       "  'kyle',\n",
       "  'sandilands',\n",
       "  'shithead',\n",
       "  'chaser',\n",
       "  'tonight',\n",
       "  'abc1'],\n",
       " ['amaze', 'cookout', 'fun', 'night'],\n",
       " [\"n't\", 'go', 'able', 'voice', 'recital'],\n",
       " ['sad'],\n",
       " ['seriously', 'u', 'make', 'bear', 'buy', 'lol'],\n",
       " ['say', 'gold', 'golf', 'mean', 'golf'],\n",
       " ['hot', 'chocolate', 'marshmallow'],\n",
       " ['go',\n",
       "  'wee',\n",
       "  'sunbed',\n",
       "  'see',\n",
       "  'get',\n",
       "  'little',\n",
       "  'colour',\n",
       "  'ill',\n",
       "  'plane',\n",
       "  'way',\n",
       "  'new',\n",
       "  'york',\n",
       "  'time',\n",
       "  'next',\n",
       "  'week',\n",
       "  'x'],\n",
       " ['yes', 'humility', 'interest', 'soon', 'well', 'lose', 'lt'],\n",
       " ['yesterday', 'punish', 'today', 'record', '4', 'follower', 'xd'],\n",
       " ['sort', 'book', 'movie', 'pack'],\n",
       " ['update',\n",
       "  'iphone',\n",
       "  'restore',\n",
       "  'backup',\n",
       "  'sync',\n",
       "  'apps',\n",
       "  'back',\n",
       "  'could',\n",
       "  'take',\n",
       "  'night'],\n",
       " ['ughh',\n",
       "  'always',\n",
       "  'try',\n",
       "  'look',\n",
       "  'like',\n",
       "  'damn',\n",
       "  'clown',\n",
       "  'afterwards',\n",
       "  'lmaoo',\n",
       "  \"'m\",\n",
       "  'horrible',\n",
       "  'girly',\n",
       "  'girly',\n",
       "  'crap'],\n",
       " ['prima', 'wish', 'u', 'p', 'didnt', 'know', 'tall', 'either', 'lol'],\n",
       " ['back',\n",
       "  'home',\n",
       "  'listening',\n",
       "  '2h',\n",
       "  'live',\n",
       "  'jimmy',\n",
       "  'wale',\n",
       "  'long',\n",
       "  'nice',\n",
       "  'walk',\n",
       "  'amp'],\n",
       " ['jesus',\n",
       "  'legs',\n",
       "  'hurt',\n",
       "  'like',\n",
       "  'hell',\n",
       "  'swam',\n",
       "  '5',\n",
       "  'mile',\n",
       "  'run',\n",
       "  '6',\n",
       "  'mile',\n",
       "  'right'],\n",
       " ['still', 'work'],\n",
       " ['last',\n",
       "  'sunday',\n",
       "  'pioneer',\n",
       "  \"'m\",\n",
       "  'go',\n",
       "  'miss',\n",
       "  'everyone',\n",
       "  'next',\n",
       "  'stop',\n",
       "  'kentucky'],\n",
       " ['look', 'back', 'really', 'hilarious', 'time', 'tho'],\n",
       " ['first',\n",
       "  'day',\n",
       "  'okay',\n",
       "  'learnt',\n",
       "  'take',\n",
       "  'blood',\n",
       "  'pressure',\n",
       "  'really',\n",
       "  'look',\n",
       "  'forward',\n",
       "  'practical',\n",
       "  'exam',\n",
       "  'one'],\n",
       " ['watch', 'poison', 'concert', 'vh1', 'classic'],\n",
       " ['thankyou', 'hun', 'buzzer', 'bit', 'great'],\n",
       " ['hahahaha', 'crack'],\n",
       " ['producer',\n",
       "  'neil',\n",
       "  'favorite',\n",
       "  'red',\n",
       "  'velvet',\n",
       "  'really',\n",
       "  'cupcakes',\n",
       "  'wonderful'],\n",
       " ['go', 'yumcha', 'ss2', 'friend'],\n",
       " ['w', 'baby', 'jrichhh', 'goodnight'],\n",
       " ['damn', 'spilted', 'noodle', 'juice', 'laptop', 'smell', 'nooodleeees'],\n",
       " ['take', 'noble', 'blood', 'work', 'poor', 'lil', 'dude', 'glad', \"'s\"],\n",
       " ['follow', 'chat'],\n",
       " ['problem', 'want', 'site', 'mob3', 'mbtv', 'font', 'tell'],\n",
       " ['love', 'mcfly'],\n",
       " ['hill', 'mtv', 'music', 'award', 'way', 'end', 'weekend'],\n",
       " ['wo',\n",
       "  \"n't\",\n",
       "  'able',\n",
       "  'attend',\n",
       "  'rock',\n",
       "  'bell',\n",
       "  'dc',\n",
       "  'year',\n",
       "  'get',\n",
       "  'new',\n",
       "  'car',\n",
       "  'really',\n",
       "  'set',\n",
       "  'back'],\n",
       " ['buy',\n",
       "  'album',\n",
       "  'love',\n",
       "  'favorite',\n",
       "  'song',\n",
       "  'speed',\n",
       "  'dial',\n",
       "  \"'s\",\n",
       "  'perfect',\n",
       "  'song',\n",
       "  'right'],\n",
       " ['sure', 'crash', 'phone'],\n",
       " ['baby', 'sunflower', 'get', 'blasted', 'rain', 'poor', 'creature'],\n",
       " ['tn',\n",
       "  'wake',\n",
       "  'hubby',\n",
       "  'range',\n",
       "  'late',\n",
       "  'range',\n",
       "  'yesterday',\n",
       "  'hes',\n",
       "  '5hrs',\n",
       "  'sleep',\n",
       "  'poor',\n",
       "  'baby'],\n",
       " ['hate', 'dream', 'work', 'wake', 'go', 'work'],\n",
       " [\"'s\", 'cause', 'worrying', 'something'],\n",
       " ['heaven', \"'s\", 'one', 'month', 'anniversary', 'miss', 'buddy'],\n",
       " ['sims', '3', 'release', 'could', 'download'],\n",
       " ['stay',\n",
       "  'peter',\n",
       "  \"'s\",\n",
       "  'tonight',\n",
       "  'time',\n",
       "  'go',\n",
       "  'hospital',\n",
       "  'mammogram',\n",
       "  'snuggle',\n",
       "  'keep',\n",
       "  'thought',\n",
       "  'amp',\n",
       "  'prayer'],\n",
       " ['unc', 'budget', 'cut', 'may', 'rise', '18'],\n",
       " ['superrrr',\n",
       "  'mamatay',\n",
       "  'nko',\n",
       "  'sa',\n",
       "  'mga',\n",
       "  'assignment',\n",
       "  'shet',\n",
       "  'tpos',\n",
       "  'boardwork',\n",
       "  'p',\n",
       "  'sa',\n",
       "  'math',\n",
       "  'handbook',\n",
       "  'test',\n",
       "  'waaaa',\n",
       "  'shetingness',\n",
       "  'naman',\n",
       "  'haha'],\n",
       " ['stink', 'steres', 'storm', 'come', 'eek'],\n",
       " ['excite', 'see'],\n",
       " ['fresh',\n",
       "  'outta',\n",
       "  'showah',\n",
       "  'calve',\n",
       "  'kill',\n",
       "  'fuck',\n",
       "  'exercise',\n",
       "  'haha',\n",
       "  'fair',\n",
       "  'tonightttt'],\n",
       " ['wooooop', 'lakers', 'doin', \"'\", 'work', 'alst', 'night', 'three', 'win'],\n",
       " ['great', 'sunday', 'say', 'hello', 'tarumorales'],\n",
       " ['christian', \"'s\"],\n",
       " ['katie',\n",
       "  'talk',\n",
       "  'dad',\n",
       "  'muse',\n",
       "  'likely',\n",
       "  'tomorrow',\n",
       "  'he',\n",
       "  'still',\n",
       "  'home',\n",
       "  'discus',\n",
       "  'tomorrow',\n",
       "  'night'],\n",
       " ['fitful',\n",
       "  'sleep',\n",
       "  'officially',\n",
       "  'ill',\n",
       "  'impressed',\n",
       "  'want',\n",
       "  'go',\n",
       "  'shopping'],\n",
       " ['lol', 'use', 'kleenex', 'tissue', 'ahahah'],\n",
       " ['fresh', 'metal', 'come'],\n",
       " [\"'s\", 'reasonable', 'excuse', \"'ll\", 'forgive'],\n",
       " ['omg', 'tallked', 'phone', 'sweet', 'thanks', 'answeringg', 'love', 'youu'],\n",
       " ['haha', 'gay', 'lol', 'wuu2', 'today', \"'ve\", 'beach'],\n",
       " ['post', 'go', 'disappear', 'even', 'dashboard'],\n",
       " ['oh', 'meant', 'like', 'folk', 'oz', 'etc', 'miss', 'rest', \"'em\"],\n",
       " ['congratulation', 'drive', 'thing', 'many', 'adventure'],\n",
       " ['cant', 'aww', 'thats', 'nice', 'know'],\n",
       " ['guy', 'win', 'know', 'um', 'quot', 'beck', 'quot', 'stand'],\n",
       " ['watch', 'hangover', 'movie', 'high', 'expectation', 'live'],\n",
       " ['happy', 'birthday'],\n",
       " ['good',\n",
       "  'point',\n",
       "  'saw',\n",
       "  'six',\n",
       "  'day',\n",
       "  'ago',\n",
       "  'look',\n",
       "  'added',\n",
       "  'video',\n",
       "  'itunes',\n",
       "  'cheyeah',\n",
       "  'bo'],\n",
       " ['thank',\n",
       "  'u',\n",
       "  'much',\n",
       "  '4',\n",
       "  'follow',\n",
       "  'twttr',\n",
       "  'hope',\n",
       "  'u',\n",
       "  'find',\n",
       "  'excite',\n",
       "  'look',\n",
       "  'forward',\n",
       "  '2',\n",
       "  'yr',\n",
       "  'tweet'],\n",
       " ['airtel',\n",
       "  'sell',\n",
       "  'unlocked',\n",
       "  'iphones',\n",
       "  'bet',\n",
       "  'price',\n",
       "  'gt',\n",
       "  '55000',\n",
       "  'r',\n",
       "  'guess',\n",
       "  'lower',\n",
       "  'price',\n",
       "  'macbook',\n",
       "  'would',\n",
       "  'cost'],\n",
       " ['oh',\n",
       "  'men',\n",
       "  \"'s\",\n",
       "  'beautiful',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'communicate',\n",
       "  'becaause',\n",
       "  'cellphone'],\n",
       " ['tummy', 'hurt'],\n",
       " ['hmmmm', 'especially', 'impressed'],\n",
       " ['hahha', 'really', 'guess', 'wat', 'im', 'still'],\n",
       " ['music', 'monday', 'bullet', 'valentine', 'wake', 'demon'],\n",
       " ['girlfriend', \"n't\", 'answer', 'phone'],\n",
       " ['inside',\n",
       "  'dive',\n",
       "  'chocolate',\n",
       "  'wrapper',\n",
       "  'quot',\n",
       "  'chocolate',\n",
       "  'therapy',\n",
       "  'oh',\n",
       "  'good',\n",
       "  'quot',\n",
       "  'yup'],\n",
       " ['gah',\n",
       "  'eye',\n",
       "  'get',\n",
       "  'swimmy',\n",
       "  'gt',\n",
       "  'well',\n",
       "  'go',\n",
       "  'see',\n",
       "  'head',\n",
       "  'pas',\n",
       "  'turn',\n",
       "  'migraine'],\n",
       " ['meet', 'prof', 'spenta', 'wadia', 'get', 'enough', 'work', 'day'],\n",
       " ['thing',\n",
       "  'love',\n",
       "  'twitter',\n",
       "  '1',\n",
       "  \"'s\",\n",
       "  'entertain',\n",
       "  'undistracting',\n",
       "  'enough',\n",
       "  'open',\n",
       "  'study'],\n",
       " ['prob', 'til', 'next', 'week', \"'m\", 'damn', 'busy'],\n",
       " ['disappointed',\n",
       "  'great',\n",
       "  'depth',\n",
       "  'unbothered',\n",
       "  'perceived',\n",
       "  'need',\n",
       "  'happy',\n",
       "  'ending'],\n",
       " ['love',\n",
       "  'rollerblade',\n",
       "  'refresh',\n",
       "  'wish',\n",
       "  'sibling',\n",
       "  'think',\n",
       "  'might',\n",
       "  'often'],\n",
       " ['one', 'mad', 'lone', 'palm', 'tree'],\n",
       " ['dangg', 'back', 'school', 'today'],\n",
       " ['viola', 'new', 'layout', 'theme'],\n",
       " ['phoneee', 'fer', 'age', 'ily', 'boy', 'he', 'rant', 'butterfly', 'hahahaa'],\n",
       " ['kinda',\n",
       "  'confuse',\n",
       "  'like',\n",
       "  \"'m\",\n",
       "  'really',\n",
       "  'feelin',\n",
       "  '3',\n",
       "  '0',\n",
       "  'update',\n",
       "  'tho'],\n",
       " ['ah', 'good', 'nap', 'want', 'go', 'work'],\n",
       " ['guess', 'know', 'compound', 'owning', 'radio', 'host', 'listen'],\n",
       " ['ty', 'ty', 'look', 'ridiculous'],\n",
       " ['muack', 'muack', 'muack', 'muack', 'muack', 'muack', 'muack'],\n",
       " ['two', 'double', 'phillies', 'west', 'coast', 'game', 'run', 'yet'],\n",
       " ['wish', 'outside', 'radio', 'one'],\n",
       " ['hate', 'people', 'steal'],\n",
       " [\"n't\", 'expect', 'grow', \"'s\", 'window', 'soaking', 'sun', 'right'],\n",
       " ['im', 'screw'],\n",
       " ['buy',\n",
       "  'maxi',\n",
       "  'dress',\n",
       "  'need',\n",
       "  'loose',\n",
       "  'like',\n",
       "  '6',\n",
       "  'inch',\n",
       "  'drag',\n",
       "  'like',\n",
       "  'wet',\n",
       "  'dress'],\n",
       " ['anyone',\n",
       "  'know',\n",
       "  'review',\n",
       "  'dealer',\n",
       "  'tony',\n",
       "  'royden',\n",
       "  'would',\n",
       "  'like',\n",
       "  'know',\n",
       "  'would',\n",
       "  'thank'],\n",
       " ['wish', 'marijuana', 'legal', 'lmao', 'seriously'],\n",
       " ['ant', 'house', 'planning', 'vacay'],\n",
       " ['yeah', 'annoy', 'deserves', 'sooo', 'much'],\n",
       " ['must', 'difficult', 'watch'],\n",
       " ['happen',\n",
       "  'store',\n",
       "  'corner',\n",
       "  'mitsuwa',\n",
       "  'jk',\n",
       "  'broadway',\n",
       "  'mirror',\n",
       "  'want',\n",
       "  'new',\n",
       "  'one'],\n",
       " ['first', 'day', 'college', 'beat'],\n",
       " ['start', 'day', 'feel', 'little', 'sick', 'stomach', 'though', 'ick'],\n",
       " ['hellooo', 'thankyou', 'add', 'sppreciate'],\n",
       " ['bbq', 'work', 'tomorrow'],\n",
       " ['crap', 'bummer', 'say'],\n",
       " ['get', 'art', 'supply', 'ca', \"n't\", 'find', 'black', 'sketch', 'book'],\n",
       " ['stud',\n",
       "  'oral',\n",
       "  'radiology',\n",
       "  'exam',\n",
       "  'tough',\n",
       "  'subject',\n",
       "  'ever',\n",
       "  'wish',\n",
       "  'luck',\n",
       "  'nervous'],\n",
       " ['let', 'u', 'pray', 'passenger', 'miss', 'air', 'france', 'jet'],\n",
       " ['come',\n",
       "  'back',\n",
       "  'cousin',\n",
       "  \"'s\",\n",
       "  'wedding',\n",
       "  'da',\n",
       "  'chain',\n",
       "  'oh',\n",
       "  'btw',\n",
       "  'get',\n",
       "  'key',\n",
       "  'car',\n",
       "  'let',\n",
       "  'say',\n",
       "  'break'],\n",
       " [\"'ve\",\n",
       "  'mia',\n",
       "  'due',\n",
       "  'problemas',\n",
       "  'con',\n",
       "  'mi',\n",
       "  'familia',\n",
       "  'still',\n",
       "  'love',\n",
       "  'guy'],\n",
       " ['beer', 'man', 'mac', 'could', 'girl', 'ask', 'sound', 'great'],\n",
       " ['take', 'girl', 'trip', 'sometime'],\n",
       " ['im', 'addict', 'chocolate', 'pure', 'rush', 'omg', 'good'],\n",
       " ['whooo', 'come'],\n",
       " ['well',\n",
       "  'thats',\n",
       "  'good',\n",
       "  'yea',\n",
       "  'deff',\n",
       "  'good',\n",
       "  'way',\n",
       "  'express',\n",
       "  'didnt',\n",
       "  'go',\n",
       "  'prom',\n",
       "  'year',\n",
       "  'work'],\n",
       " ['amuse',\n",
       "  'little',\n",
       "  'musical',\n",
       "  'joke',\n",
       "  'quot',\n",
       "  'obsolete',\n",
       "  'quot',\n",
       "  'listen',\n",
       "  'old',\n",
       "  'computer',\n",
       "  'modern',\n",
       "  'like',\n",
       "  'ipod'],\n",
       " ['buda',\n",
       "  'kyle',\n",
       "  'glad',\n",
       "  \"n't\",\n",
       "  'leave',\n",
       "  '1',\n",
       "  '5',\n",
       "  'hour',\n",
       "  'advance',\n",
       "  'anymore',\n",
       "  '45',\n",
       "  'min'],\n",
       " ['never', 'let', 'drink', 'slushie', 'ever', 'wake', 'sick', 'hell', 'kill'],\n",
       " ['yey',\n",
       "  'finally',\n",
       "  'teeth',\n",
       "  'pull',\n",
       "  \"'s\",\n",
       "  'gon',\n",
       "  'na',\n",
       "  'hurt',\n",
       "  'three',\n",
       "  'day'],\n",
       " ['remember',\n",
       "  'apostrophe',\n",
       "  'quot',\n",
       "  'week',\n",
       "  \"'s\",\n",
       "  'quot',\n",
       "  'week',\n",
       "  'hope',\n",
       "  'get',\n",
       "  'math',\n",
       "  'challenge',\n",
       "  'answer',\n",
       "  'though',\n",
       "  'know'],\n",
       " ['lie', 'still', 'recieving'],\n",
       " ['havent', 'get', 'decent', 'sleep', 'days'],\n",
       " ['block', 'nose', 'grr', 'hope', \"n't\", 'cold'],\n",
       " ['isca', 'bb'],\n",
       " ['wish', 'could', 'come', 'canton'],\n",
       " ['poor', 'stanley'],\n",
       " ['get', 'regular', 'email', 'call', 'miss', 'hold', 'miss', 'boyfriend'],\n",
       " ['wish', 'vacation', 'kinda', 'suck'],\n",
       " ['wish', 'go', 'hope', 'fun', 'laul'],\n",
       " ['sophies', 'kitty', 'myla', 'asleep', 'chest', 'want', 'kitty'],\n",
       " ['buddying',\n",
       "  'spymaster',\n",
       "  \"n't\",\n",
       "  'know',\n",
       "  'win',\n",
       "  'game',\n",
       "  'like',\n",
       "  'concentrate',\n",
       "  'real',\n",
       "  'estate'],\n",
       " ['listen',\n",
       "  'original',\n",
       "  'music',\n",
       "  'feature',\n",
       "  'gossip',\n",
       "  'girl',\n",
       "  'good',\n",
       "  'morning',\n",
       "  'manhattan'],\n",
       " ['haha', 'random', 'walmart', 'trip', 'turn', 'snack', 'trip'],\n",
       " ['phone', 'wo', \"n't\", 'charge', 'fml'],\n",
       " [\"'bin\",\n",
       "  'watch',\n",
       "  'kitchen',\n",
       "  'nightmare',\n",
       "  'im',\n",
       "  'hungry',\n",
       "  'imma',\n",
       "  'get',\n",
       "  'bite',\n",
       "  'eat',\n",
       "  'lose',\n",
       "  'mofoin',\n",
       "  \"'\",\n",
       "  'follower'],\n",
       " [\"'re\",\n",
       "  'read',\n",
       "  'saga',\n",
       "  'twilight',\n",
       "  'read',\n",
       "  'amaze',\n",
       "  'enjoy',\n",
       "  'answer',\n",
       "  'make',\n",
       "  'happy'],\n",
       " ['movie',\n",
       "  'tonight',\n",
       "  'besties',\n",
       "  'ur',\n",
       "  'nvr',\n",
       "  'old',\n",
       "  'watch',\n",
       "  'night',\n",
       "  'museum'],\n",
       " ['haha',\n",
       "  'lucky',\n",
       "  'thing',\n",
       "  'aint',\n",
       "  'awesome',\n",
       "  'haha',\n",
       "  'tell',\n",
       "  'put',\n",
       "  'top',\n",
       "  'hehe',\n",
       "  'many',\n",
       "  'friend'],\n",
       " ['move', 'easy', 'say', 'do'],\n",
       " ['sssweeeeett'],\n",
       " [\"'re\", 'welcome'],\n",
       " ['start', 'already'],\n",
       " ['come',\n",
       "  'june',\n",
       "  'besteveryou',\n",
       "  'com',\n",
       "  'fitness',\n",
       "  'challenge',\n",
       "  'get',\n",
       "  'ready',\n",
       "  'shed',\n",
       "  'pound',\n",
       "  'get',\n",
       "  'healthy'],\n",
       " ['go', 'girl', 'exercise', 'key', 'faster', 'metabolism', 'keep'],\n",
       " [\"'m\", 'dennys', 'juiced', 'brown', 'area', 'town', 'amazing', 'night'],\n",
       " ['great', 'day', 'absolutely', 'wonderful'],\n",
       " ['ak', 'v', 'deeb', \"'s\", 'aq', 'sb', 'v', 'bb', 'qjx77', 'sigh', '108th'],\n",
       " ['ok', 'need', 'invitation', 'anyone', 'desperate'],\n",
       " ['live',\n",
       "  'burma',\n",
       "  'today',\n",
       "  '1st',\n",
       "  'year',\n",
       "  'anniversary',\n",
       "  'nargis',\n",
       "  'cyclone',\n",
       "  'bad',\n",
       "  'memory'],\n",
       " ['move',\n",
       "  '9',\n",
       "  'day',\n",
       "  'ahhh',\n",
       "  'love',\n",
       "  'new',\n",
       "  'apartment',\n",
       "  \"'ll\",\n",
       "  '21',\n",
       "  '6',\n",
       "  'month',\n",
       "  'tomorrow',\n",
       "  'lol'],\n",
       " ['sore',\n",
       "  'throat',\n",
       "  'hurt',\n",
       "  'bad',\n",
       "  'buy',\n",
       "  'new',\n",
       "  'volume',\n",
       "  'clique',\n",
       "  \"'m\",\n",
       "  'kind',\n",
       "  'happy'],\n",
       " ['get', 'oh', 'goodness', 'yay', 'eisley', 'brother', 'one', 'right'],\n",
       " [\"'s\", 'amazing', 'lakers', 'mission', 'complete'],\n",
       " ['really',\n",
       "  'go',\n",
       "  'learn',\n",
       "  'make',\n",
       "  'beef',\n",
       "  'stew',\n",
       "  'hehe',\n",
       "  'unless',\n",
       "  'beef',\n",
       "  'prefer'],\n",
       " ['get', 'work', 'early'],\n",
       " ['house', 'season', '1'],\n",
       " [\"'m\",\n",
       "  'rude',\n",
       "  \"n't\",\n",
       "  'say',\n",
       "  'hello',\n",
       "  'everyone',\n",
       "  'amp',\n",
       "  'new',\n",
       "  'follower',\n",
       "  'hi',\n",
       "  'amp',\n",
       "  'welcome'],\n",
       " ['time', 'get', 'cross', 'trainer', 'work', 'crappy', '10', 'hour', 'shift'],\n",
       " ['thank'],\n",
       " ['aww', 'sorry', 'hear', '1', 'day', 'never', 'alone', 'twitterverse'],\n",
       " ['yeah',\n",
       "  'likewise',\n",
       "  'get',\n",
       "  'nowt',\n",
       "  'ready',\n",
       "  'clothes',\n",
       "  'wise',\n",
       "  'busy',\n",
       "  'day',\n",
       "  'tomorrow'],\n",
       " ['ha', 'ha', 'lol', \"'d\", 'big', 'time', 'want', 'meet', 'bloke'],\n",
       " ['oh',\n",
       "  'gosh',\n",
       "  \"'s\",\n",
       "  'student',\n",
       "  'de',\n",
       "  'la',\n",
       "  'salle',\n",
       "  'university',\n",
       "  'manila',\n",
       "  'possible',\n",
       "  'victim',\n",
       "  'ah1n1'],\n",
       " ['great', 'article', 'guy'],\n",
       " ['oh',\n",
       "  'darn',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'two',\n",
       "  'session',\n",
       "  'tomorrow',\n",
       "  'sorry',\n",
       "  'lady',\n",
       "  'time',\n",
       "  'dinner',\n",
       "  'lunch',\n",
       "  'time',\n",
       "  'drive'],\n",
       " ['get', 'back', 'dance', 'competion', 'check', 'amazingly', 'well'],\n",
       " ['think',\n",
       "  'today',\n",
       "  'good',\n",
       "  'day',\n",
       "  'try',\n",
       "  'mobile',\n",
       "  'broadband',\n",
       "  'sit',\n",
       "  'sun',\n",
       "  '1001'],\n",
       " ['terimayo', 'yummy', 'perfect', 'way', 'spend', 'lunchtime'],\n",
       " ['thats',\n",
       "  'commit',\n",
       "  'cant',\n",
       "  'afford',\n",
       "  '1',\n",
       "  'show',\n",
       "  'year',\n",
       "  'stupid',\n",
       "  'financial',\n",
       "  'crisis',\n",
       "  'wish',\n",
       "  'get',\n",
       "  'k',\n",
       "  'rudd',\n",
       "  'money'],\n",
       " ['tweet', 'tweet', \"'s\", 'summer'],\n",
       " ['argh',\n",
       "  'go',\n",
       "  'dentist',\n",
       "  'tomorrow',\n",
       "  'fair',\n",
       "  'nothing',\n",
       "  'wrong',\n",
       "  'teeth',\n",
       "  \"n't\",\n",
       "  'hope'],\n",
       " ['thus',\n",
       "  'mourn',\n",
       "  'ephra',\n",
       "  'j',\n",
       "  'bone',\n",
       "  'faithful',\n",
       "  '30g',\n",
       "  'ipod',\n",
       "  'lose',\n",
       "  'forever',\n",
       "  'depth'],\n",
       " ['job',\n",
       "  'save',\n",
       "  'may',\n",
       "  'able',\n",
       "  'manage',\n",
       "  'something',\n",
       "  'need',\n",
       "  'see',\n",
       "  'lt',\n",
       "  '33'],\n",
       " ['heart',\n",
       "  'beast',\n",
       "  'powderhorn',\n",
       "  \"n't\",\n",
       "  'miss',\n",
       "  'year',\n",
       "  'kid',\n",
       "  'oodles',\n",
       "  'homework',\n",
       "  'amp',\n",
       "  'show',\n",
       "  'prep'],\n",
       " ['real', 'imagine', 'strawberry', 'one', 'would', 'taste', 'like'],\n",
       " ['home', 'line', 'engage', 'stephy', 'stop', 'talk', 'phoneeeeeee'],\n",
       " ['absolutely',\n",
       "  'right',\n",
       "  'maybe',\n",
       "  'people',\n",
       "  'see',\n",
       "  \"'normal\",\n",
       "  \"'\",\n",
       "  'individual',\n",
       "  'way'],\n",
       " ['set',\n",
       "  'wit',\n",
       "  'crew',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'believe',\n",
       "  'work',\n",
       "  'today',\n",
       "  'shoot',\n",
       "  'last',\n",
       "  'scene'],\n",
       " ['get', 'mudpack', 'look', 'great', 'two', 'day', 'mud', 'fell'],\n",
       " ['sad', 'happen', 'madonna', 'pap', 'cause', 'take', 'fall', 'insensitive'],\n",
       " ['combat', 'arm', 'server', 'busy', 'lt'],\n",
       " [\"'s\",\n",
       "  'business',\n",
       "  'time',\n",
       "  'rush',\n",
       "  'o2',\n",
       "  'flight',\n",
       "  'delay',\n",
       "  'feel',\n",
       "  'good',\n",
       "  'back',\n",
       "  'london',\n",
       "  'go',\n",
       "  'major',\n",
       "  'withdrawal',\n",
       "  'moment'],\n",
       " ['super',\n",
       "  'long',\n",
       "  'day',\n",
       "  'work',\n",
       "  'babysitting',\n",
       "  'cousin',\n",
       "  '12',\n",
       "  'hour',\n",
       "  'tomorrow',\n",
       "  'yoiu',\n",
       "  \"'re\",\n",
       "  'gon',\n",
       "  'na',\n",
       "  'tampa',\n",
       "  'come',\n",
       "  'visit'],\n",
       " ['wish', 'live', 'narridge', 'tory', 'dominate', 'suffolk', 'coastal'],\n",
       " ['oh', 'jules', 'shitey', 'headache', 'thinking', 'go', 'back', 'bed'],\n",
       " ['thanks'],\n",
       " ['love', 'video'],\n",
       " ['psc',\n",
       "  'ka',\n",
       "  'neiet',\n",
       "  'tad',\n",
       "  'neiet',\n",
       "  'ugunsm',\n",
       "  'r',\n",
       "  'aizgriezts',\n",
       "  'viss',\n",
       "  'iesp',\n",
       "  'jamais',\n",
       "  'vpn',\n",
       "  'neiet',\n",
       "  'ftp',\n",
       "  'neiet',\n",
       "  'ftp',\n",
       "  'neiet',\n",
       "  'fuck',\n",
       "  'taisam',\n",
       "  'http',\n",
       "  'ftp'],\n",
       " ['finish',\n",
       "  'response',\n",
       "  'doc',\n",
       "  'sh',\n",
       "  'award',\n",
       "  'tonight',\n",
       "  'posse',\n",
       "  'n',\n",
       "  'crew',\n",
       "  'late',\n",
       "  'one',\n",
       "  'mind',\n",
       "  'presentation',\n",
       "  'first',\n",
       "  'thing',\n",
       "  'tomorrow'],\n",
       " ['exhaust', 'soccer', 'throughout', 'entire', 'week', 'much'],\n",
       " ['finally',\n",
       "  'saw',\n",
       "  'star',\n",
       "  'trek',\n",
       "  'good',\n",
       "  'sushi',\n",
       "  'good',\n",
       "  'leave',\n",
       "  'two',\n",
       "  'week',\n",
       "  'tomorrow',\n",
       "  'morning',\n",
       "  'sad',\n",
       "  'miss',\n",
       "  'wife'],\n",
       " ['writting',\n",
       "  'fate',\n",
       "  'im',\n",
       "  'try',\n",
       "  'make',\n",
       "  'script',\n",
       "  'play',\n",
       "  'like',\n",
       "  'quot',\n",
       "  'creator',\n",
       "  'quot',\n",
       "  'n',\n",
       "  'like'],\n",
       " ['lady', 'room', 'fo', 'sho', 'miss', 'ya', '33', 'day', 'love'],\n",
       " ['one'],\n",
       " ['eu', 'voto', 'fake'],\n",
       " ['ca', \"n't\", 'wait', 'new', 'moon', 'come', 'come', 'birthday'],\n",
       " ['errr', 'want', 'new', 'back', 'grond', 'picture', 'wo', \"n't\", 'let'],\n",
       " ['awe', 'man', \"'ll\", 'still', 'talk'],\n",
       " ['yay',\n",
       "  'welcome',\n",
       "  'back',\n",
       "  'think',\n",
       "  \"'d\",\n",
       "  'abandon',\n",
       "  'u',\n",
       "  'lol',\n",
       "  'sorry',\n",
       "  '2',\n",
       "  'hear',\n",
       "  'bout',\n",
       "  'ur',\n",
       "  'memorial',\n",
       "  'hope',\n",
       "  'everything',\n",
       "  'well',\n",
       "  'kissgive',\n",
       "  'baby'],\n",
       " ['im', 'go', 'another', 'twitterbreak', 'later'],\n",
       " ['pizza', 'tonight', 'yessir'],\n",
       " ['sooo',\n",
       "  'last',\n",
       "  'night',\n",
       "  'fun',\n",
       "  'wake',\n",
       "  'math',\n",
       "  'wasnt',\n",
       "  'haha',\n",
       "  'u',\n",
       "  'reap',\n",
       "  'u',\n",
       "  'sew'],\n",
       " [\"'m\", 'back', '5', 'day', 'without', 'internet', 'fun', 'survive'],\n",
       " ['petten',\n",
       "  'new',\n",
       "  'photo',\n",
       "  'gallery',\n",
       "  'comment',\n",
       "  'highly',\n",
       "  'appreciate',\n",
       "  'holland',\n",
       "  'netherlands',\n",
       "  'photo'],\n",
       " ['still',\n",
       "  'almost',\n",
       "  '1',\n",
       "  'clue',\n",
       "  \"'m\",\n",
       "  'guess',\n",
       "  \"'ll\",\n",
       "  'pretty',\n",
       "  'tired',\n",
       "  'work',\n",
       "  'tomorrow',\n",
       "  '3',\n",
       "  'day'],\n",
       " ['today', 'shock', 'sad', 'day', 'work', 'wo', \"n't\", 'anymore'],\n",
       " [\"'m\", 'sry', 'boo', 'honest', 'mistake', 'get', 'u', 'later'],\n",
       " ['still', 'feel', 'really', 'sick', 'ca', \"n't\", 'sleep'],\n",
       " ['know', 'u', \"n't\", 'luv', \"'s\", 'u', \"n't\", 'hittin'],\n",
       " ['little', 'one', 'strep', 'throat', 'another', 'long', 'night'],\n",
       " ['8',\n",
       "  'ugly',\n",
       "  'one',\n",
       "  'something',\n",
       "  'beautiful',\n",
       "  'ohh',\n",
       "  'whatta',\n",
       "  'tune',\n",
       "  'propper',\n",
       "  'love',\n",
       "  'kook'],\n",
       " ['know', 'number', \"n't\", 'slap'],\n",
       " ['thi', 'm4', 'v', 'c', 'u', 'b', 'n'],\n",
       " ['dino', 'gala', 'yesterday'],\n",
       " ['throat', 'hurt', 'cough'],\n",
       " [\"'re\", 'welcome'],\n",
       " ['back'],\n",
       " ['ah', 'joe', 'love', 'haha'],\n",
       " ['im', 'want', 'waggle', 'dance'],\n",
       " ['wow', 'rainin', 'agn', 'really', 'coldd'],\n",
       " ['neither'],\n",
       " ['granny', 'come', 'house', 'luuuuvs', 'run', 'hug'],\n",
       " ['seem', 'lose', 'profile', 'pic'],\n",
       " ['honest'],\n",
       " ['know',\n",
       "  'think',\n",
       "  'actually',\n",
       "  'hate',\n",
       "  'jane',\n",
       "  'would',\n",
       "  'kirstie',\n",
       "  'keren',\n",
       "  'komarl',\n",
       "  \"'s\",\n",
       "  'annoy',\n",
       "  'poor',\n",
       "  'komarl'],\n",
       " ['yes', 'make', 'sense'],\n",
       " ['want',\n",
       "  'get',\n",
       "  'back',\n",
       "  'sleep',\n",
       "  'really',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'shit',\n",
       "  \"'s\",\n",
       "  'harder',\n",
       "  'thought',\n",
       "  'would'],\n",
       " ['head', 'home', 'long', 'car', 'ride'],\n",
       " ['haha',\n",
       "  'yeah',\n",
       "  'know',\n",
       "  'fly',\n",
       "  'house',\n",
       "  'jib',\n",
       "  'job',\n",
       "  \"'s\",\n",
       "  'rus',\n",
       "  'meyer',\n",
       "  \"'s\",\n",
       "  \"'up\",\n",
       "  \"'\"],\n",
       " [\"g'night\", 'gabe', 'thanks'],\n",
       " ['woot', 'time', 'mostly', 'nude', 'comfort', 'good', 'time'],\n",
       " ['kyuhyun',\n",
       "  'oppa',\n",
       "  'nice',\n",
       "  'fan',\n",
       "  'make',\n",
       "  'day',\n",
       "  'add',\n",
       "  'music',\n",
       "  'twitter',\n",
       "  'aw',\n",
       "  'man'],\n",
       " ['funny', 'rude', 'joke', 'go', 'well'],\n",
       " ['haha', 'fine'],\n",
       " ['least', \"n't\", '450', 'shoe', 'still', 'though'],\n",
       " ['spill', 'bag', 'cherrios', 'purse'],\n",
       " ['reaaly',\n",
       "  'miss',\n",
       "  'john',\n",
       "  'mayer',\n",
       "  \"'s\",\n",
       "  'twitter',\n",
       "  'fuck',\n",
       "  'rude',\n",
       "  'people',\n",
       "  'remove',\n",
       "  \"n't\",\n",
       "  'like',\n",
       "  'tweet',\n",
       "  'bitchfucks'],\n",
       " ['actually',\n",
       "  'bad',\n",
       "  'facebook',\n",
       "  'app',\n",
       "  'coz',\n",
       "  'send',\n",
       "  'innumerable',\n",
       "  'dm',\n",
       "  'follower'],\n",
       " ['iranian', 'twitterer', 'start', 'report', 'sound', 'gunfire', 'street'],\n",
       " ['happen', 'summer', 'rain', 'pouringgg', 'get', 'ta', 'go', 'soon'],\n",
       " ['feel',\n",
       "  'really',\n",
       "  'nervous',\n",
       "  \"'ll\",\n",
       "  'get',\n",
       "  'abmeldebest',\n",
       "  'tigung',\n",
       "  'today',\n",
       "  'r',\n",
       "  'mlang',\n",
       "  'gemeindehaus',\n",
       "  'close',\n",
       "  'work',\n",
       "  'holiday',\n",
       "  'horror'],\n",
       " ['7',\n",
       "  'cavity',\n",
       "  'get',\n",
       "  '4',\n",
       "  'wisdom',\n",
       "  'teeth',\n",
       "  'extract',\n",
       "  'happen',\n",
       "  'even',\n",
       "  'floss',\n",
       "  'daily'],\n",
       " ['darling', 'shall', 'cha', 'cha', 'cha', 'way'],\n",
       " ['f', 'test', 'sam', 'wouldnt', 'let', 'copy'],\n",
       " ['smack', 'around', 'bit', 'teach', 'lesson'],\n",
       " ['love',\n",
       "  'speed',\n",
       "  'tech',\n",
       "  'support',\n",
       "  'guy',\n",
       "  'host',\n",
       "  'provider',\n",
       "  'problem',\n",
       "  'find',\n",
       "  'sort',\n",
       "  'time'],\n",
       " ['home', 'nice', 'brew'],\n",
       " ['think', 'may', 'right'],\n",
       " ['pack', 'get', 'ready', 'move'],\n",
       " ['quot',\n",
       "  'new',\n",
       "  'divide',\n",
       "  'quot',\n",
       "  'today',\n",
       "  'available',\n",
       "  'itunes',\n",
       "  '8am',\n",
       "  'est'],\n",
       " ['geez', 'still', 'upset', 'lol', 'im', 'sorry', 'bad', 'timing', 'honest'],\n",
       " ['account', 'delete', 'second', 'last', 'post', \"n't\", 'know', 'type'],\n",
       " ['yeah', 'dude', 'mo', 'cant', 'touch', 'u'],\n",
       " ['espn', 'com', \"'re\", 'cite', 'paul', 'hamilton', 'say', 'wgr'],\n",
       " ['omg', 'soooo', 'sad'],\n",
       " ['make', 'hot'],\n",
       " [\"'s\",\n",
       "  'sad',\n",
       "  'alone',\n",
       "  'home',\n",
       "  'day',\n",
       "  'bright',\n",
       "  'side',\n",
       "  'agency',\n",
       "  'reunite',\n",
       "  'tonight',\n",
       "  'see',\n",
       "  'hangover'],\n",
       " ['ok',\n",
       "  'go',\n",
       "  'taekwondo',\n",
       "  'lesson',\n",
       "  'call',\n",
       "  'teacher',\n",
       "  'say',\n",
       "  'oh',\n",
       "  'get',\n",
       "  'give',\n",
       "  '20',\n",
       "  'min',\n",
       "  'oh',\n",
       "  'call'],\n",
       " ['art', 'teacher', 'go', 'kill', 'meee', 'music', 'tues'],\n",
       " ['ooohhh', 'great', 'hash', 'tag', 'thanks'],\n",
       " ['wow', 'london', \"'s\", 'awesome', \"'re\", 'wait', 'brazil', 'gaga', 'muah'],\n",
       " ['sleep', 'back', 'richmond'],\n",
       " [\"'s\", 'horrible'],\n",
       " ['guess', 'everyone', 'want', 'right'],\n",
       " ['omg', 'ca', \"n't\", 'believe', 'weekend', 'want'],\n",
       " ['yeah', \"'s\", 'work', 'either', '140', 'conf'],\n",
       " ['yo', 'peep', 'keep', 'request', \"'s\", 'song', 'radio', 'go', 'mwc'],\n",
       " ['aaaaaa', 'belly', 'terrible', 'pain', 'help'],\n",
       " ['thought',\n",
       "  'w',\n",
       "  'brother',\n",
       "  'si',\n",
       "  'law',\n",
       "  'three',\n",
       "  'little',\n",
       "  'boys',\n",
       "  'home',\n",
       "  'hopefully',\n",
       "  \"'ll\",\n",
       "  'ok'],\n",
       " ['something', 'worthwhile', 'argue', 'board', 'atm', \"n't\"],\n",
       " ['funny', 'housemate', 'parking', 'job'],\n",
       " ['night',\n",
       "  'zoom',\n",
       "  'dawn',\n",
       "  'new',\n",
       "  'day',\n",
       "  'peek',\n",
       "  'burm',\n",
       "  'landfill',\n",
       "  'ah',\n",
       "  'freedom',\n",
       "  'soon',\n",
       "  'mine'],\n",
       " ['sun'],\n",
       " ['ca', \"n't\", 'work', \"'re\", 'diesel', 'rice', 'burner'],\n",
       " ['yes', 'see', 'info', 'english'],\n",
       " ['think',\n",
       "  'might',\n",
       "  'go',\n",
       "  'home',\n",
       "  'early',\n",
       "  \"'m\",\n",
       "  'sort',\n",
       "  'achy',\n",
       "  'need',\n",
       "  'warm',\n",
       "  'cover'],\n",
       " ['miss', 'fiance', 'already'],\n",
       " [\"n't\", 'watch', 'yet'],\n",
       " ['lol',\n",
       "  'u',\n",
       "  'ever',\n",
       "  'want',\n",
       "  'write',\n",
       "  'somebody',\n",
       "  'twitter',\n",
       "  'u',\n",
       "  'aint',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'seem',\n",
       "  'thirsty',\n",
       "  'dont',\n",
       "  'u',\n",
       "  'gon',\n",
       "  'feel',\n",
       "  'madd',\n",
       "  'driddy',\n",
       "  'smhhh'],\n",
       " ['community', \"'s\", 'lose', '50', 'yriold', 'gm', 'car', 'dealership'],\n",
       " ['nice', 'natalie', 'taught', 'jameson', 'quot', 'quot'],\n",
       " ['cmmi',\n",
       "  \"n't\",\n",
       "  'bad',\n",
       "  'million',\n",
       "  'spare',\n",
       "  'lean',\n",
       "  'agile',\n",
       "  'ok',\n",
       "  \"'ve\",\n",
       "  'get',\n",
       "  '50',\n",
       "  'spare'],\n",
       " ['hey',\n",
       "  'hun',\n",
       "  \"'m\",\n",
       "  'good',\n",
       "  'thanks',\n",
       "  'yes',\n",
       "  'still',\n",
       "  'remember',\n",
       "  'user',\n",
       "  'pas',\n",
       "  \"i'ma\",\n",
       "  'check'],\n",
       " ['hate', 'exhaust', 'get', 'easily', 'annoy'],\n",
       " ['feel', 'sick', \"'s\", 'wrong'],\n",
       " ['thanks',\n",
       "  'bday',\n",
       "  'wish',\n",
       "  'clos',\n",
       "  'wisconsin',\n",
       "  'drinkin',\n",
       "  'good',\n",
       "  'suds',\n",
       "  'back',\n",
       "  'work'],\n",
       " ['rootbeer', 'float', 'day', 'calculus', 'tuesday'],\n",
       " [\"n't\",\n",
       "  'anything',\n",
       "  'fancy',\n",
       "  \"n't\",\n",
       "  'think',\n",
       "  'old',\n",
       "  'even',\n",
       "  '1',\n",
       "  'yr',\n",
       "  'old',\n",
       "  'thing',\n",
       "  'like',\n",
       "  'vista'],\n",
       " ['sorta',\n",
       "  'kid',\n",
       "  'booze',\n",
       "  'lol',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'get',\n",
       "  'good',\n",
       "  'stuff',\n",
       "  'today',\n",
       "  'anyway'],\n",
       " ['far',\n",
       "  'bad',\n",
       "  'goth',\n",
       "  'wannabe',\n",
       "  'performance',\n",
       "  'ever',\n",
       "  'see',\n",
       "  'amp',\n",
       "  \"'m\",\n",
       "  'pretty',\n",
       "  'open',\n",
       "  'mind',\n",
       "  \"'s\",\n",
       "  'sad'],\n",
       " ['post', 'mtp'],\n",
       " ['thank', 'recommendation'],\n",
       " ['specially',\n",
       "  'trained',\n",
       "  'squirrel',\n",
       "  'carry',\n",
       "  'database',\n",
       "  'bit',\n",
       "  'bit',\n",
       "  'backup',\n",
       "  'server'],\n",
       " ['tnx', 'follow', 'look', 'forward', 'tweet'],\n",
       " ['bad',\n",
       "  'thanks',\n",
       "  'plan',\n",
       "  'bbq',\n",
       "  'look',\n",
       "  'like',\n",
       "  'quite',\n",
       "  'alot',\n",
       "  'peep',\n",
       "  'turn'],\n",
       " ['love', '80', \"'s\", 'song', 'lml'],\n",
       " ['think',\n",
       "  'he',\n",
       "  'ninja',\n",
       "  'onstage',\n",
       "  'do',\n",
       "  'triple',\n",
       "  'kick',\n",
       "  'flip',\n",
       "  'hurt',\n",
       "  'knee'],\n",
       " ['well', 'buy', 'complete', 'set', 'dead', 'like', 'watch', 'whole', 'thing'],\n",
       " ['yay', 'do', 'get', 'sort', 'hit', 'road', 'bank', 'holiday', 'rush'],\n",
       " ['hope',\n",
       "  'one',\n",
       "  'attach',\n",
       "  'super',\n",
       "  'long',\n",
       "  'hair',\n",
       "  'cut',\n",
       "  '14',\n",
       "  'inch',\n",
       "  'week'],\n",
       " ['decided', 'sister', 'read', 'mind', 'watch', 'twilight', 'hell', 'yeah'],\n",
       " ['go',\n",
       "  'back',\n",
       "  'austin',\n",
       "  'hour',\n",
       "  '7',\n",
       "  '8',\n",
       "  'yay',\n",
       "  'go',\n",
       "  'back',\n",
       "  'boo',\n",
       "  'much',\n",
       "  'driving'],\n",
       " ['allergic', 'reaction', 'lol', 'cant', 'eat', 'anything'],\n",
       " ['good',\n",
       "  'morning',\n",
       "  'germany',\n",
       "  'kitty',\n",
       "  'vid',\n",
       "  'see',\n",
       "  '2',\n",
       "  'cat',\n",
       "  'cute',\n",
       "  'bold',\n",
       "  'hell',\n",
       "  'lol'],\n",
       " ['love', 'story', 'v'],\n",
       " ['downer', 'also', 'contain', 'fat'],\n",
       " ['wait', 'guy', 'less', 'robin', 'hood'],\n",
       " ['good', 'night'],\n",
       " ['lol', \"'ll\", 'try', 'hope', 'well'],\n",
       " ['know', 'understand', 'feel', 'work', 'mean', '5', '30', 'boo', 'buzzkill'],\n",
       " ['girl',\n",
       "  'night',\n",
       "  'watch',\n",
       "  'twilight',\n",
       "  'day',\n",
       "  'tomorrow',\n",
       "  'cook',\n",
       "  'pack',\n",
       "  'plan',\n",
       "  'etc'],\n",
       " ['another',\n",
       "  'adventure',\n",
       "  'sam',\n",
       "  'gon',\n",
       "  'na',\n",
       "  'go',\n",
       "  'watch',\n",
       "  'plane',\n",
       "  'tonight'],\n",
       " ['productive', 'today'],\n",
       " ['decide',\n",
       "  'imac',\n",
       "  'amp',\n",
       "  'mac',\n",
       "  'mini',\n",
       "  'mac',\n",
       "  'mini',\n",
       "  'get',\n",
       "  'big',\n",
       "  'screen',\n",
       "  'future',\n",
       "  'want',\n",
       "  'hmmm'],\n",
       " ['walk',\n",
       "  'around',\n",
       "  'sunburn',\n",
       "  'legs',\n",
       "  'give',\n",
       "  'painful',\n",
       "  'thigh',\n",
       "  'muscle',\n",
       "  'hate',\n",
       "  'also',\n",
       "  'miss',\n",
       "  'crazy',\n",
       "  'tweencom',\n",
       "  'convo'],\n",
       " ['think',\n",
       "  'actually',\n",
       "  'work',\n",
       "  'tomorrow',\n",
       "  'maybe',\n",
       "  'even',\n",
       "  'blog',\n",
       "  'update',\n",
       "  'really',\n",
       "  \"'ll\",\n",
       "  'want',\n",
       "  'sleep',\n",
       "  'take',\n",
       "  'picture',\n",
       "  'alone'],\n",
       " ['hav', 'go', 'vote'],\n",
       " ['black',\n",
       "  'one',\n",
       "  'wearing',\n",
       "  'last',\n",
       "  'night',\n",
       "  'remembrance',\n",
       "  'lt',\n",
       "  '3',\n",
       "  'lot',\n",
       "  'love'],\n",
       " ['long', 'day', 'amp', 'big', 'weekend', 'ahead', 'girl'],\n",
       " ['yes',\n",
       "  'remember',\n",
       "  'mall',\n",
       "  '2',\n",
       "  'year',\n",
       "  'old',\n",
       "  'suck',\n",
       "  'sorry',\n",
       "  'peter',\n",
       "  'fac',\n",
       "  'ell',\n",
       "  'peter',\n",
       "  'fac',\n",
       "  'ell',\n",
       "  'peter',\n",
       "  'fac',\n",
       "  'ell'],\n",
       " ['think',\n",
       "  'story',\n",
       "  'go',\n",
       "  'take',\n",
       "  'place',\n",
       "  'brittany',\n",
       "  'france',\n",
       "  'welsh',\n",
       "  'yes',\n",
       "  'correlation'],\n",
       " [\"n't\", 'get', 'pjs', 'morning'],\n",
       " ['boyf', 'want', 'charge', 'internet'],\n",
       " ['myy', 'fridge', 'lookin', 'empty'],\n",
       " [\"n't\", 'walk', 'away', 'lt', '3', \"'s\", 'repeat', 'past', 'two', 'hour'],\n",
       " ['new', 'tv', 'show', 'quot', 'sit', 'shut', 'quot', 'funny'],\n",
       " ['cry', 'hotel', 'dog', 'sad'],\n",
       " ['os',\n",
       "  '3',\n",
       "  '0',\n",
       "  'iphone',\n",
       "  'darn',\n",
       "  'os',\n",
       "  '3',\n",
       "  '0',\n",
       "  'make',\n",
       "  'iphone',\n",
       "  'take',\n",
       "  'age',\n",
       "  'turn'],\n",
       " ['didnt', 'phone', 'date'],\n",
       " [\"'m\",\n",
       "  'gon',\n",
       "  'na',\n",
       "  'bit',\n",
       "  'today',\n",
       "  'check',\n",
       "  'often',\n",
       "  'please',\n",
       "  'feel',\n",
       "  'free',\n",
       "  'chat',\n",
       "  'amongst',\n",
       "  'twin',\n",
       "  'tuesday'],\n",
       " ['ugh',\n",
       "  'wet',\n",
       "  'miserable',\n",
       "  'look',\n",
       "  'saturday',\n",
       "  'oh',\n",
       "  \"'s\",\n",
       "  'even',\n",
       "  '7am',\n",
       "  'yet',\n",
       "  'lucky'],\n",
       " ['wait', 'suppose', 'mean', 'locke', 'actually', 'dead', 'like', 'locke'],\n",
       " ['totally',\n",
       "  'say',\n",
       "  'quot',\n",
       "  'ah',\n",
       "  'quot',\n",
       "  'instead',\n",
       "  'quot',\n",
       "  'uh',\n",
       "  'dee',\n",
       "  'dus',\n",
       "  'quot',\n",
       "  'lmao',\n",
       "  'will',\n",
       "  'still',\n",
       "  'laugh',\n",
       "  'mee'],\n",
       " ['busy',\n",
       "  'busy',\n",
       "  'busy',\n",
       "  'today',\n",
       "  'get',\n",
       "  'ta',\n",
       "  'get',\n",
       "  'focus',\n",
       "  'want',\n",
       "  'say',\n",
       "  'happy',\n",
       "  'even',\n",
       "  'though',\n",
       "  \"'m\",\n",
       "  'sick',\n",
       "  'lt',\n",
       "  '3'],\n",
       " ['good', 'morning', 'think', 'recover', 'dont', 'think', 'yet'],\n",
       " [\"n't\", 'know', 'twitter', 'one', 'favorite', 'canadian'],\n",
       " ['haha',\n",
       "  'wtf',\n",
       "  'carrot',\n",
       "  'juice',\n",
       "  'toki',\n",
       "  'rabbit',\n",
       "  \"'s\",\n",
       "  'little',\n",
       "  'bit',\n",
       "  'much',\n",
       "  'lmao'],\n",
       " ['food', 'amaze', 'really', 'use', 'emoticon', 'earlier'],\n",
       " ['get'],\n",
       " ['bff', \"n't\", 'mind'],\n",
       " ['mtv', 'movie', 'award', 'stanley', 'cup'],\n",
       " ['get',\n",
       "  'atl',\n",
       "  'quite',\n",
       "  'important',\n",
       "  'meeting',\n",
       "  'tomorrow',\n",
       "  'outkast',\n",
       "  'j',\n",
       "  'k',\n",
       "  'wish',\n",
       "  \"n't\",\n",
       "  'late',\n",
       "  'go',\n",
       "  'see',\n",
       "  'band',\n",
       "  'play'],\n",
       " ['rain', 'outside', 'rad', 'least', 'breath', 'fresh', 'air'],\n",
       " ['rid', 'today'],\n",
       " ['lol', 'cute'],\n",
       " ['reflector',\n",
       "  'birthday',\n",
       "  'exactly',\n",
       "  'opposite',\n",
       "  'side',\n",
       "  'year',\n",
       "  'actual',\n",
       "  'one'],\n",
       " ['ohmyy', 'secret', 'life', 'awesome', 'poor', 'grace'],\n",
       " ['night', 'fill', 'dream', 'lately', 'let', \"'s\", 'see', 'tonight', 'brings'],\n",
       " ['bed', 'say'],\n",
       " ['2',\n",
       "  'day',\n",
       "  'row',\n",
       "  'ate',\n",
       "  'pizza',\n",
       "  'hut',\n",
       "  '3',\n",
       "  'day',\n",
       "  'row',\n",
       "  'ate',\n",
       "  'kfc',\n",
       "  'simply',\n",
       "  'love',\n",
       "  'life'],\n",
       " ['oh', 'god', 'ca', \"n't\", 'get', 'find', 'bff'],\n",
       " ['hard',\n",
       "  'know',\n",
       "  'people',\n",
       "  'think',\n",
       "  'good',\n",
       "  'enough',\n",
       "  'even',\n",
       "  'hard',\n",
       "  'learn',\n",
       "  'love',\n",
       "  'one'],\n",
       " [\"'m\", 'feel', 'quite', 'lonely'],\n",
       " ['go', 'line', 'night', 'get', 'ta', 'get', 'super', 'early', 'tomorrow'],\n",
       " ['ugh', 'im', 'sooo', 'sore', 'wii', 'fit'],\n",
       " ['want', 'see', 'troupe'],\n",
       " [\"'m\", 'give', 'away', 'activeion', 'reach', '100', 'follower', 'blog'],\n",
       " ['nada', \"'s\"],\n",
       " ['try', 'live', 'england', 'isnt', 'rain', 'pouring'],\n",
       " ['tbh',\n",
       "  'dnt',\n",
       "  'know',\n",
       "  'xd',\n",
       "  'could',\n",
       "  '2moro',\n",
       "  'could',\n",
       "  'nxt',\n",
       "  'month',\n",
       "  'haha',\n",
       "  'eva',\n",
       "  \"'m\",\n",
       "  'gunna',\n",
       "  'wait'],\n",
       " ['get', 'exam', 'weeeeeeeek'],\n",
       " ['im', 'never', 'happy', 'people', 'piss', 'make'],\n",
       " [\"'m\", 'lateeee'],\n",
       " ['clean'],\n",
       " ['yay', 'love', 'drunk', 'lol', 'least', 'home', 'safe'],\n",
       " ['ask',\n",
       "  'feedback',\n",
       "  'need',\n",
       "  'comic',\n",
       "  'sans',\n",
       "  'replacement',\n",
       "  'need',\n",
       "  'help',\n",
       "  'together',\n",
       "  'rid',\n",
       "  'world',\n",
       "  'c',\n",
       "  'm'],\n",
       " ['ate', 'tgi', 'fridays', 'lovely', 'aurora', 'cinema'],\n",
       " ['slept',\n",
       "  'nats',\n",
       "  'house',\n",
       "  'ahhhh',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'wait',\n",
       "  'hs',\n",
       "  'show',\n",
       "  '23rd'],\n",
       " ['im',\n",
       "  'good',\n",
       "  'thanx',\n",
       "  'ola',\n",
       "  'cant',\n",
       "  'wait',\n",
       "  'tomorrow',\n",
       "  'see',\n",
       "  'surprise',\n",
       "  'thingy',\n",
       "  'still',\n",
       "  'dont',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'go',\n",
       "  'back'],\n",
       " ['happy', 'birthday', 'throw', 'delicious', 'bon', 'bon', 'party'],\n",
       " ['go',\n",
       "  'eat',\n",
       "  'birthday',\n",
       "  'dinner',\n",
       "  'friends',\n",
       "  'party',\n",
       "  'woo',\n",
       "  'wish',\n",
       "  'dallas',\n",
       "  'though'],\n",
       " ['read',\n",
       "  'danielle',\n",
       "  'crittendon',\n",
       "  \"'s\",\n",
       "  'mother',\n",
       "  \"n't\",\n",
       "  'tell',\n",
       "  'u',\n",
       "  'get',\n",
       "  'ready',\n",
       "  'tonight',\n",
       "  \"'s\",\n",
       "  'bbq',\n",
       "  'feast'],\n",
       " ['yesterday', 'today', 'lazy'],\n",
       " ['unfortunately', 'cant', 'call', 'haha'],\n",
       " ['glad',\n",
       "  'hear',\n",
       "  'someone',\n",
       "  \"n't\",\n",
       "  'bore',\n",
       "  'mind',\n",
       "  \"'m\",\n",
       "  'still',\n",
       "  'wait',\n",
       "  'daddy',\n",
       "  'peter',\n",
       "  'hold',\n",
       "  'skin',\n",
       "  'mag',\n",
       "  'camera'],\n",
       " ['finish',\n",
       "  '3',\n",
       "  'last',\n",
       "  'night',\n",
       "  'xd',\n",
       "  'read',\n",
       "  'one',\n",
       "  'morning',\n",
       "  'review',\n",
       "  'book',\n",
       "  '4',\n",
       "  'right',\n",
       "  'book',\n",
       "  '5',\n",
       "  'almost',\n",
       "  'do'],\n",
       " ['sunday', 'already', 'weekend', 'go', 'fast'],\n",
       " [\"'ll\", 'try', 'catch', 'videochat', 'show', 'tonight'],\n",
       " [\"'ve\",\n",
       "  'get',\n",
       "  'ticket',\n",
       "  'final',\n",
       "  'queen',\n",
       "  \"'s\",\n",
       "  'finger',\n",
       "  'cross',\n",
       "  \"'ll\",\n",
       "  'hop',\n",
       "  'see',\n",
       "  'battle',\n",
       "  'andy',\n",
       "  \"'s\"],\n",
       " ['feel', 'sick', \"'ve\", 'get', 'cold'],\n",
       " ['happy', 'birthday', 'tweeter', 'restaurant', 'cafe', 'bear', 'july'],\n",
       " ['nothing', 'wrong', \"'m\", 'hang', 'pool'],\n",
       " ['go', 'mom', \"n't\", 'wan', 'na', 'go'],\n",
       " ['wan', 'na', 'go', 'gym'],\n",
       " ['bah', 'h8', 'wake'],\n",
       " ['yes',\n",
       "  'gona',\n",
       "  'ask',\n",
       "  'want',\n",
       "  'walk',\n",
       "  'doggy',\n",
       "  'together',\n",
       "  'soon',\n",
       "  'ziggy',\n",
       "  \"'s\",\n",
       "  'old',\n",
       "  'enough',\n",
       "  'socialize',\n",
       "  'yet',\n",
       "  'tho'],\n",
       " ['well', 'day', 'back', 'work', 'today'],\n",
       " ['thk', 'u'],\n",
       " [\"'m\", 'go', 'lombok', 'tonight'],\n",
       " ['well',\n",
       "  'yeah',\n",
       "  'hormone',\n",
       "  'thing',\n",
       "  'basically',\n",
       "  'give',\n",
       "  'thought',\n",
       "  'misbehave',\n",
       "  'specifically',\n",
       "  'upset',\n",
       "  'mom'],\n",
       " ['stress',\n",
       "  'work',\n",
       "  'drama',\n",
       "  'life',\n",
       "  'drama',\n",
       "  'stuff',\n",
       "  'ignore',\n",
       "  'blissfully',\n",
       "  'vacay',\n",
       "  'haunt',\n",
       "  'boo'],\n",
       " ['yes', 'let', 'know', 'either', 'friday', 'sit', 'cool'],\n",
       " ['ahhh', \"'s\", 'friend', 'anae', \"'s\", 'thanks', \"'ll\", 'well', 'happy'],\n",
       " ['look',\n",
       "  'like',\n",
       "  'need',\n",
       "  'book',\n",
       "  'flight',\n",
       "  'cali',\n",
       "  'im',\n",
       "  'actually',\n",
       "  'write',\n",
       "  '1920s',\n",
       "  'research',\n",
       "  'paper',\n",
       "  'need',\n",
       "  'concrete',\n",
       "  'detail'],\n",
       " ['like', 'ice', 'cream'],\n",
       " ['go', 'sleep', 'goodnight', 'people', 'lt', '3'],\n",
       " ['thank', 'work', 'really', 'hard', 'want', 'birthday'],\n",
       " ['lay', 'bed', \"n't\", 'fun', 'text'],\n",
       " ['try', 'put', 'photo', 'hubby', 'profile', 'photo', 'go'],\n",
       " ['good',\n",
       "  'night',\n",
       "  'twitter',\n",
       "  'show',\n",
       "  'do',\n",
       "  'night',\n",
       "  'friday',\n",
       "  'night',\n",
       "  'show'],\n",
       " ['ca', \"n't\", 'beat', 'hot', 'hot', 'heat', 'quot', 'talk', 'dance', 'quot'],\n",
       " ['gon',\n",
       "  'na',\n",
       "  'lie',\n",
       "  'bed',\n",
       "  'watch',\n",
       "  'white',\n",
       "  'chick',\n",
       "  'til',\n",
       "  'fall',\n",
       "  'asleep',\n",
       "  'mmm',\n",
       "  'night'],\n",
       " ['oh', 'dear', 'careful', 'wish'],\n",
       " ['wear',\n",
       "  'black',\n",
       "  'work',\n",
       "  'sun',\n",
       "  'hit',\n",
       "  'skintight',\n",
       "  'shirt',\n",
       "  'feel',\n",
       "  'shirt',\n",
       "  'burn',\n",
       "  'skin'],\n",
       " ['love', 'outdoors', \"'s\", 'night', 'time'],\n",
       " ['go', 'find', \"'m\", 'take', 'hospital'],\n",
       " ['yeah',\n",
       "  'heros',\n",
       "  'awesome',\n",
       "  'follow',\n",
       "  'season',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'really',\n",
       "  'say',\n",
       "  'ability',\n",
       "  'favorite',\n",
       "  'maybe',\n",
       "  'super',\n",
       "  'memory'],\n",
       " ['aww',\n",
       "  'someone',\n",
       "  'dog',\n",
       "  'get',\n",
       "  'hit',\n",
       "  'drive',\n",
       "  'around',\n",
       "  'people',\n",
       "  'try',\n",
       "  'figure'],\n",
       " ['today', 'bird', 'fly', 'window', 'die', 'poor', 'thang'],\n",
       " ['concert',\n",
       "  'web',\n",
       "  'live',\n",
       "  'martin',\n",
       "  'jondo',\n",
       "  'konzert',\n",
       "  'live',\n",
       "  'stream',\n",
       "  '07',\n",
       "  '4',\n",
       "  '2009',\n",
       "  '9',\n",
       "  'pm',\n",
       "  'jeeehhhaaaaa',\n",
       "  'nice'],\n",
       " ['random',\n",
       "  'rap',\n",
       "  'video',\n",
       "  'background',\n",
       "  'bet',\n",
       "  'great',\n",
       "  'need',\n",
       "  'lift',\n",
       "  'something',\n",
       "  'fresh',\n",
       "  'like',\n",
       "  'real',\n",
       "  'talk',\n",
       "  'become'],\n",
       " [\"n't\", 'forever'],\n",
       " ['never', 'sing'],\n",
       " ['error', 'find', 'cant', 'vote'],\n",
       " ['home', 'freakim', 'cant', 'sleep'],\n",
       " ['yea',\n",
       "  'wish',\n",
       "  'open',\n",
       "  'since',\n",
       "  'drive',\n",
       "  'every',\n",
       "  'morning',\n",
       "  'settle',\n",
       "  'beyond',\n",
       "  'coffee'],\n",
       " ['lol', 'poor', 'dog'],\n",
       " ['pick', 'square', 'space', 'winner', 'like', '40', 'min', 'ago'],\n",
       " ['mynd',\n",
       "  'nol',\n",
       "  'ticedi',\n",
       "  'gwylie',\n",
       "  'mewn',\n",
       "  'bach',\n",
       "  'thursday',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'come',\n",
       "  'soon',\n",
       "  'enough'],\n",
       " ['lose', '300k', 'fb', 'poker', 'tonight', 'broke'],\n",
       " ['beingng', 'lazy', 'amp', 'hungover', 'back', 'wor', '5'],\n",
       " [],\n",
       " ['micheal', 'jackson', 'soo', 'cute', 'back', 'day'],\n",
       " ['nope', 'miss', 'anything'],\n",
       " ['camera', 'clickin', \"'\", 'pic', 'facebook', 'profile'],\n",
       " ['wii',\n",
       "  'fit',\n",
       "  'calculates',\n",
       "  'weight',\n",
       "  'height',\n",
       "  'current',\n",
       "  'age',\n",
       "  'mean',\n",
       "  \"'m\",\n",
       "  'back',\n",
       "  'quot',\n",
       "  'normal',\n",
       "  'quot',\n",
       "  'category'],\n",
       " ['jess', 'doiing', 'today', 'work', 'see', 'ya', \"'ll\", '8'],\n",
       " ['perhaps',\n",
       "  'quot',\n",
       "  'live',\n",
       "  'long',\n",
       "  'prosper',\n",
       "  'quot',\n",
       "  'good',\n",
       "  'enough',\n",
       "  'answer'],\n",
       " ['guess',\n",
       "  'long',\n",
       "  'run',\n",
       "  'long',\n",
       "  'content',\n",
       "  'good',\n",
       "  'quality',\n",
       "  'worth',\n",
       "  'always',\n",
       "  'glad',\n",
       "  'join'],\n",
       " ['thanks', \"n't\", 'see', 'long', 'time'],\n",
       " ['morning',\n",
       "  'go',\n",
       "  'windermere',\n",
       "  'today',\n",
       "  'last',\n",
       "  'day',\n",
       "  'holiday',\n",
       "  'great',\n",
       "  'time',\n",
       "  'though',\n",
       "  'feeling',\n",
       "  'fat'],\n",
       " ['hey', 'check', 'post'],\n",
       " ['yeah', 'fuck', 'em', 'luv', 'ur', 'tweet', 'keep', 'bein', 'saucy'],\n",
       " ['could', 'lose', 'follow'],\n",
       " ['get',\n",
       "  'home',\n",
       "  \"'s\",\n",
       "  'weird',\n",
       "  'miss',\n",
       "  'home',\n",
       "  'lodge',\n",
       "  \"n't\",\n",
       "  'hear',\n",
       "  'waterfall'],\n",
       " ['ok',\n",
       "  'far',\n",
       "  'iphone',\n",
       "  'os',\n",
       "  '3',\n",
       "  '0',\n",
       "  \"n't\",\n",
       "  'bad',\n",
       "  'step',\n",
       "  'right',\n",
       "  'direction',\n",
       "  'copy',\n",
       "  'paste',\n",
       "  'gooood',\n",
       "  'mm',\n",
       "  'work',\n",
       "  'yet',\n",
       "  'bad'],\n",
       " ['great', 'saturday'],\n",
       " ['good', 'morning', 'tweeties'],\n",
       " [\"'ll\", 'back', 'home', 'soon', 'oooh', 'noooo'],\n",
       " ['back', 'huntington', 'probably', 'last', 'visit', 'big', 'move'],\n",
       " ['oh', 'see', 'everyone', 'play', 'ispy', 'want', 'play'],\n",
       " ['awww',\n",
       "  'talk',\n",
       "  'smack',\n",
       "  'abt',\n",
       "  'tb',\n",
       "  'computer',\n",
       "  'need',\n",
       "  'coordinate',\n",
       "  'comp',\n",
       "  'time',\n",
       "  'team',\n",
       "  'style'],\n",
       " ['morning', 'everyone'],\n",
       " ['every', 'one', 'get', 'confused', 'review', 'okay', 'okay'],\n",
       " ['go', 'lay', 'sun', 'b4', 'go', 'work'],\n",
       " ['stupid', 'manhattan', 'street', 'givin', 'poor', 'whip', 'flat', 'tire'],\n",
       " ['yay',\n",
       "  \"'m\",\n",
       "  'happy',\n",
       "  'find',\n",
       "  'mitchell',\n",
       "  'amp',\n",
       "  'ness',\n",
       "  'magic',\n",
       "  'johnson',\n",
       "  'jersey',\n",
       "  'time',\n",
       "  'grad',\n",
       "  'amp',\n",
       "  'dad',\n",
       "  'day'],\n",
       " ['oh', '3', 'day', 'go', 'want', 'ny'],\n",
       " ['tryin', 'find', 'bryan'],\n",
       " ['really', 'bad', 'headache', 'good'],\n",
       " ['play', 'pup', 'go', 'work', 'miss', 'mama'],\n",
       " ['say',\n",
       "  'goodbye',\n",
       "  'forum',\n",
       "  'mat',\n",
       "  \"n't\",\n",
       "  'get',\n",
       "  'onehd',\n",
       "  'best',\n",
       "  'leak',\n",
       "  'spoiler'],\n",
       " [\"'m\",\n",
       "  'exhausted',\n",
       "  'time',\n",
       "  'bed',\n",
       "  'back',\n",
       "  'grind',\n",
       "  'first',\n",
       "  'thing',\n",
       "  'goodnight'],\n",
       " ['fml', 'run', 'fever', 'dizzy', 'achy', 'awesome', 'wrong', 'week', 'sick'],\n",
       " ['byerssss', 'j', 'tv'],\n",
       " ['omg', 'buy', 'backpack', 'amaze', 'love', 'soon', 'saw'],\n",
       " ['ooh', 'ouch', 'lovely', 'weather', 'damn', 'cover', 'sugar', 'x'],\n",
       " ['oh',\n",
       "  \"'ve\",\n",
       "  '3',\n",
       "  'half',\n",
       "  'hour',\n",
       "  'already',\n",
       "  'need',\n",
       "  'nap',\n",
       "  'could',\n",
       "  \"'m\",\n",
       "  'ill'],\n",
       " ['phoenix', 'petersburg', 'baltimore', 'else', 'end', 'summer'],\n",
       " [\"'re\", 'bilingual', 'kewl'],\n",
       " [\"y'arr\", 'go', 'u', 'work', 'tuesday', 'well', 'monday'],\n",
       " ['n',\n",
       "  'chris',\n",
       "  'really',\n",
       "  'want',\n",
       "  'get',\n",
       "  'nothing',\n",
       "  'way',\n",
       "  'responses',\n",
       "  'anyone',\n",
       "  'else'],\n",
       " ['im', 'ugh', 'im', 'derby'],\n",
       " ['thats',\n",
       "  'sick',\n",
       "  'yew',\n",
       "  'try',\n",
       "  'music',\n",
       "  'sound',\n",
       "  'like',\n",
       "  'play',\n",
       "  'run',\n",
       "  'way',\n",
       "  '202',\n",
       "  'update'],\n",
       " ['oh', 'read', 'confession', 'day'],\n",
       " ['sit', 'beach', 'right'],\n",
       " ['ca', \"n't\", 'sleep', 'without'],\n",
       " ['omg', 'farrah', 'fawcett', 'die', 'half', 'hour', 'ago'],\n",
       " ['dreamcast', 'win'],\n",
       " ['hair',\n",
       "  'go',\n",
       "  'tan',\n",
       "  'go',\n",
       "  'mall',\n",
       "  'get',\n",
       "  'check',\n",
       "  'cash',\n",
       "  'ashleys',\n",
       "  'nite'],\n",
       " ['liken', 'nepa', 'weather', 'week'],\n",
       " ['real', 'work', 'grrrr', 'like', 'say', 'wish', 'still', 'vacation', 'damn'],\n",
       " ['oh',\n",
       "  'hell',\n",
       "  'yes',\n",
       "  'dude',\n",
       "  'england',\n",
       "  'seem',\n",
       "  'fun',\n",
       "  'band',\n",
       "  'seem',\n",
       "  'like',\n",
       "  'funny',\n",
       "  'english',\n",
       "  'comedian',\n",
       "  'russell',\n",
       "  'noel'],\n",
       " ['alice', 'download', 'need', 'new', 'drive', 'music', 'anyway'],\n",
       " ['good',\n",
       "  'morning',\n",
       "  \"'ve\",\n",
       "  'read',\n",
       "  'book',\n",
       "  'night',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'fuck',\n",
       "  'sleep',\n",
       "  'ever'],\n",
       " ['get', 'ready', 'dance'],\n",
       " [\"'s\", '3', '26am', 'tire', 'mood'],\n",
       " [\"n't\", 'understand', 'immuno'],\n",
       " ['grrrrrrrrr', 'dauphinfest', 'ticket', 'tim', 'mcgraw', 'show', 'go'],\n",
       " ['wish',\n",
       "  \"n't\",\n",
       "  'tire',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'finish',\n",
       "  'book',\n",
       "  'tonight',\n",
       "  'even',\n",
       "  'ipod',\n",
       "  'drug',\n",
       "  'tonight',\n",
       "  \"'s\",\n",
       "  'gon',\n",
       "  'na',\n",
       "  'die',\n",
       "  'soon'],\n",
       " ['emery', 'take', 'back', 'sunday', 'record', 'drop', 'tomorrow'],\n",
       " ['pray', \"'m\", 'face', 'one', 'hard', 'day', 'life'],\n",
       " ['uhm', 'thai', 'food', \"n't\", 'magic', 'w', 'spicy'],\n",
       " ['horrible', 'day'],\n",
       " ['assingments',\n",
       "  'bite',\n",
       "  'bit',\n",
       "  'miss',\n",
       "  'girlfirend',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'wait',\n",
       "  'transformer',\n",
       "  '2',\n",
       "  'july',\n",
       "  '1st',\n",
       "  'yay'],\n",
       " ['watch', 'jumbafund'],\n",
       " ['though',\n",
       "  'small',\n",
       "  'papercuts',\n",
       "  'hurt',\n",
       "  'like',\n",
       "  'mutha',\n",
       "  'get',\n",
       "  'one',\n",
       "  'finger'],\n",
       " ['ohh', \"'d\", 'cull', 'get', '3', 'gb'],\n",
       " ['get', 'ta', 'work', 'early', 'tonight'],\n",
       " ['thanks', 'boy', 'appreciate'],\n",
       " ['need', 'go', 'shoe', 'shopping', 'bad', 'need', 'heel'],\n",
       " ['help',\n",
       "  'quot',\n",
       "  'kasaysayan',\n",
       "  'ng',\n",
       "  'wika',\n",
       "  'quot',\n",
       "  'thingy',\n",
       "  'filipino',\n",
       "  'homework',\n",
       "  'tomorrow',\n",
       "  'college',\n",
       "  'suck',\n",
       "  'dont',\n",
       "  'internet',\n",
       "  'connection',\n",
       "  'home'],\n",
       " ['great',\n",
       "  'music',\n",
       "  'video',\n",
       "  'hour',\n",
       "  'come',\n",
       "  'vh1',\n",
       "  'fourish',\n",
       "  'night',\n",
       "  'love'],\n",
       " [\"'s\",\n",
       "  'late',\n",
       "  'amp',\n",
       "  \"'m\",\n",
       "  'even',\n",
       "  'party',\n",
       "  'drive',\n",
       "  'back',\n",
       "  'beeville',\n",
       "  'nephew',\n",
       "  'grad',\n",
       "  'party',\n",
       "  'twitter',\n",
       "  'save',\n",
       "  'life',\n",
       "  'keep',\n",
       "  'awake'],\n",
       " ['play', 'sexy', 'girl', 'aloud', 'plskthx', 'jen', 'hartlepool'],\n",
       " ['dmb', 'louise', 'btw', 'thel'],\n",
       " ['dog',\n",
       "  'tire',\n",
       "  'long',\n",
       "  'day',\n",
       "  'tomorrow',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'bring',\n",
       "  'go',\n",
       "  'bed',\n",
       "  'either',\n",
       "  'hurt',\n",
       "  'inside',\n",
       "  'right'],\n",
       " ['sad',\n",
       "  'cop',\n",
       "  'tellin',\n",
       "  'today',\n",
       "  'find',\n",
       "  'body',\n",
       "  'miss',\n",
       "  '19y',\n",
       "  'die',\n",
       "  'heroin',\n",
       "  'overdose'],\n",
       " ['good', 'morning', 'everybody'],\n",
       " ['hooray',\n",
       "  'take',\n",
       "  'bath',\n",
       "  'seriously',\n",
       "  'one',\n",
       "  'favorite',\n",
       "  'thing',\n",
       "  'world',\n",
       "  'besides',\n",
       "  'cartoon',\n",
       "  'food'],\n",
       " ['mz',\n",
       "  'duke',\n",
       "  'hazard',\n",
       "  'charlie',\n",
       "  \"'s\",\n",
       "  'angel',\n",
       "  'bombshell',\n",
       "  'put',\n",
       "  'really',\n",
       "  'good',\n",
       "  'fight',\n",
       "  'sad'],\n",
       " ['dont', 'know', 'whats', 'hurt', 'neck', 'knee', 'ankle'],\n",
       " ['real',\n",
       "  'strawberry',\n",
       "  'ice',\n",
       "  'cream',\n",
       "  'pink',\n",
       "  'ice',\n",
       "  'cream',\n",
       "  'jog',\n",
       "  'last',\n",
       "  'night',\n",
       "  'sound',\n",
       "  'like',\n",
       "  'obscene',\n",
       "  'phone',\n",
       "  'caller',\n",
       "  '4',\n",
       "  'lap'],\n",
       " ['oh',\n",
       "  'noooo',\n",
       "  \"'ll\",\n",
       "  'figure',\n",
       "  'something',\n",
       "  'oh',\n",
       "  'dear',\n",
       "  'wisdom',\n",
       "  'teeth',\n",
       "  'sound',\n",
       "  'like',\n",
       "  'drag'],\n",
       " ['need',\n",
       "  'long',\n",
       "  'vacation',\n",
       "  'husband',\n",
       "  'thing',\n",
       "  'vacation',\n",
       "  'always',\n",
       "  'come',\n",
       "  'back'],\n",
       " ['aww', \"n't\", 'really', 'drink'],\n",
       " ['oh', 'twitter', 'miss', 'im', 'go', 'agree', 'ily', 'queeny'],\n",
       " ['watch',\n",
       "  'hero',\n",
       "  'season1',\n",
       "  'box',\n",
       "  'set',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'wait',\n",
       "  'season',\n",
       "  '4',\n",
       "  'start',\n",
       "  'season',\n",
       "  '3',\n",
       "  'release',\n",
       "  'dvd'],\n",
       " ['stay', 'much', 'longer', \"'s\", 'tea', 'hot', 'butter', 'toast'],\n",
       " ['like', 'cool', 'awesome', 'person', 'world', 'love', 'bestest', 'friend'],\n",
       " ['today',\n",
       "  'officially',\n",
       "  'day',\n",
       "  'music',\n",
       "  'die',\n",
       "  'central',\n",
       "  'ny',\n",
       "  '105',\n",
       "  '5',\n",
       "  'kiss',\n",
       "  'fm',\n",
       "  'officially',\n",
       "  'go',\n",
       "  'air',\n",
       "  '7',\n",
       "  '30',\n",
       "  'morning'],\n",
       " ['oh', 'yeah', 'thats', 'right', 'get', 'back'],\n",
       " ['adopt', 'careful', 'combination', 'txt', 'speak', 'selective', 'omission'],\n",
       " ['10', '8', '1', '09'],\n",
       " ['lmao',\n",
       "  'shush',\n",
       "  'illness',\n",
       "  'come',\n",
       "  'joys',\n",
       "  'multiple',\n",
       "  'mouth',\n",
       "  'ulcer',\n",
       "  'think',\n",
       "  'come',\n",
       "  'back',\n",
       "  'heal'],\n",
       " ['thanks',\n",
       "  'marla',\n",
       "  'think',\n",
       "  'may',\n",
       "  'allergies',\n",
       "  'poor',\n",
       "  'little',\n",
       "  'man',\n",
       "  'miserable'],\n",
       " ['second', 'juuune'],\n",
       " ['people',\n",
       "  'dont',\n",
       "  'stop',\n",
       "  'talk',\n",
       "  'cooky',\n",
       "  \"'m\",\n",
       "  'gon',\n",
       "  'na',\n",
       "  'need',\n",
       "  'eat',\n",
       "  'another',\n",
       "  'row',\n",
       "  'oreo',\n",
       "  'work'],\n",
       " [\"n't\", 'get', 'carded', 'drink', 'saddens'],\n",
       " ['wan', 'na', 'go', 'party'],\n",
       " ['miss', 'really'],\n",
       " ['oops', 'mean', 'complete', 'drive', 'theatre', 'western', '240'],\n",
       " ['nooo', 'dont', 'juggler', \"'re\", 'voice', 'nice'],\n",
       " ['hahahahahaha', 'ok', 'great', 'cool', \"n't\", 'hate', \"n't\", 'hate'],\n",
       " ['one'],\n",
       " ['yes',\n",
       "  \"'re\",\n",
       "  'quot',\n",
       "  'nokia',\n",
       "  'quot',\n",
       "  'new',\n",
       "  'york',\n",
       "  'think',\n",
       "  'nokia',\n",
       "  'l',\n",
       "  'live'],\n",
       " [\"n't\",\n",
       "  'watch',\n",
       "  'basketball',\n",
       "  'lately',\n",
       "  'every',\n",
       "  'since',\n",
       "  'bull',\n",
       "  'lose',\n",
       "  'hope',\n",
       "  'lakers',\n",
       "  \"n't\",\n",
       "  'win'],\n",
       " ['go',\n",
       "  'first',\n",
       "  'page',\n",
       "  'sanskrit',\n",
       "  'text',\n",
       "  'translation',\n",
       "  'english',\n",
       "  'bloody',\n",
       "  'horrible',\n",
       "  'egrr',\n",
       "  'suck'],\n",
       " ['sit', 'starbucks', 'nice', 'caramel', 'frap', \"'\", 'good', 'time'],\n",
       " ['good', 'know', 'dont', 'swine', 'flu', 'krudd'],\n",
       " ['try', 'get', 'fuck', 'data', 'back', 'power', 'go'],\n",
       " ['kinda', 'bum', 'missed', 'copeland', 'day'],\n",
       " ['meet', 'lady', 'lunch'],\n",
       " ['get',\n",
       "  'arrest',\n",
       "  'swing',\n",
       "  'sword',\n",
       "  'around',\n",
       "  'top',\n",
       "  'school',\n",
       "  'innuendo',\n",
       "  'real',\n",
       "  'sword',\n",
       "  'long',\n",
       "  'story'],\n",
       " [\"'m\", 'go', 'die', 'keep', 'try', 'take', 'picture', 'fml', '120'],\n",
       " ['endure',\n",
       "  'trip',\n",
       "  'doc',\n",
       "  'today',\n",
       "  'decide',\n",
       "  'tonsillitis',\n",
       "  'struck',\n",
       "  'time',\n",
       "  'see',\n",
       "  'name',\n",
       "  'light'],\n",
       " ['back',\n",
       "  'shop',\n",
       "  'bake',\n",
       "  'father',\n",
       "  \"'s\",\n",
       "  'day',\n",
       "  'listen',\n",
       "  'jb',\n",
       "  'still',\n",
       "  'haha',\n",
       "  'still',\n",
       "  'wish',\n",
       "  'could',\n",
       "  'see',\n",
       "  'summer'],\n",
       " ['thanks',\n",
       "  'hope',\n",
       "  \"n't\",\n",
       "  'really',\n",
       "  'flu',\n",
       "  'hope',\n",
       "  'feel',\n",
       "  'well',\n",
       "  'soon',\n",
       "  \"n't\",\n",
       "  'get'],\n",
       " ['occur',\n",
       "  'see',\n",
       "  'might',\n",
       "  'little',\n",
       "  'creepy',\n",
       "  'since',\n",
       "  \"'s\",\n",
       "  'sorta',\n",
       "  'kinda',\n",
       "  'kid',\n",
       "  'movie'],\n",
       " ['btw',\n",
       "  'good',\n",
       "  'morning',\n",
       "  'great',\n",
       "  'day',\n",
       "  'everyone',\n",
       "  'oh',\n",
       "  'yeah',\n",
       "  'fire',\n",
       "  'pratt'],\n",
       " [\"'m\", 'stuck', 'lie', 'week', 'solid', 'count', 'lucky'],\n",
       " ['thanks', 'lot', 'help'],\n",
       " ['want', 'see', 'terminator', 'everyone', \"'s\", 'already', 'see'],\n",
       " ['oh', \"n't\", 'think', \"'ll\", 'look', 'picspams', 'thanks', 'tip', 'saul'],\n",
       " [\"'s\", 'rain', 'stepped', 'house', \"'m\", 'figure', 'use', 'reader'],\n",
       " ['oh',\n",
       "  'beg',\n",
       "  'add',\n",
       "  '3rd',\n",
       "  'nite',\n",
       "  'dublin',\n",
       "  'october',\n",
       "  'pleeeeeeeassseeeeeee'],\n",
       " ['fracture', 'clinic', 'wait', 'room', 'jeremy', 'kyle', 'tv'],\n",
       " [\"n't\",\n",
       "  'sound',\n",
       "  'pleasant',\n",
       "  'haha',\n",
       "  'sound',\n",
       "  'like',\n",
       "  'thing',\n",
       "  'right',\n",
       "  'track',\n",
       "  'new',\n",
       "  'stuff',\n",
       "  \"'s\",\n",
       "  'great',\n",
       "  'see'],\n",
       " ['give', 'hiccup', 'jerkface'],\n",
       " ['yo', 'thanks', 'head', 'might', 'chelyabinsk', 'weekend'],\n",
       " ['see', 'dead', 'cant', 'find', 'trakkies'],\n",
       " ['swear',\n",
       "  'stress',\n",
       "  'level',\n",
       "  'way',\n",
       "  'high',\n",
       "  'might',\n",
       "  'stroke',\n",
       "  'heart',\n",
       "  'attack',\n",
       "  'epilectic',\n",
       "  'fit',\n",
       "  'death',\n",
       "  'soon'],\n",
       " ['wow', 'go', 'fast', 'thanks', 'write', 'article', 'come', 'great'],\n",
       " ['mer', 'get', 'form', 'awredy', 'thanx'],\n",
       " ['dont', 'throw', 'miley'],\n",
       " [\"n't\", 'remember', 'episode', \"'s\", 'see', 'friend'],\n",
       " ['fine', 'today', 'mor', 'talk', 'haha', 'lol', 'xx'],\n",
       " ['wish', 'home', 'computer', 'nice', 'work', 'machine'],\n",
       " [\"'s\", 'enough', 'room'],\n",
       " ['even', \"'d\", 'tweet'],\n",
       " ['buy', 'magazine'],\n",
       " [],\n",
       " ['southwest',\n",
       "  'sooo',\n",
       "  'fail',\n",
       "  'flight',\n",
       "  'delay',\n",
       "  'oversold',\n",
       "  'next',\n",
       "  'available',\n",
       "  'tomorrow',\n",
       "  'forget'],\n",
       " ['little', 'dance', 'get', 'fossil', 'job', 'rum', 'around'],\n",
       " ['probably', 'wo', \"n't\", 'anything', 'eat'],\n",
       " ['yes', 'god', 'yes', 'looooooooooooooooooooool', 'need', 'forum'],\n",
       " ['lol',\n",
       "  'nooo',\n",
       "  \"'ve\",\n",
       "  'month',\n",
       "  'way',\n",
       "  'bad',\n",
       "  'attn',\n",
       "  'span',\n",
       "  'ok',\n",
       "  'maybe',\n",
       "  'watch'],\n",
       " ['already', 'work', 'drink', 'beer', 'wait', \"'m\"],\n",
       " ['lol',\n",
       "  'ask',\n",
       "  'womp',\n",
       "  'womp',\n",
       "  'lol',\n",
       "  'problem',\n",
       "  'ooooooh',\n",
       "  'well',\n",
       "  'dont',\n",
       "  'black',\n",
       "  'berry',\n",
       "  'suck'],\n",
       " ['ca',\n",
       "  \"n't\",\n",
       "  'believe',\n",
       "  'friday',\n",
       "  'finally',\n",
       "  'time',\n",
       "  'spend',\n",
       "  'time',\n",
       "  'friend',\n",
       "  'family',\n",
       "  'relax'],\n",
       " ['hate'],\n",
       " ['sad',\n",
       "  'farrah',\n",
       "  'pray',\n",
       "  \"n't\",\n",
       "  'vocabulary',\n",
       "  'thinking',\n",
       "  'partner',\n",
       "  'son',\n",
       "  'family'],\n",
       " ['siri',\n",
       "  'downtown',\n",
       "  'never',\n",
       "  'hear',\n",
       "  'upgrade',\n",
       "  'phone',\n",
       "  'man',\n",
       "  'want',\n",
       "  'blackberry',\n",
       "  'though',\n",
       "  'still',\n",
       "  'drive'],\n",
       " ['energetic', 'much', 'kinda', 'excite'],\n",
       " ['lose',\n",
       "  'follower',\n",
       "  'well',\n",
       "  'coz',\n",
       "  'andy',\n",
       "  'hurley',\n",
       "  'day',\n",
       "  'say',\n",
       "  'sorry',\n",
       "  'pain',\n",
       "  'as',\n",
       "  'tweet'],\n",
       " ['need',\n",
       "  'rain',\n",
       "  'complain',\n",
       "  'last',\n",
       "  'week',\n",
       "  'serious',\n",
       "  'sleeping',\n",
       "  'problem'],\n",
       " ['sloooow', 'today', 'go', 'see', \"'s\"],\n",
       " ['saw', 'best', 'friend', 'amp', 'peer', 'graduate', 'lovely'],\n",
       " ['relisted', 'expire', 'list', 'bette', 'midler', 'record', 'bowl', 'ets'],\n",
       " ['forgiveness',\n",
       "  'move',\n",
       "  'forward',\n",
       "  'really',\n",
       "  'want',\n",
       "  'regardless',\n",
       "  'past',\n",
       "  'wrong',\n",
       "  'happy',\n",
       "  'move',\n",
       "  'forward',\n",
       "  'today'],\n",
       " [\"'re\", 'twitter', 'addict', 'friend'],\n",
       " ['case', \"'re\", 'wonder', 'rachel', 'alexandra', 'horse'],\n",
       " ['go', 'back', 'hospital', 'surgery', 'monday', 'hop', 'cancer'],\n",
       " ['may', 'get', 'another', 'head'],\n",
       " ['full', 'win', 'awesome', 'share', 'usual', 'reason'],\n",
       " [\"'s\", 'get'],\n",
       " ['wish',\n",
       "  'could',\n",
       "  'go',\n",
       "  'tonight',\n",
       "  'vacay',\n",
       "  'ovr',\n",
       "  'get',\n",
       "  'ta',\n",
       "  'wrk',\n",
       "  'morn'],\n",
       " ['interview', 'friday', 'woohoooo', 'wish', 'luck'],\n",
       " ['bet',\n",
       "  'wikipedia',\n",
       "  'google',\n",
       "  'take',\n",
       "  'hammer',\n",
       "  \"'torticollis\",\n",
       "  \"'\",\n",
       "  'poor',\n",
       "  'girl'],\n",
       " ['really', 'hope', 'real'],\n",
       " ['hi', \"'m\", 'test', 'quot', 'twitter', 'quot'],\n",
       " ['first', 'day', 'work', 'pink', 'eye', 'wish', 'luckkkk'],\n",
       " ['school', '6', 'subject', 'leave', 'today'],\n",
       " [\"'s\", '26degrees', 'glasgow', 'hell', 'get', 'cold', \"'s\", 'fair'],\n",
       " ['think',\n",
       "  \"'s\",\n",
       "  'pretty',\n",
       "  'cool',\n",
       "  'nar',\n",
       "  'mac',\n",
       "  'friendly',\n",
       "  'great',\n",
       "  'machine'],\n",
       " ['see',\n",
       "  'em',\n",
       "  'yet',\n",
       "  'ppl',\n",
       "  'use',\n",
       "  'twitch',\n",
       "  '4',\n",
       "  'bitch',\n",
       "  'tick',\n",
       "  'crazy',\n",
       "  'person',\n",
       "  'amp',\n",
       "  \"'m\",\n",
       "  'confirmed',\n",
       "  'insomniac',\n",
       "  'w',\n",
       "  'limit',\n",
       "  'ambien'],\n",
       " ['im', 'sry', 'btw', 'wut', 'every', 'happend', 'grape'],\n",
       " ['blackberry', 'deff', 'donzo', 'twit', 'txt', 'use', 'net'],\n",
       " ['hey',\n",
       "  'mark',\n",
       "  'use',\n",
       "  'volunteer',\n",
       "  'actionaid',\n",
       "  'uk',\n",
       "  'would',\n",
       "  'love',\n",
       "  'hear',\n",
       "  'guy'],\n",
       " ['oh', 'see', 'sorry', 'side', 'one', 'week', 'til', 'see'],\n",
       " ['oh', 'way', \"'m\", 'get', 'fat', \"'m\", 'still', 'lose'],\n",
       " ['never',\n",
       "  'mind',\n",
       "  'chance',\n",
       "  'could',\n",
       "  'pick',\n",
       "  'em',\n",
       "  'p',\n",
       "  'want',\n",
       "  'go',\n",
       "  'rowan',\n",
       "  \"'s\",\n",
       "  'birthday',\n",
       "  'party',\n",
       "  '2moz',\n",
       "  'x'],\n",
       " ['look',\n",
       "  'forward',\n",
       "  'apprentice',\n",
       "  'final',\n",
       "  'big',\n",
       "  'brother',\n",
       "  'tonight',\n",
       "  'think',\n",
       "  'need',\n",
       "  'get'],\n",
       " ['concerned', 'computer', \"'s\", 'take', 'awful', 'long', 'time', 'boot'],\n",
       " ['mosquito', 'bite', 'everywhere', 'last', 'week'],\n",
       " ['rp',\n",
       "  'tz',\n",
       "  'aw',\n",
       "  'canada',\n",
       "  'rob',\n",
       "  \"'s\",\n",
       "  'birthday',\n",
       "  '2',\n",
       "  'end',\n",
       "  '3',\n",
       "  'min',\n",
       "  'noooooooo',\n",
       "  'well',\n",
       "  'nikki',\n",
       "  \"'s\",\n",
       "  'birthday',\n",
       "  'sunday'],\n",
       " [\"'s\", 'go', 'go', 'go', 'didnt', 'expect', 'soon'],\n",
       " ['crappy', 'friend', 'family', 'stupidity'],\n",
       " ['realy', 'realy', 'realy', 'want', 'sleep', 'cant', 'sad', 'night'],\n",
       " [\"'m\", 'block', 'party', 'get', 'loose', 'lol', 'wat', 'u', 'doin', 'today'],\n",
       " ['go',\n",
       "  'sit',\n",
       "  'home',\n",
       "  'absolutely',\n",
       "  'nothing',\n",
       "  'today',\n",
       "  'sound',\n",
       "  'like',\n",
       "  'good',\n",
       "  'plan'],\n",
       " [\"'s\", 'wrong', 'grammar'],\n",
       " ['enjoy', \"'s\", 'raining'],\n",
       " ['sooo', 'tire', 'feel', 'bit', 'rough', 'anol', 'good', 'night'],\n",
       " ['mmm',\n",
       "  'still',\n",
       "  'one',\n",
       "  'leave',\n",
       "  'actually',\n",
       "  'forgot',\n",
       "  'ate',\n",
       "  'whole',\n",
       "  'thing',\n",
       "  'feel',\n",
       "  'bad'],\n",
       " ['ur',\n",
       "  'stun',\n",
       "  'girl',\n",
       "  'planet',\n",
       "  'amp',\n",
       "  'love',\n",
       "  'lt',\n",
       "  '3',\n",
       "  'skye',\n",
       "  'way',\n",
       "  'australia',\n",
       "  'call',\n",
       "  'yesterday'],\n",
       " ['let', 'know'],\n",
       " ['thanxxx'],\n",
       " ['fail', 'sharon', 'fail'],\n",
       " ['merge', 'csproj', 'file', 'hand', 'get', 'hold', 'merges'],\n",
       " ['math', 'graphic', 'exam', 'tomorrowzzz'],\n",
       " ['always', 'ruin', 'thing'],\n",
       " ['hey', 'hw', 'u'],\n",
       " [\"'ll\", 'get', 'soon', 'unless', 'want', 'one', 'friend', 'macarena'],\n",
       " ['um',\n",
       "  'thank',\n",
       "  'think',\n",
       "  'sure',\n",
       "  'meant',\n",
       "  'direct',\n",
       "  'compliment',\n",
       "  'katherine',\n",
       "  'heighl',\n",
       "  'blond',\n",
       "  'thank',\n",
       "  'u',\n",
       "  'anyway'],\n",
       " ['want', 'bbq', \"'s\", 'really', 'nice', 'aswell', 'xxx'],\n",
       " ['im', 'hunger', 'update', 'oh', 'please', 'tabi', 'please', \"'re\", 'kill'],\n",
       " ['well', \"'ve\", 'get', 'cold', 'everyone'],\n",
       " ['ferry', 'isle', 'white', 'great', 'week', 'miss', 'tinternet', 'though'],\n",
       " ['yay', 'sound', 'really', 'good', 'midnight', 'reason', 'still'],\n",
       " [\"'s\", 'sad'],\n",
       " ['well',\n",
       "  'pier',\n",
       "  'say',\n",
       "  'wanted',\n",
       "  'win',\n",
       "  'performance',\n",
       "  'crowd',\n",
       "  'booed',\n",
       "  'rather',\n",
       "  'harsh'],\n",
       " ['lt',\n",
       "  'hungry',\n",
       "  'crave',\n",
       "  'chicken',\n",
       "  'noodle',\n",
       "  'soup',\n",
       "  'soup',\n",
       "  'general',\n",
       "  'think',\n",
       "  'lot',\n",
       "  'dreary',\n",
       "  'weather',\n",
       "  'wa'],\n",
       " ['think', \"'s\", 'wrap', 'pc', 'wo', \"n't\", 'let', 'download', 'nothing'],\n",
       " ['go', 'pick', 'dash'],\n",
       " ['ah', 'friday', 'best'],\n",
       " ['lance', 'u', 'pick', 'u'],\n",
       " ['buldum', 'buldummmmmmm'],\n",
       " ['recommendation',\n",
       "  'private',\n",
       "  'health',\n",
       "  'cover',\n",
       "  'extras',\n",
       "  'chip',\n",
       "  'tooth',\n",
       "  'also',\n",
       "  'want',\n",
       "  'sorted',\n",
       "  'tax',\n",
       "  'time'],\n",
       " ['ohh', 'poor', 'meredith', 'grey'],\n",
       " ['please', \"n't\", 'spend', 'money', 'silly', 'parade'],\n",
       " ['butbut', 'get', 'online'],\n",
       " ['love', '4', 'little', 'casper'],\n",
       " ['yaaaaaaaay', 'vote', 'like', 'million', 'fan', 'also', 'whatever'],\n",
       " ['good',\n",
       "  'morning',\n",
       "  \"'s\",\n",
       "  '7',\n",
       "  '46',\n",
       "  'ready',\n",
       "  'go',\n",
       "  'school',\n",
       "  'philosophy',\n",
       "  'test',\n",
       "  'thanks'],\n",
       " ['yeah', 'mine', 'way', 'retirement'],\n",
       " ['sorry',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'see',\n",
       "  'toy',\n",
       "  'story',\n",
       "  'come',\n",
       "  'blu',\n",
       "  'ray',\n",
       "  'think',\n",
       "  'monster',\n",
       "  'inc',\n",
       "  'lol'],\n",
       " [\"'s\",\n",
       "  'amazing',\n",
       "  'news',\n",
       "  'people',\n",
       "  'outside',\n",
       "  'state',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'vote',\n",
       "  'though',\n",
       "  'good',\n",
       "  'luck',\n",
       "  'new',\n",
       "  'zealand'],\n",
       " ['screen', 'blurry', 'good'],\n",
       " ['red', 'button', 'luckily', \"'s\", 'live', 'web'],\n",
       " ['homework', 'want', 'bed', 'badd', 'grrrr'],\n",
       " ['aaaaahhhh',\n",
       "  'spacebar',\n",
       "  'silly',\n",
       "  'stuck',\n",
       "  'spanish',\n",
       "  'lesson',\n",
       "  'nice',\n",
       "  'lol',\n",
       "  'cool',\n",
       "  'post',\n",
       "  \"'m\",\n",
       "  'gon',\n",
       "  'na',\n",
       "  'use',\n",
       "  '140',\n",
       "  '9',\n",
       "  '7',\n",
       "  '5',\n",
       "  '3'],\n",
       " ['good', 'night', 'see', 'ya', '6am'],\n",
       " ['omg', 'im', 'bout', 'late', 'last', 'day', 'school', 'get', 'ta', 'rush'],\n",
       " ['still', 'love', 'music'],\n",
       " ['go',\n",
       "  'awhile',\n",
       "  'computer',\n",
       "  'get',\n",
       "  'fried',\n",
       "  'night',\n",
       "  'storm',\n",
       "  'useing',\n",
       "  'friend'],\n",
       " ['need', 'study', 'exam'],\n",
       " ['yey', \"'m\", 'happie', 'papa', 'thanks', 'bro', 'one', 'one', 'go'],\n",
       " ['blk',\n",
       "  'ppl',\n",
       "  'always',\n",
       "  'fuckin',\n",
       "  'plan',\n",
       "  'gon',\n",
       "  'go',\n",
       "  'bowling',\n",
       "  'wth',\n",
       "  'ima',\n",
       "  '2nite',\n",
       "  'loft',\n",
       "  'hmm',\n",
       "  'probably',\n",
       "  'go',\n",
       "  'lumiere',\n",
       "  'smh',\n",
       "  'dnt',\n",
       "  'gamble'],\n",
       " ['imagine', 'tonight', '2', 'month', 'straight', 'welcome', 'natstown'],\n",
       " [],\n",
       " ['holy',\n",
       "  'crap',\n",
       "  'slimthug',\n",
       "  'im',\n",
       "  'follow',\n",
       "  'twitter',\n",
       "  'right',\n",
       "  'say',\n",
       "  'im',\n",
       "  'boat',\n",
       "  'lt',\n",
       "  '3',\n",
       "  'perfection',\n",
       "  'damn',\n",
       "  'twitter',\n",
       "  'get',\n",
       "  'fuck'],\n",
       " ['hope', \"n't\", 'forget', 'work', 'part', 'get', 'come', 'two', 'day'],\n",
       " ['wow',\n",
       "  'nothing',\n",
       "  'ruin',\n",
       "  'good',\n",
       "  'time',\n",
       "  'pool',\n",
       "  'quicker',\n",
       "  'leaky',\n",
       "  'diaper',\n",
       "  'yuck'],\n",
       " ['pipe', 'good', 'lord', 'kind', 'specimen', 'man', 'marry', 'xo'],\n",
       " ['might',\n",
       "  'make',\n",
       "  'clug',\n",
       "  'install',\n",
       "  'party',\n",
       "  \"'ve\",\n",
       "  '2',\n",
       "  'music',\n",
       "  'cancellation',\n",
       "  'back',\n",
       "  'back'],\n",
       " ['goin', 'home', 'thirty'],\n",
       " ['saw', 'audition', 'last', 'night', 'great', 'show', 'ict', 'revision'],\n",
       " ['yuppieee',\n",
       "  'doodoooo',\n",
       "  'haha',\n",
       "  'yeh',\n",
       "  'enjoy',\n",
       "  'rest',\n",
       "  'week',\n",
       "  'seeyaaaa',\n",
       "  'soon'],\n",
       " ['ate', 'mi', 'goreng', 'put', 'much', 'chilli', 'mouth', 'burn'],\n",
       " ['gym', 'saturday', 'morning', 'run'],\n",
       " ['lol', 'well', 'cant', 'beat', 'u', 'wats', 'uppp', 'wats', 'happen'],\n",
       " ['still', 'love', 'stuff', 'though', 'never', 'see', 'honeytint', 'good'],\n",
       " ['awesome', 'time', 'telus', 'walk', 'cure', 'diabetes', 'yesterday'],\n",
       " ['aww', 'poor', 'u', 'least', 'twitter', 'part', 'still', 'work', 'right'],\n",
       " ['oh',\n",
       "  'realize',\n",
       "  'forgot',\n",
       "  'pay',\n",
       "  'credit',\n",
       "  'card',\n",
       "  'bill',\n",
       "  'one',\n",
       "  \"'s\",\n",
       "  'overdue',\n",
       "  'hate',\n",
       "  'pay',\n",
       "  'late',\n",
       "  'fee',\n",
       "  'waste'],\n",
       " ['know', 'long', 'ur', 'gon', 'na', 'wait'],\n",
       " ['quot', 'climb', 'quot', 'love', 'song'],\n",
       " ['26', 'week', 'today'],\n",
       " ['adian', 'davis', 'soo', 'hot'],\n",
       " ['realise',\n",
       "  'lame',\n",
       "  'conversation',\n",
       "  'weather',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'geeky',\n",
       "  'neighbour',\n",
       "  'stalk'],\n",
       " ['pusing', 'pusing', 'pusing', 'mual', 'mual', 'mual'],\n",
       " ['xavier',\n",
       "  'come',\n",
       "  'kl',\n",
       "  'last',\n",
       "  'week',\n",
       "  'bad',\n",
       "  'schedule',\n",
       "  'already',\n",
       "  'meet',\n",
       "  'pg',\n",
       "  'nice',\n",
       "  'organize',\n",
       "  'gathering',\n",
       "  '4'],\n",
       " ['go', 'marche', 'big', 'smile'],\n",
       " ['nothing', 'soft', 'yogging'],\n",
       " ['go', '2nd', 'gym', 'induction'],\n",
       " ['knew',\n",
       "  'man',\n",
       "  'dream',\n",
       "  'would',\n",
       "  'bald',\n",
       "  'toothless',\n",
       "  'amp',\n",
       "  'chubby',\n",
       "  'spending',\n",
       "  'time',\n",
       "  'amazing',\n",
       "  'kid'],\n",
       " ['also', 'sadly', 'remind', 'luc', 'burdon', 'today'],\n",
       " ['get', 'website'],\n",
       " ['robert', 'pack', 'surgery', 'prepare', 'week', 'without', 'brother'],\n",
       " ['crackberry', 'figure', 'follow', 'someone'],\n",
       " ['haha', 'yes', 'youw', 'atched', 'awesome'],\n",
       " [\"'s\",\n",
       "  'horrible',\n",
       "  'feeling',\n",
       "  'want',\n",
       "  'help',\n",
       "  'someone',\n",
       "  'know',\n",
       "  'fact',\n",
       "  'there',\n",
       "  'nothing',\n",
       "  'absolutely'],\n",
       " ['igual', 'amie', 'much'],\n",
       " ['work', 'unfortunately', 'leave', 'chugar'],\n",
       " ['dont',\n",
       "  'like',\n",
       "  'like',\n",
       "  'thought',\n",
       "  \"n't\",\n",
       "  'know',\n",
       "  'tell',\n",
       "  'fml',\n",
       "  'always',\n",
       "  'happen'],\n",
       " [\"n't\", 'think', \"'ve\", 'get', 'energy', 'get', 'bed'],\n",
       " ['soo', 'tire', 'kant', 'fall', 'asleep', 'without', 'special', 'goodnight'],\n",
       " ['back',\n",
       "  'trusty',\n",
       "  'thinkpad',\n",
       "  'r50',\n",
       "  'wait',\n",
       "  'woot',\n",
       "  'com',\n",
       "  'take',\n",
       "  'nearly',\n",
       "  '2',\n",
       "  'wks',\n",
       "  'ship',\n",
       "  'rma',\n",
       "  'laptop',\n",
       "  'woohoo'],\n",
       " ['thank', 'shout', 'follow'],\n",
       " ['thats', \"'m\", 'afraid'],\n",
       " ['fowl'],\n",
       " ['good',\n",
       "  'morning',\n",
       "  'ny',\n",
       "  'tweeter',\n",
       "  'wake',\n",
       "  'n',\n",
       "  'good',\n",
       "  'thing',\n",
       "  'alarm',\n",
       "  \"n't\",\n",
       "  'go',\n",
       "  'oops'],\n",
       " ['matter', 'good', 'friend', 'never', 'someone', 'beside'],\n",
       " ['6', 'day', 'go', 'disney'],\n",
       " ['aw', 'miss', 'miss', 'praise', 'worship', 'sure', 'sunday'],\n",
       " ['listens', 'much', 'jeff', 'hmph'],\n",
       " ['good', 'job', 'congratzzzz'],\n",
       " ['thanks',\n",
       "  'drea',\n",
       "  'good',\n",
       "  'hear',\n",
       "  'morning',\n",
       "  'twitter',\n",
       "  \"'m\",\n",
       "  'still',\n",
       "  'try',\n",
       "  'learn',\n",
       "  'reply',\n",
       "  'message',\n",
       "  'later'],\n",
       " [\"'s\", 'great', 'really', 'enjoy', 'course', 'love', 'liverpool', 'general'],\n",
       " ['smoke', 'restore', 'lto', 'take', 'hour', 'seek', 'block', '3895458'],\n",
       " ['gettin', 'sleepy', 'bad', 'bed', 'time', 'without', 'charlie'],\n",
       " ['fashion', 'show', 'empire', 'next', 'wed', 'wish', 'see'],\n",
       " ['yo',\n",
       "  'as',\n",
       "  'get',\n",
       "  'ta',\n",
       "  'come',\n",
       "  'get',\n",
       "  'greyhound',\n",
       "  'cant',\n",
       "  'put',\n",
       "  'mile',\n",
       "  'didi'],\n",
       " ['damn', 'sleep'],\n",
       " ['love',\n",
       "  'weather',\n",
       "  'never',\n",
       "  'sunny',\n",
       "  'manchester',\n",
       "  'woooo',\n",
       "  'sooooo',\n",
       "  'excite',\n",
       "  'signed',\n",
       "  'college',\n",
       "  'tomorrow',\n",
       "  'xx'],\n",
       " ['stoy'],\n",
       " ['unable',\n",
       "  'deny',\n",
       "  'fact',\n",
       "  'suffer',\n",
       "  'recede',\n",
       "  'hair',\n",
       "  'line',\n",
       "  'bad',\n",
       "  'part',\n",
       "  'get',\n",
       "  'monthly',\n",
       "  'hair',\n",
       "  'cut',\n",
       "  'haha'],\n",
       " [\"'s\", 'rain', '3', 'day', 'vacation', 'swim', 'chippewa', 'river'],\n",
       " ['great',\n",
       "  'show',\n",
       "  'want',\n",
       "  'em',\n",
       "  'run',\n",
       "  'rain',\n",
       "  'hug',\n",
       "  'dad',\n",
       "  'say',\n",
       "  'goodbye'],\n",
       " ['woo',\n",
       "  'convince',\n",
       "  'buddy',\n",
       "  'jon',\n",
       "  'buy',\n",
       "  'coffee',\n",
       "  'tomorrow',\n",
       "  'morning',\n",
       "  'head',\n",
       "  'morrison'],\n",
       " ['true', 'life', 'quot', 'im', 'addict', 'crystal', 'meth', 'quot', 'mtv'],\n",
       " ['get',\n",
       "  'shirt',\n",
       "  'rock',\n",
       "  'excite',\n",
       "  'sport',\n",
       "  'mine',\n",
       "  'plane',\n",
       "  'ride',\n",
       "  'tomorrow'],\n",
       " ['ugh', 'hate', \"n't\", 'caple', 'anymore'],\n",
       " ['wan', 'na', 'come'],\n",
       " ['see',\n",
       "  'feel',\n",
       "  'sportscenter',\n",
       "  'football',\n",
       "  'season',\n",
       "  'would',\n",
       "  'rather',\n",
       "  'see',\n",
       "  'baseball'],\n",
       " [\"'ve\",\n",
       "  'get',\n",
       "  'fucking',\n",
       "  'work',\n",
       "  'tonight',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'play',\n",
       "  'still',\n",
       "  \"n't\",\n",
       "  'play'],\n",
       " ['find',\n",
       "  'within',\n",
       "  'second',\n",
       "  'although',\n",
       "  \"n't\",\n",
       "  'like',\n",
       "  'something',\n",
       "  'like',\n",
       "  'thunderstuck',\n",
       "  'well',\n",
       "  'cheesy',\n",
       "  'haha'],\n",
       " ['monkey', 'could', 'job', 'go', 'build', 'fort', 'later'],\n",
       " ['happy', 'hump', 'day', 'everyone', 'fun', 'parade', 'gota', 'work'],\n",
       " ['haha', \"'s\", \"'re\", 'order', 'food', 'p'],\n",
       " ['excel', 'love', \"'m\", 'go', 'master', 'someday'],\n",
       " ['u', 'know', 'video', 'beautiful', 'come'],\n",
       " ['oki',\n",
       "  'doki',\n",
       "  'headin',\n",
       "  'z',\n",
       "  \"'s\",\n",
       "  'head',\n",
       "  'explode',\n",
       "  'nighty',\n",
       "  'night',\n",
       "  'c',\n",
       "  'ya',\n",
       "  'tomorro',\n",
       "  'ive',\n",
       "  'soakd',\n",
       "  'sun'],\n",
       " ['remember',\n",
       "  'song',\n",
       "  'play',\n",
       "  'constantly',\n",
       "  'repeat',\n",
       "  'relize',\n",
       "  'kenzie',\n",
       "  'cant',\n",
       "  'take',\n",
       "  'away',\n",
       "  'amp',\n",
       "  'apart',\n",
       "  'long'],\n",
       " ['limit', 'story', '140', 'ch', 'tell', 'annoy', 'help', 'relieve', 'stress'],\n",
       " ['sulk', 'biby'],\n",
       " ['worry',\n",
       "  'noone',\n",
       "  'get',\n",
       "  'one',\n",
       "  'next',\n",
       "  'question',\n",
       "  'start',\n",
       "  '1',\n",
       "  'minute',\n",
       "  'get',\n",
       "  'thinking',\n",
       "  'cap'],\n",
       " ['oooo', 'look', 'get', 'pic', 'much', 'swimming', 'today'],\n",
       " ['want',\n",
       "  'blanka',\n",
       "  'visic',\n",
       "  \"'s\",\n",
       "  'body',\n",
       "  'hopefully',\n",
       "  'go',\n",
       "  'see',\n",
       "  'berlin',\n",
       "  'august'],\n",
       " ['go', 'iga', 'house'],\n",
       " ['tummy', 'hurt'],\n",
       " ['exhaust', 'sunburned', 'third', 'weekend', 'row'],\n",
       " ['cant',\n",
       "  'believe',\n",
       "  '3',\n",
       "  'year',\n",
       "  'still',\n",
       "  'make',\n",
       "  'knee',\n",
       "  'go',\n",
       "  'weak',\n",
       "  'need',\n",
       "  'dude'],\n",
       " ['u', 'gon', 'na', 'show', 'around'],\n",
       " ['sleepy', 'love', 'camp', 'sweetie'],\n",
       " ['talkin',\n",
       "  'baby',\n",
       "  'bout',\n",
       "  'hit',\n",
       "  'sack',\n",
       "  'dont',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'goin',\n",
       "  'ish',\n",
       "  'bext',\n",
       "  'four',\n",
       "  'day'],\n",
       " ['noooo', 'fuckin', 'waaay'],\n",
       " ['w0', '0', 'cant', 'wait'],\n",
       " ['google', 'search', 'return', 'result', 'suspect', 'spam'],\n",
       " ['apt',\n",
       "  'florence',\n",
       "  'already',\n",
       "  'book',\n",
       "  'search',\n",
       "  'another',\n",
       "  'apt',\n",
       "  'room',\n",
       "  'flat'],\n",
       " ['feel', 'super', 'sick'],\n",
       " ['get', 'ta', 'clean', 'today', 'hope', 'go', 'lunch'],\n",
       " ['throat', 'hurt', 'good', 'come', 'today', 'ask', 'detail'],\n",
       " ['get', 'burnt'],\n",
       " ['need', 'one', 'backyard', 'gangsta'],\n",
       " ['make', 'lol'],\n",
       " ['guy', 'mail', 'back', 'mgmt', 'r18'],\n",
       " [],\n",
       " [\"'s\", 'leave', 'july', '3rd', 'go', 'home', 'five', 'week', 'noooo'],\n",
       " ['welcome',\n",
       "  'thank',\n",
       "  'sad',\n",
       "  'ta',\n",
       "  'ta',\n",
       "  'tailgate',\n",
       "  'may',\n",
       "  'happen',\n",
       "  'scranton'],\n",
       " ['drop', 'melisa'],\n",
       " ['goodnght',\n",
       "  'tweeties',\n",
       "  \"n't\",\n",
       "  'pay',\n",
       "  'attention',\n",
       "  'anything',\n",
       "  'write',\n",
       "  'byeeeeeeeeee'],\n",
       " ['oh', 'yeh', 'like', 'lol', 'man', 'believe', 'everything', 'read', 'paper'],\n",
       " ['andriod'],\n",
       " [\"'m\", 'confused', 'twitter', 'haha'],\n",
       " ['sound', 'like', \"'re\", 'bad', 'travel', 'day'],\n",
       " ['really', 'dig', 'spur', 'moment', 'plan'],\n",
       " [\"'ve\",\n",
       "  'stay',\n",
       "  'night',\n",
       "  'actually',\n",
       "  'night',\n",
       "  'try',\n",
       "  'brainstorm',\n",
       "  'want',\n",
       "  'write',\n",
       "  'last',\n",
       "  'essay',\n",
       "  'lesson',\n",
       "  'plan'],\n",
       " [\"n't\",\n",
       "  'cry',\n",
       "  'thought',\n",
       "  \"'d\",\n",
       "  'complete',\n",
       "  'mess',\n",
       "  'grand',\n",
       "  'get',\n",
       "  'everything',\n",
       "  'sort',\n",
       "  'tan',\n",
       "  'hair',\n",
       "  'make',\n",
       "  'do'],\n",
       " ['right', 'im', 'go', 'vote', 'teen', 'choice', 'award'],\n",
       " ['seriously',\n",
       "  'need',\n",
       "  'freak',\n",
       "  'chapstick',\n",
       "  'left',\n",
       "  'mine',\n",
       "  'home',\n",
       "  'guess',\n",
       "  'work',\n",
       "  'past',\n",
       "  '6',\n",
       "  'go',\n",
       "  'happen'],\n",
       " ['soooo', 'sore', 'beastin', '3', 'push', 'ups'],\n",
       " [\"'m\", 'jealous', \"'m\", 'gutted', 'switch', 'sky', 'good', 'enjoy'],\n",
       " ['back',\n",
       "  'ballpark',\n",
       "  'feeling',\n",
       "  'padres',\n",
       "  'sweep',\n",
       "  'today',\n",
       "  'get',\n",
       "  'boys',\n",
       "  'back',\n",
       "  'track',\n",
       "  'ticket',\n",
       "  'still',\n",
       "  'available'],\n",
       " ['black', 'coffee', 'cup', 'sound', 'like', 'accident', 'wait', 'happen'],\n",
       " ['lg', 'cookie'],\n",
       " [\"'m\", 'sorry', 'hear', 'make', 'sad'],\n",
       " ['link', 'b0rken'],\n",
       " ['ohhh', 'thomas', 'cutie', 'crazy', 'serial', 'killer', 'eye'],\n",
       " ['chat',\n",
       "  'go',\n",
       "  'grab',\n",
       "  'snack',\n",
       "  'nutella',\n",
       "  'chai',\n",
       "  'tea',\n",
       "  'come',\n",
       "  'back',\n",
       "  'amp',\n",
       "  'go',\n",
       "  'hope',\n",
       "  'fell',\n",
       "  'asleep',\n",
       "  'finally'],\n",
       " ['yeah', 'haha'],\n",
       " ['haha', 'yeaaaa', 'tooooo', 'omg', 'display'],\n",
       " ['guy',\n",
       "  'track',\n",
       "  'h1n1',\n",
       "  'case',\n",
       "  'fl',\n",
       "  'would',\n",
       "  'make',\n",
       "  'good',\n",
       "  'feature',\n",
       "  'site',\n",
       "  'especially',\n",
       "  'case',\n",
       "  'develop',\n",
       "  'tally'],\n",
       " ['yep', \"n't\", 'understand', 'icon', 'need', 'unity'],\n",
       " ['welcome',\n",
       "  'board',\n",
       "  'new',\n",
       "  'follower',\n",
       "  'appreciate',\n",
       "  'follow',\n",
       "  'see',\n",
       "  'world',\n",
       "  'large',\n",
       "  'trimaran'],\n",
       " ['wan', 'na', 'go', 'freak', 'broke', 'fun'],\n",
       " ['thats', 'cute', 'axl', 'rise', 'lol'],\n",
       " ['shame', 'cant', 'sun'],\n",
       " ['l', 'beyond', 'devastate', 'draws', 'music', 'ridiculous'],\n",
       " ['student', 'loan', 'make', 'difference', 'im', 'still', 'fecking', 'break'],\n",
       " ['slept', '2', 'hour', \"'m\", 'ca', \"n't\", 'get', 'back', 'sleep', 'ugh'],\n",
       " ['argh', 'as', 'hurt'],\n",
       " ['thanks'],\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg length of tweet:  7.62029453125\n"
     ]
    }
   ],
   "source": [
    "''' Analysis of our set ''' \n",
    "\n",
    "# avg length of each tweet\n",
    "\n",
    "avg = 0\n",
    "for i in train_set:\n",
    "    avg += len(i)\n",
    "    \n",
    "avg = avg/len(train_set)\n",
    "print('Avg length of tweet: ', avg)\n",
    "\n",
    "# freq of words\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for i in train_set: # all words through all rows\n",
    "    for word in i:\n",
    "        all_words.append(word)\n",
    "\n",
    "#all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374558    [ya, quot, 'd, like, palm, pre, touchstone, ch...\n",
       "1389115       [felt, earthquake, afternoon, seem, epicenter]\n",
       "1137831                         [ruffle, shirt, like, likey]\n",
       "790714     [pretty, bad, night, crappy, morning, fml, but...\n",
       "1117911                                  [yeah, clear, view]\n",
       "                                 ...                        \n",
       "259178     [song, 's, middle, change, n't, want, born, ar...\n",
       "1414414                                         [good, luck]\n",
       "131932                              [rather, average, 32370]\n",
       "671155     [pickin, waitin, 2, hurry, odeeee, miss, dem, ...\n",
       "121958        [home, study, math, wooot, im, go, fail, shit]\n",
       "Name: tokens, Length: 1280000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ya',\n",
       " 'quot',\n",
       " \"'d\",\n",
       " 'like',\n",
       " 'palm',\n",
       " 'pre',\n",
       " 'touchstone',\n",
       " 'charger',\n",
       " 'readynow',\n",
       " 'yes',\n",
       " 'sound',\n",
       " 'good',\n",
       " 'beer',\n",
       " 'ready',\n",
       " \"'\",\n",
       " 'pre',\n",
       " 'launch',\n",
       " 'felt',\n",
       " 'earthquake',\n",
       " 'afternoon',\n",
       " 'seem',\n",
       " 'epicenter',\n",
       " 'ruffle',\n",
       " 'shirt',\n",
       " 'like',\n",
       " 'likey',\n",
       " 'pretty',\n",
       " 'bad',\n",
       " 'night',\n",
       " 'crappy',\n",
       " 'morning',\n",
       " 'fml',\n",
       " 'buttface',\n",
       " 'didnt',\n",
       " 'say',\n",
       " 'could',\n",
       " 'go',\n",
       " 'work',\n",
       " 'today',\n",
       " 'yeah',\n",
       " 'clear',\n",
       " 'view',\n",
       " 'one',\n",
       " 'time',\n",
       " 'follow',\n",
       " 'fam',\n",
       " 'ff',\n",
       " 'welcome',\n",
       " 'friday',\n",
       " 'follow',\n",
       " '2',\n",
       " 'u',\n",
       " 'rock',\n",
       " 'thanks',\n",
       " 'add',\n",
       " 'next',\n",
       " 'robcrotch',\n",
       " 'tm',\n",
       " 'video',\n",
       " 'yes',\n",
       " 'please',\n",
       " 'meet',\n",
       " 'fiancee',\n",
       " 'ian',\n",
       " \"'s\",\n",
       " 'party',\n",
       " 'day',\n",
       " 'btw',\n",
       " \"'s\",\n",
       " 'sweet',\n",
       " 'congrats',\n",
       " 'guy',\n",
       " 'gawd',\n",
       " \"'ve\",\n",
       " 'finish',\n",
       " 'whole',\n",
       " 'bowl',\n",
       " 'even',\n",
       " 'post',\n",
       " 'previous',\n",
       " 'tweet',\n",
       " 'make',\n",
       " '2nd',\n",
       " 'bowl',\n",
       " 'awesome',\n",
       " 'game',\n",
       " 'short',\n",
       " \"'ll\",\n",
       " 'finish',\n",
       " 'crave',\n",
       " 'god',\n",
       " 'war',\n",
       " 'action',\n",
       " 'happy',\n",
       " 'mother',\n",
       " 'day',\n",
       " 'grow',\n",
       " 'n',\n",
       " 'happy',\n",
       " 'amp',\n",
       " 'lil',\n",
       " 'prego',\n",
       " 'smut',\n",
       " 'lol',\n",
       " 'play',\n",
       " 'everyone',\n",
       " 'love',\n",
       " 'happy',\n",
       " 'mother',\n",
       " 'day',\n",
       " 'amp',\n",
       " 'god',\n",
       " 'bless',\n",
       " 'back',\n",
       " 'eating',\n",
       " 'dinner',\n",
       " 'could',\n",
       " 'follow',\n",
       " 'chat',\n",
       " 'room',\n",
       " 'could',\n",
       " 'please',\n",
       " 'follow',\n",
       " \"n't\",\n",
       " 'many',\n",
       " 'lol',\n",
       " 'heck',\n",
       " 'way',\n",
       " 'height',\n",
       " 'want',\n",
       " 'trade',\n",
       " 'tired',\n",
       " 'say',\n",
       " 'goodbye',\n",
       " 'chicago',\n",
       " 'steal',\n",
       " 'half',\n",
       " '12',\n",
       " 'mai',\n",
       " 'tai',\n",
       " 'inbound',\n",
       " 'suddenly',\n",
       " 'trouble',\n",
       " 'relax',\n",
       " 'lol',\n",
       " \"'m\",\n",
       " 'still',\n",
       " 'even',\n",
       " 'closeee',\n",
       " 'figure',\n",
       " 'ahhh',\n",
       " 'want',\n",
       " 'help',\n",
       " 'lol',\n",
       " 'thank',\n",
       " 'support',\n",
       " 'maternal',\n",
       " 'health',\n",
       " 'hell',\n",
       " 'yeah',\n",
       " 'starbucks',\n",
       " 'rule',\n",
       " 'lol',\n",
       " 'know',\n",
       " 'work',\n",
       " 'pay',\n",
       " 'doctor',\n",
       " '50',\n",
       " 'hahahah',\n",
       " 'mah',\n",
       " 'freakin',\n",
       " 'bad',\n",
       " 'final',\n",
       " 'week',\n",
       " 'yes',\n",
       " \"fo'sho\",\n",
       " \"'ll\",\n",
       " 'visit',\n",
       " 'like',\n",
       " 'foreal',\n",
       " 'new',\n",
       " 'prospect',\n",
       " 'make',\n",
       " 'greeting',\n",
       " 'card',\n",
       " 'say',\n",
       " 'eff',\n",
       " 'maybe',\n",
       " 'five',\n",
       " 'month',\n",
       " 'nobody',\n",
       " 'die',\n",
       " \"'s\",\n",
       " 'treatable',\n",
       " 'unlike',\n",
       " 'flu',\n",
       " 'guess',\n",
       " \"n't\",\n",
       " 'mean',\n",
       " \"n't\",\n",
       " 'care',\n",
       " 'daughter',\n",
       " 'hook',\n",
       " 'sims',\n",
       " 'buy',\n",
       " '3',\n",
       " 'day',\n",
       " 'come',\n",
       " 'love',\n",
       " 'boyfriend',\n",
       " 'alone',\n",
       " 'friday',\n",
       " 'night',\n",
       " 'temp',\n",
       " '102f',\n",
       " 'dunno',\n",
       " 'dunno',\n",
       " 'god',\n",
       " 'love',\n",
       " 'shaun',\n",
       " 'dont',\n",
       " 'time',\n",
       " 'shit',\n",
       " 'anymore',\n",
       " 'spx',\n",
       " '920',\n",
       " 'floor',\n",
       " 'thats',\n",
       " 'iv',\n",
       " \"'\",\n",
       " 'e',\n",
       " 'fun',\n",
       " 'move',\n",
       " 'la',\n",
       " 'already',\n",
       " 'please',\n",
       " 'tired',\n",
       " \"'m\",\n",
       " 'go',\n",
       " 'back',\n",
       " 'sleep',\n",
       " 'iv',\n",
       " 'get',\n",
       " 'work',\n",
       " 'tonite',\n",
       " 'come',\n",
       " 'u',\n",
       " 'didnt',\n",
       " 'make',\n",
       " 'apperance',\n",
       " 'yesterday',\n",
       " 'webcast',\n",
       " 'tisk',\n",
       " 'tisk',\n",
       " 'remember',\n",
       " 'suppose',\n",
       " 'wear',\n",
       " 'hotter',\n",
       " 'dress',\n",
       " 'bride',\n",
       " 'congrats',\n",
       " 'andy',\n",
       " 'bet',\n",
       " \"'re\",\n",
       " 'great',\n",
       " 'day',\n",
       " 'worry',\n",
       " 'go',\n",
       " 'love',\n",
       " 'song',\n",
       " 'sang',\n",
       " 'american',\n",
       " 'idol',\n",
       " 'make',\n",
       " '1st',\n",
       " 'break',\n",
       " 'fell',\n",
       " 'bit',\n",
       " 'near',\n",
       " 'end',\n",
       " '2nd',\n",
       " 'level',\n",
       " 'bad',\n",
       " 'though',\n",
       " '3550',\n",
       " 'chip',\n",
       " 'actually',\n",
       " \"'m\",\n",
       " 'hella',\n",
       " 'bore',\n",
       " 'friday',\n",
       " \"'m\",\n",
       " 'totally',\n",
       " 'work',\n",
       " 'mode',\n",
       " 'let',\n",
       " 'know',\n",
       " 'check',\n",
       " 'schedule',\n",
       " 'ethan',\n",
       " 'still',\n",
       " 'go',\n",
       " 'home',\n",
       " 'piss',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'get',\n",
       " 'yea',\n",
       " 'bet',\n",
       " 'tell',\n",
       " 'u',\n",
       " 'want',\n",
       " 'shirt',\n",
       " 'holla',\n",
       " 'empress',\n",
       " 'empressccp',\n",
       " 'com',\n",
       " 'watch',\n",
       " 'camp',\n",
       " 'rock',\n",
       " 'get',\n",
       " 'ta',\n",
       " 'find',\n",
       " 'joe',\n",
       " 'sang',\n",
       " 'last',\n",
       " 'night',\n",
       " 'sound',\n",
       " 'beautiful',\n",
       " 'doesnt',\n",
       " 'friend',\n",
       " 'twitter',\n",
       " 'back',\n",
       " 'work',\n",
       " 'today',\n",
       " 'bad',\n",
       " 'time',\n",
       " \"'s\",\n",
       " 'probably',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'take',\n",
       " '2',\n",
       " 'week',\n",
       " 'filter',\n",
       " 'daily',\n",
       " 'basis',\n",
       " 'lot',\n",
       " 'good',\n",
       " 'stuff',\n",
       " '455',\n",
       " 'atm',\n",
       " 'tweet',\n",
       " 'include',\n",
       " 'long',\n",
       " 'poem',\n",
       " 'world',\n",
       " 'sry',\n",
       " 'yep',\n",
       " 'school',\n",
       " 'friend',\n",
       " 'jonathan',\n",
       " 'donde',\n",
       " 'estas',\n",
       " 'bro',\n",
       " 'turn',\n",
       " 'twitter',\n",
       " 'junkie',\n",
       " 'likey',\n",
       " 'need',\n",
       " 'scooby',\n",
       " 'snack',\n",
       " \"'m\",\n",
       " 'sure',\n",
       " 'new',\n",
       " 'betablockers',\n",
       " \"'m\",\n",
       " 'give',\n",
       " 'munchies',\n",
       " 'back',\n",
       " 'work',\n",
       " 'tomorrow',\n",
       " 'oh',\n",
       " 'dead',\n",
       " 'computer',\n",
       " \"'s\",\n",
       " 'good',\n",
       " 'poor',\n",
       " 'macbook',\n",
       " \"'m\",\n",
       " 'sad',\n",
       " 'whats',\n",
       " 'everyone',\n",
       " 'holiday',\n",
       " 'rain',\n",
       " 'course',\n",
       " 'wicked',\n",
       " 'long',\n",
       " 'weekend',\n",
       " 'miami',\n",
       " \"n't\",\n",
       " 'sleep',\n",
       " 'head',\n",
       " 'airport',\n",
       " 'back',\n",
       " 'life',\n",
       " 'back',\n",
       " 'reality',\n",
       " 'thanks',\n",
       " 'bday',\n",
       " 'love',\n",
       " 'saw',\n",
       " 'curragh',\n",
       " 'weekend',\n",
       " 'say',\n",
       " 'hello',\n",
       " 'really',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'work',\n",
       " 'today',\n",
       " 'awwwwwwwww',\n",
       " 'love',\n",
       " 'awesome',\n",
       " 'clown',\n",
       " 'wtf',\n",
       " 'sarcastic',\n",
       " 'lol',\n",
       " '771',\n",
       " \"'s\",\n",
       " 'pretty',\n",
       " 'amazing',\n",
       " 'ill',\n",
       " 'catch',\n",
       " 'tonight',\n",
       " '400',\n",
       " 'text',\n",
       " 'come',\n",
       " 'good',\n",
       " 'morning',\n",
       " 'luvlies',\n",
       " 'spent',\n",
       " 'non',\n",
       " 'productive',\n",
       " 'day',\n",
       " 'today',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'even',\n",
       " 'believe',\n",
       " 'ha',\n",
       " 'jus',\n",
       " 'bum',\n",
       " 'home',\n",
       " 'day',\n",
       " 'hope',\n",
       " 'dun',\n",
       " 'get',\n",
       " 'use',\n",
       " 'twitter',\n",
       " 'slow',\n",
       " 'today',\n",
       " 'maintenance',\n",
       " 'tgif',\n",
       " 'excite',\n",
       " 'see',\n",
       " 'everyone',\n",
       " \"'s\",\n",
       " 'grad',\n",
       " 'party',\n",
       " 'saturday',\n",
       " 'finish',\n",
       " 'homework',\n",
       " 'move',\n",
       " 'onto',\n",
       " 'assignment',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " \"'twibe\",\n",
       " \"'\",\n",
       " \"'m\",\n",
       " 'feel',\n",
       " 'lonely',\n",
       " 'today',\n",
       " 'heyy',\n",
       " 'wats',\n",
       " 'mite',\n",
       " 'comin',\n",
       " '2',\n",
       " 'florida',\n",
       " 'summer',\n",
       " 'dance',\n",
       " 'puddle',\n",
       " 'like',\n",
       " 'little',\n",
       " 'kid',\n",
       " 'really',\n",
       " 'make',\n",
       " 'day',\n",
       " 'well',\n",
       " 'baby',\n",
       " 'dog',\n",
       " 'ashlee',\n",
       " '24a3e9ee',\n",
       " \"'s\",\n",
       " 'bb',\n",
       " 'pin',\n",
       " 'anyone',\n",
       " 'want',\n",
       " 'jodi',\n",
       " 'also',\n",
       " 'uncool',\n",
       " 'twitter',\n",
       " 'love',\n",
       " 'sorry',\n",
       " \"'s\",\n",
       " 'occasion',\n",
       " 'leave',\n",
       " 'early',\n",
       " 'also',\n",
       " 'wan',\n",
       " 'na',\n",
       " 'leave',\n",
       " 'somee',\n",
       " 'ppl',\n",
       " 'mean',\n",
       " 'sumtimes',\n",
       " 'im',\n",
       " 'best',\n",
       " 'time',\n",
       " 'ever',\n",
       " 'right',\n",
       " 'record',\n",
       " 'audition',\n",
       " 'make',\n",
       " 'think',\n",
       " \"'re\",\n",
       " 'easy',\n",
       " 'ha',\n",
       " 'ha',\n",
       " 'kidding',\n",
       " \"'m\",\n",
       " 'problem',\n",
       " 'far',\n",
       " \"'m\",\n",
       " 'enjoy',\n",
       " 'new',\n",
       " 'alexisonfire',\n",
       " 'album',\n",
       " 'sick',\n",
       " 'mac',\n",
       " 'cope',\n",
       " 'later',\n",
       " 'spending',\n",
       " 'time',\n",
       " 'faves',\n",
       " 'take',\n",
       " 'precedence',\n",
       " \"c'mon\",\n",
       " 'jon',\n",
       " 'u',\n",
       " 'cant',\n",
       " 'tell',\n",
       " 'u',\n",
       " 'stuff',\n",
       " 'like',\n",
       " \"'possibly\",\n",
       " 'last',\n",
       " 'show',\n",
       " \"'\",\n",
       " 'bwwwaaa',\n",
       " 'yeah',\n",
       " 'send',\n",
       " 'info',\n",
       " \"n't\",\n",
       " 'mind',\n",
       " \"'re\",\n",
       " 'tweetup',\n",
       " 'monday',\n",
       " 'sure',\n",
       " 'thing',\n",
       " \"'ll\",\n",
       " 'make',\n",
       " 'sure',\n",
       " 'bring',\n",
       " 'extra',\n",
       " 'load',\n",
       " 'vegemite',\n",
       " 'haha',\n",
       " 'well',\n",
       " 'see',\n",
       " 'soon',\n",
       " 'yea',\n",
       " 'im',\n",
       " 'tower',\n",
       " 'internet',\n",
       " 'anyways',\n",
       " 'r',\n",
       " 'u',\n",
       " 'get',\n",
       " 'school',\n",
       " 'work',\n",
       " 'do',\n",
       " 'semester',\n",
       " 'ridiculously',\n",
       " 'short',\n",
       " 'oh',\n",
       " 'no',\n",
       " 'final',\n",
       " 'sorry',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'poop',\n",
       " 'sick',\n",
       " 'today',\n",
       " 'plan',\n",
       " 'episode',\n",
       " 'radio',\n",
       " 'show',\n",
       " 'talkshoe',\n",
       " 'tomorrow',\n",
       " '8',\n",
       " 'eastern',\n",
       " 'time',\n",
       " 'cleaninggg',\n",
       " 'hit',\n",
       " 'glad',\n",
       " 'one',\n",
       " 'true',\n",
       " 'great',\n",
       " 'wonderful',\n",
       " 'weekend',\n",
       " 'happy',\n",
       " 'monday',\n",
       " \"'m\",\n",
       " 'excited',\n",
       " 'week',\n",
       " 'get',\n",
       " 'warm',\n",
       " 'day',\n",
       " 'get',\n",
       " 'long',\n",
       " 'miss',\n",
       " 'nana',\n",
       " 'grandad',\n",
       " 'l',\n",
       " 'urgh',\n",
       " 'listen',\n",
       " 'song',\n",
       " 'make',\n",
       " 'sad',\n",
       " 'woof',\n",
       " 'woof',\n",
       " \"'s\",\n",
       " 'colder',\n",
       " 'today',\n",
       " 'walky',\n",
       " 'thrift',\n",
       " 'shop',\n",
       " 'find',\n",
       " 'bracelet',\n",
       " 'watch',\n",
       " 'lol',\n",
       " 'wrong',\n",
       " 'ahah',\n",
       " 'suppose',\n",
       " 'wrap',\n",
       " 'nasty',\n",
       " 'goo',\n",
       " 'thing',\n",
       " 'wait',\n",
       " 'new',\n",
       " 'tnt',\n",
       " 'triathletes',\n",
       " 'get',\n",
       " 'store',\n",
       " 'brick',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'teach',\n",
       " 'em',\n",
       " 'nutrition',\n",
       " 'value',\n",
       " 'endurolytes',\n",
       " 'fl',\n",
       " 'que',\n",
       " 'think',\n",
       " 'fail',\n",
       " 'exam',\n",
       " 'ugh',\n",
       " 'fml',\n",
       " 'barn',\n",
       " 'shawn',\n",
       " 'throw',\n",
       " 'shoe',\n",
       " 'great',\n",
       " 'lesson',\n",
       " 'little',\n",
       " 'chilly',\n",
       " 'wear',\n",
       " 'jacket',\n",
       " 'follow',\n",
       " 'twitter',\n",
       " 'still',\n",
       " 'happy',\n",
       " 'leave',\n",
       " 'facebook',\n",
       " 'lindsey',\n",
       " 'know',\n",
       " 'cubby',\n",
       " 'hole',\n",
       " 'bed',\n",
       " 'open',\n",
       " 'get',\n",
       " 'home',\n",
       " 'omg',\n",
       " 'rumble',\n",
       " 'thunder',\n",
       " 'make',\n",
       " 'ground',\n",
       " 'shake',\n",
       " 'freak',\n",
       " 'whole',\n",
       " 'house',\n",
       " 'shook',\n",
       " 'want',\n",
       " 'give',\n",
       " 'phil',\n",
       " 'labonte',\n",
       " 'hug',\n",
       " 'get',\n",
       " '1k',\n",
       " 'last',\n",
       " 'week',\n",
       " 'get',\n",
       " 'list',\n",
       " 'top',\n",
       " '25',\n",
       " 'ppl',\n",
       " 'follow',\n",
       " 'twitter',\n",
       " \"'re\",\n",
       " 'go',\n",
       " 'risk',\n",
       " 'car',\n",
       " 'size',\n",
       " 'pothole',\n",
       " 'let',\n",
       " 'adventure',\n",
       " 'begin',\n",
       " 'playiing',\n",
       " 'card',\n",
       " 'wit',\n",
       " 'bebes',\n",
       " 'fam',\n",
       " 'libby',\n",
       " 'bday',\n",
       " 'party',\n",
       " 'yay',\n",
       " 'shes',\n",
       " 'three',\n",
       " 'muah',\n",
       " 'baby',\n",
       " 'come',\n",
       " 'back',\n",
       " 'disappointed',\n",
       " 'find',\n",
       " 'triple',\n",
       " 'mocha',\n",
       " 'appear',\n",
       " 'actual',\n",
       " 'mocha',\n",
       " 'part',\n",
       " 'haha',\n",
       " 'call',\n",
       " 'soda',\n",
       " 'pop',\n",
       " 'well',\n",
       " \"'m\",\n",
       " 'actually',\n",
       " 'catch',\n",
       " 'call',\n",
       " 'soda',\n",
       " 'aww',\n",
       " \"'s\",\n",
       " 'unfair',\n",
       " 'well',\n",
       " 'always',\n",
       " 'offer',\n",
       " 'male',\n",
       " 'stripper',\n",
       " 'think',\n",
       " \"'ll\",\n",
       " 'hide',\n",
       " 'gun',\n",
       " 'first',\n",
       " 'square',\n",
       " 'space',\n",
       " \"'m\",\n",
       " 'crazy',\n",
       " 'bout',\n",
       " 'precious',\n",
       " 'phone',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'afford',\n",
       " \"'ve\",\n",
       " 'work',\n",
       " '24',\n",
       " 'hour',\n",
       " 'weekend',\n",
       " 'simply',\n",
       " 'run',\n",
       " 'time',\n",
       " 'dansk',\n",
       " 'folkeparti',\n",
       " 'get',\n",
       " 'approximately',\n",
       " '15',\n",
       " 'vote',\n",
       " 'wheel',\n",
       " 'keep',\n",
       " 'turn',\n",
       " \"'re\",\n",
       " 'start',\n",
       " 'line',\n",
       " '10',\n",
       " 'select',\n",
       " 'store',\n",
       " 'hopefully',\n",
       " 'somewhere',\n",
       " 'ger',\n",
       " 'list',\n",
       " 'sure',\n",
       " 'norfolk',\n",
       " 'also',\n",
       " 'realize',\n",
       " 'digital',\n",
       " 'voice',\n",
       " 'recorder',\n",
       " \"n't\",\n",
       " 'usb',\n",
       " 'port',\n",
       " 'think',\n",
       " 'pick',\n",
       " 'wrong',\n",
       " 'model',\n",
       " 'thoroughly',\n",
       " 'feed',\n",
       " 'even',\n",
       " 'really',\n",
       " 'f',\n",
       " 'e',\n",
       " 'u',\n",
       " 'p',\n",
       " 'wish',\n",
       " 'stress',\n",
       " 'would',\n",
       " 'vanish',\n",
       " 'life',\n",
       " 'rainy',\n",
       " 'day',\n",
       " 'well',\n",
       " 'today',\n",
       " 'tomorrow',\n",
       " 'though',\n",
       " 'boyfriend',\n",
       " 'go',\n",
       " 'one',\n",
       " 'night',\n",
       " 'amp',\n",
       " 'get',\n",
       " 'call',\n",
       " 'ashton',\n",
       " 'cut',\n",
       " 'cher',\n",
       " 'dem',\n",
       " 'moore',\n",
       " \"'s\",\n",
       " 'say',\n",
       " 'hilarious',\n",
       " 'note',\n",
       " 'self',\n",
       " 'never',\n",
       " 'use',\n",
       " \"'smooth\",\n",
       " 'away',\n",
       " \"'\",\n",
       " 'unless',\n",
       " \"'m\",\n",
       " 'sandpaper',\n",
       " 'buffing',\n",
       " 'look',\n",
       " 'ok',\n",
       " 'last',\n",
       " 'time',\n",
       " 'twitter',\n",
       " 'leave',\n",
       " 'mate',\n",
       " 'house',\n",
       " 'c',\n",
       " 'ya',\n",
       " 'world',\n",
       " 'tweet',\n",
       " 'last',\n",
       " 'exam',\n",
       " '2mozzeh',\n",
       " 'xo',\n",
       " 'thanks',\n",
       " 'go',\n",
       " 'really',\n",
       " 'well',\n",
       " 'proud',\n",
       " 'self',\n",
       " 'babysitting',\n",
       " 'baby',\n",
       " \"'s\",\n",
       " 'asleep',\n",
       " 'fun',\n",
       " 'check',\n",
       " 'engine',\n",
       " 'light',\n",
       " 'since',\n",
       " 'drive',\n",
       " 'glendale',\n",
       " 'yesterday',\n",
       " 'yuvraj',\n",
       " 'hit',\n",
       " 'hard',\n",
       " 'india',\n",
       " 'look',\n",
       " 'preety',\n",
       " 'good',\n",
       " 'think',\n",
       " 'need',\n",
       " 'worry',\n",
       " 'india',\n",
       " 'win',\n",
       " 'fun',\n",
       " 'tweetup',\n",
       " 'epic',\n",
       " 'proud',\n",
       " 'manny',\n",
       " 'pacquiao',\n",
       " 'amaze',\n",
       " 'fight',\n",
       " 'place',\n",
       " 'blow',\n",
       " 'ubertwitter',\n",
       " \"n't\",\n",
       " 'function',\n",
       " 'switch',\n",
       " 'tb',\n",
       " 'mighty',\n",
       " 'boosh',\n",
       " 'luv',\n",
       " 'um',\n",
       " 'haha',\n",
       " 'loave',\n",
       " 'load',\n",
       " 'diffrent',\n",
       " 'one',\n",
       " 'aswell',\n",
       " 'ba',\n",
       " 'one',\n",
       " 'last',\n",
       " 'time',\n",
       " '2',\n",
       " 'update',\n",
       " 'status',\n",
       " 'many',\n",
       " 'eft',\n",
       " 'practitioner',\n",
       " 'anyway',\n",
       " 'last',\n",
       " 'day',\n",
       " 'holiday',\n",
       " 'wan',\n",
       " 'na',\n",
       " 'cry',\n",
       " 'yay',\n",
       " 'im',\n",
       " 'bed',\n",
       " 'kinda',\n",
       " 'call',\n",
       " 'u',\n",
       " 'want',\n",
       " 'nite',\n",
       " 'ntie',\n",
       " 'sweet',\n",
       " 'dream',\n",
       " 'lt',\n",
       " '33333',\n",
       " 'meant',\n",
       " 'say',\n",
       " 'earlier',\n",
       " \"'s\",\n",
       " 'bad',\n",
       " 'cycle',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding with Gensim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create Word2Vec model on our data\n",
    "\n",
    "- in order to create word embeddings for our training words\n",
    "- embed words by context neighbors\n",
    "\n",
    "'''\n",
    "\n",
    "# setting vocabulary parameters\n",
    "\n",
    "model_size = 256 # dimensionality of dense vector to represent word - # of context neighboring words we want\n",
    "model_window = 8 # max distance between target word and neighboring words in sentence, outliers are not considered similar\n",
    "model_min_count = 10 # ignore infrequent words\n",
    "model_workers = 4 # distibution of threads to use\n",
    "model_epochs = 32 # number of epochs over corpus\n",
    "\n",
    "# building word2vec model on train_set\n",
    "\n",
    "cbow_model = gensim.models.word2vec.Word2Vec(min_count=model_min_count, \n",
    "                                            size=model_size, \n",
    "                                            window=model_window, \n",
    "                                            workers=model_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 21:51:18,413 : INFO : collecting all words and their counts\n",
      "2020-09-22 21:51:18,414 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-09-22 21:51:18,438 : INFO : PROGRESS: at sentence #10000, processed 76693 words, keeping 11785 word types\n",
      "2020-09-22 21:51:18,465 : INFO : PROGRESS: at sentence #20000, processed 152763 words, keeping 18074 word types\n",
      "2020-09-22 21:51:18,493 : INFO : PROGRESS: at sentence #30000, processed 228267 words, keeping 23014 word types\n",
      "2020-09-22 21:51:18,526 : INFO : PROGRESS: at sentence #40000, processed 304919 words, keeping 27350 word types\n",
      "2020-09-22 21:51:18,561 : INFO : PROGRESS: at sentence #50000, processed 380604 words, keeping 31367 word types\n",
      "2020-09-22 21:51:18,592 : INFO : PROGRESS: at sentence #60000, processed 456306 words, keeping 35121 word types\n",
      "2020-09-22 21:51:18,619 : INFO : PROGRESS: at sentence #70000, processed 532012 words, keeping 38611 word types\n",
      "2020-09-22 21:51:18,649 : INFO : PROGRESS: at sentence #80000, processed 608682 words, keeping 41938 word types\n",
      "2020-09-22 21:51:18,688 : INFO : PROGRESS: at sentence #90000, processed 684186 words, keeping 44975 word types\n",
      "2020-09-22 21:51:18,719 : INFO : PROGRESS: at sentence #100000, processed 759938 words, keeping 47931 word types\n",
      "2020-09-22 21:51:18,750 : INFO : PROGRESS: at sentence #110000, processed 836160 words, keeping 50679 word types\n",
      "2020-09-22 21:51:18,782 : INFO : PROGRESS: at sentence #120000, processed 912721 words, keeping 53428 word types\n",
      "2020-09-22 21:51:18,816 : INFO : PROGRESS: at sentence #130000, processed 988683 words, keeping 56036 word types\n",
      "2020-09-22 21:51:18,849 : INFO : PROGRESS: at sentence #140000, processed 1064403 words, keeping 58514 word types\n",
      "2020-09-22 21:51:18,883 : INFO : PROGRESS: at sentence #150000, processed 1140316 words, keeping 61103 word types\n",
      "2020-09-22 21:51:18,919 : INFO : PROGRESS: at sentence #160000, processed 1215844 words, keeping 63560 word types\n",
      "2020-09-22 21:51:18,951 : INFO : PROGRESS: at sentence #170000, processed 1292244 words, keeping 66021 word types\n",
      "2020-09-22 21:51:18,983 : INFO : PROGRESS: at sentence #180000, processed 1368913 words, keeping 68475 word types\n",
      "2020-09-22 21:51:19,013 : INFO : PROGRESS: at sentence #190000, processed 1446016 words, keeping 70870 word types\n",
      "2020-09-22 21:51:19,041 : INFO : PROGRESS: at sentence #200000, processed 1521853 words, keeping 73134 word types\n",
      "2020-09-22 21:51:19,067 : INFO : PROGRESS: at sentence #210000, processed 1598479 words, keeping 75313 word types\n",
      "2020-09-22 21:51:19,101 : INFO : PROGRESS: at sentence #220000, processed 1674675 words, keeping 77569 word types\n",
      "2020-09-22 21:51:19,127 : INFO : PROGRESS: at sentence #230000, processed 1750963 words, keeping 79792 word types\n",
      "2020-09-22 21:51:19,157 : INFO : PROGRESS: at sentence #240000, processed 1827355 words, keeping 81902 word types\n",
      "2020-09-22 21:51:19,189 : INFO : PROGRESS: at sentence #250000, processed 1903946 words, keeping 83996 word types\n",
      "2020-09-22 21:51:19,217 : INFO : PROGRESS: at sentence #260000, processed 1980128 words, keeping 86041 word types\n",
      "2020-09-22 21:51:19,247 : INFO : PROGRESS: at sentence #270000, processed 2056343 words, keeping 88032 word types\n",
      "2020-09-22 21:51:19,273 : INFO : PROGRESS: at sentence #280000, processed 2132327 words, keeping 90069 word types\n",
      "2020-09-22 21:51:19,303 : INFO : PROGRESS: at sentence #290000, processed 2208980 words, keeping 91992 word types\n",
      "2020-09-22 21:51:19,330 : INFO : PROGRESS: at sentence #300000, processed 2284633 words, keeping 93868 word types\n",
      "2020-09-22 21:51:19,357 : INFO : PROGRESS: at sentence #310000, processed 2360694 words, keeping 95790 word types\n",
      "2020-09-22 21:51:19,386 : INFO : PROGRESS: at sentence #320000, processed 2436979 words, keeping 97765 word types\n",
      "2020-09-22 21:51:19,412 : INFO : PROGRESS: at sentence #330000, processed 2513104 words, keeping 99712 word types\n",
      "2020-09-22 21:51:19,439 : INFO : PROGRESS: at sentence #340000, processed 2589336 words, keeping 101496 word types\n",
      "2020-09-22 21:51:19,470 : INFO : PROGRESS: at sentence #350000, processed 2665703 words, keeping 103339 word types\n",
      "2020-09-22 21:51:19,501 : INFO : PROGRESS: at sentence #360000, processed 2742062 words, keeping 105149 word types\n",
      "2020-09-22 21:51:19,526 : INFO : PROGRESS: at sentence #370000, processed 2818189 words, keeping 106880 word types\n",
      "2020-09-22 21:51:19,556 : INFO : PROGRESS: at sentence #380000, processed 2894416 words, keeping 108693 word types\n",
      "2020-09-22 21:51:19,585 : INFO : PROGRESS: at sentence #390000, processed 2970596 words, keeping 110446 word types\n",
      "2020-09-22 21:51:19,613 : INFO : PROGRESS: at sentence #400000, processed 3047188 words, keeping 112198 word types\n",
      "2020-09-22 21:51:19,648 : INFO : PROGRESS: at sentence #410000, processed 3123030 words, keeping 113967 word types\n",
      "2020-09-22 21:51:19,684 : INFO : PROGRESS: at sentence #420000, processed 3199249 words, keeping 115784 word types\n",
      "2020-09-22 21:51:19,722 : INFO : PROGRESS: at sentence #430000, processed 3275837 words, keeping 117551 word types\n",
      "2020-09-22 21:51:19,756 : INFO : PROGRESS: at sentence #440000, processed 3351957 words, keeping 119214 word types\n",
      "2020-09-22 21:51:19,787 : INFO : PROGRESS: at sentence #450000, processed 3428878 words, keeping 120867 word types\n",
      "2020-09-22 21:51:19,820 : INFO : PROGRESS: at sentence #460000, processed 3505204 words, keeping 122535 word types\n",
      "2020-09-22 21:51:19,855 : INFO : PROGRESS: at sentence #470000, processed 3582295 words, keeping 124243 word types\n",
      "2020-09-22 21:51:19,887 : INFO : PROGRESS: at sentence #480000, processed 3658293 words, keeping 125875 word types\n",
      "2020-09-22 21:51:19,920 : INFO : PROGRESS: at sentence #490000, processed 3734213 words, keeping 127495 word types\n",
      "2020-09-22 21:51:19,956 : INFO : PROGRESS: at sentence #500000, processed 3810503 words, keeping 129127 word types\n",
      "2020-09-22 21:51:19,998 : INFO : PROGRESS: at sentence #510000, processed 3886875 words, keeping 130762 word types\n",
      "2020-09-22 21:51:20,043 : INFO : PROGRESS: at sentence #520000, processed 3963743 words, keeping 132363 word types\n",
      "2020-09-22 21:51:20,080 : INFO : PROGRESS: at sentence #530000, processed 4040347 words, keeping 133982 word types\n",
      "2020-09-22 21:51:20,123 : INFO : PROGRESS: at sentence #540000, processed 4116583 words, keeping 135516 word types\n",
      "2020-09-22 21:51:20,156 : INFO : PROGRESS: at sentence #550000, processed 4192501 words, keeping 137069 word types\n",
      "2020-09-22 21:51:20,191 : INFO : PROGRESS: at sentence #560000, processed 4269239 words, keeping 138657 word types\n",
      "2020-09-22 21:51:20,225 : INFO : PROGRESS: at sentence #570000, processed 4345635 words, keeping 140231 word types\n",
      "2020-09-22 21:51:20,266 : INFO : PROGRESS: at sentence #580000, processed 4421905 words, keeping 141766 word types\n",
      "2020-09-22 21:51:20,296 : INFO : PROGRESS: at sentence #590000, processed 4498152 words, keeping 143202 word types\n",
      "2020-09-22 21:51:20,328 : INFO : PROGRESS: at sentence #600000, processed 4574612 words, keeping 144733 word types\n",
      "2020-09-22 21:51:20,364 : INFO : PROGRESS: at sentence #610000, processed 4651133 words, keeping 146278 word types\n",
      "2020-09-22 21:51:20,395 : INFO : PROGRESS: at sentence #620000, processed 4727426 words, keeping 147729 word types\n",
      "2020-09-22 21:51:20,423 : INFO : PROGRESS: at sentence #630000, processed 4803299 words, keeping 149197 word types\n",
      "2020-09-22 21:51:20,450 : INFO : PROGRESS: at sentence #640000, processed 4880108 words, keeping 150656 word types\n",
      "2020-09-22 21:51:20,476 : INFO : PROGRESS: at sentence #650000, processed 4956941 words, keeping 152114 word types\n",
      "2020-09-22 21:51:20,506 : INFO : PROGRESS: at sentence #660000, processed 5033084 words, keeping 153534 word types\n",
      "2020-09-22 21:51:20,534 : INFO : PROGRESS: at sentence #670000, processed 5109915 words, keeping 154963 word types\n",
      "2020-09-22 21:51:20,560 : INFO : PROGRESS: at sentence #680000, processed 5185512 words, keeping 156478 word types\n",
      "2020-09-22 21:51:20,588 : INFO : PROGRESS: at sentence #690000, processed 5262261 words, keeping 157933 word types\n",
      "2020-09-22 21:51:20,614 : INFO : PROGRESS: at sentence #700000, processed 5339025 words, keeping 159303 word types\n",
      "2020-09-22 21:51:20,643 : INFO : PROGRESS: at sentence #710000, processed 5415810 words, keeping 160738 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 21:51:20,682 : INFO : PROGRESS: at sentence #720000, processed 5491863 words, keeping 162195 word types\n",
      "2020-09-22 21:51:20,716 : INFO : PROGRESS: at sentence #730000, processed 5567462 words, keeping 163597 word types\n",
      "2020-09-22 21:51:20,748 : INFO : PROGRESS: at sentence #740000, processed 5643612 words, keeping 164952 word types\n",
      "2020-09-22 21:51:20,779 : INFO : PROGRESS: at sentence #750000, processed 5719459 words, keeping 166281 word types\n",
      "2020-09-22 21:51:20,811 : INFO : PROGRESS: at sentence #760000, processed 5795667 words, keeping 167631 word types\n",
      "2020-09-22 21:51:20,842 : INFO : PROGRESS: at sentence #770000, processed 5872341 words, keeping 169033 word types\n",
      "2020-09-22 21:51:20,872 : INFO : PROGRESS: at sentence #780000, processed 5947706 words, keeping 170365 word types\n",
      "2020-09-22 21:51:20,898 : INFO : PROGRESS: at sentence #790000, processed 6024561 words, keeping 171842 word types\n",
      "2020-09-22 21:51:20,924 : INFO : PROGRESS: at sentence #800000, processed 6101501 words, keeping 173199 word types\n",
      "2020-09-22 21:51:20,954 : INFO : PROGRESS: at sentence #810000, processed 6178276 words, keeping 174491 word types\n",
      "2020-09-22 21:51:20,989 : INFO : PROGRESS: at sentence #820000, processed 6254612 words, keeping 175869 word types\n",
      "2020-09-22 21:51:21,015 : INFO : PROGRESS: at sentence #830000, processed 6330498 words, keeping 177247 word types\n",
      "2020-09-22 21:51:21,042 : INFO : PROGRESS: at sentence #840000, processed 6405988 words, keeping 178612 word types\n",
      "2020-09-22 21:51:21,072 : INFO : PROGRESS: at sentence #850000, processed 6482205 words, keeping 179928 word types\n",
      "2020-09-22 21:51:21,099 : INFO : PROGRESS: at sentence #860000, processed 6558082 words, keeping 181206 word types\n",
      "2020-09-22 21:51:21,128 : INFO : PROGRESS: at sentence #870000, processed 6634571 words, keeping 182503 word types\n",
      "2020-09-22 21:51:21,163 : INFO : PROGRESS: at sentence #880000, processed 6710863 words, keeping 183780 word types\n",
      "2020-09-22 21:51:21,197 : INFO : PROGRESS: at sentence #890000, processed 6787068 words, keeping 185076 word types\n",
      "2020-09-22 21:51:21,226 : INFO : PROGRESS: at sentence #900000, processed 6862884 words, keeping 186443 word types\n",
      "2020-09-22 21:51:21,255 : INFO : PROGRESS: at sentence #910000, processed 6939374 words, keeping 187711 word types\n",
      "2020-09-22 21:51:21,285 : INFO : PROGRESS: at sentence #920000, processed 7015617 words, keeping 188998 word types\n",
      "2020-09-22 21:51:21,311 : INFO : PROGRESS: at sentence #930000, processed 7092162 words, keeping 190368 word types\n",
      "2020-09-22 21:51:21,338 : INFO : PROGRESS: at sentence #940000, processed 7167649 words, keeping 191678 word types\n",
      "2020-09-22 21:51:21,364 : INFO : PROGRESS: at sentence #950000, processed 7244309 words, keeping 193001 word types\n",
      "2020-09-22 21:51:21,393 : INFO : PROGRESS: at sentence #960000, processed 7320397 words, keeping 194281 word types\n",
      "2020-09-22 21:51:21,423 : INFO : PROGRESS: at sentence #970000, processed 7396738 words, keeping 195565 word types\n",
      "2020-09-22 21:51:21,449 : INFO : PROGRESS: at sentence #980000, processed 7472236 words, keeping 196850 word types\n",
      "2020-09-22 21:51:21,479 : INFO : PROGRESS: at sentence #990000, processed 7548192 words, keeping 198027 word types\n",
      "2020-09-22 21:51:21,515 : INFO : PROGRESS: at sentence #1000000, processed 7623776 words, keeping 199221 word types\n",
      "2020-09-22 21:51:21,549 : INFO : PROGRESS: at sentence #1010000, processed 7699346 words, keeping 200428 word types\n",
      "2020-09-22 21:51:21,587 : INFO : PROGRESS: at sentence #1020000, processed 7775351 words, keeping 201659 word types\n",
      "2020-09-22 21:51:21,620 : INFO : PROGRESS: at sentence #1030000, processed 7851163 words, keeping 202829 word types\n",
      "2020-09-22 21:51:21,650 : INFO : PROGRESS: at sentence #1040000, processed 7927125 words, keeping 204031 word types\n",
      "2020-09-22 21:51:21,689 : INFO : PROGRESS: at sentence #1050000, processed 8002879 words, keeping 205245 word types\n",
      "2020-09-22 21:51:21,714 : INFO : PROGRESS: at sentence #1060000, processed 8079811 words, keeping 206492 word types\n",
      "2020-09-22 21:51:21,742 : INFO : PROGRESS: at sentence #1070000, processed 8155356 words, keeping 207661 word types\n",
      "2020-09-22 21:51:21,770 : INFO : PROGRESS: at sentence #1080000, processed 8232121 words, keeping 208870 word types\n",
      "2020-09-22 21:51:21,796 : INFO : PROGRESS: at sentence #1090000, processed 8308085 words, keeping 210143 word types\n",
      "2020-09-22 21:51:21,829 : INFO : PROGRESS: at sentence #1100000, processed 8384750 words, keeping 211400 word types\n",
      "2020-09-22 21:51:21,858 : INFO : PROGRESS: at sentence #1110000, processed 8461070 words, keeping 212591 word types\n",
      "2020-09-22 21:51:21,886 : INFO : PROGRESS: at sentence #1120000, processed 8536803 words, keeping 213773 word types\n",
      "2020-09-22 21:51:21,927 : INFO : PROGRESS: at sentence #1130000, processed 8613363 words, keeping 214980 word types\n",
      "2020-09-22 21:51:21,966 : INFO : PROGRESS: at sentence #1140000, processed 8689773 words, keeping 216233 word types\n",
      "2020-09-22 21:51:22,011 : INFO : PROGRESS: at sentence #1150000, processed 8765888 words, keeping 217469 word types\n",
      "2020-09-22 21:51:22,040 : INFO : PROGRESS: at sentence #1160000, processed 8841267 words, keeping 218684 word types\n",
      "2020-09-22 21:51:22,073 : INFO : PROGRESS: at sentence #1170000, processed 8916971 words, keeping 219876 word types\n",
      "2020-09-22 21:51:22,105 : INFO : PROGRESS: at sentence #1180000, processed 8993488 words, keeping 221127 word types\n",
      "2020-09-22 21:51:22,143 : INFO : PROGRESS: at sentence #1190000, processed 9068859 words, keeping 222316 word types\n",
      "2020-09-22 21:51:22,173 : INFO : PROGRESS: at sentence #1200000, processed 9144521 words, keeping 223452 word types\n",
      "2020-09-22 21:51:22,208 : INFO : PROGRESS: at sentence #1210000, processed 9219838 words, keeping 224614 word types\n",
      "2020-09-22 21:51:22,239 : INFO : PROGRESS: at sentence #1220000, processed 9295815 words, keeping 225766 word types\n",
      "2020-09-22 21:51:22,275 : INFO : PROGRESS: at sentence #1230000, processed 9371706 words, keeping 226979 word types\n",
      "2020-09-22 21:51:22,304 : INFO : PROGRESS: at sentence #1240000, processed 9448185 words, keeping 228178 word types\n",
      "2020-09-22 21:51:22,333 : INFO : PROGRESS: at sentence #1250000, processed 9525159 words, keeping 229354 word types\n",
      "2020-09-22 21:51:22,364 : INFO : PROGRESS: at sentence #1260000, processed 9601704 words, keeping 230503 word types\n",
      "2020-09-22 21:51:22,398 : INFO : PROGRESS: at sentence #1270000, processed 9678134 words, keeping 231612 word types\n",
      "2020-09-22 21:51:22,429 : INFO : collected 232714 word types from a corpus of 9753977 raw words and 1280000 sentences\n",
      "2020-09-22 21:51:22,430 : INFO : Loading a fresh vocabulary\n",
      "2020-09-22 21:51:22,598 : INFO : effective_min_count=10 retains 25762 unique words (11% of original 232714, drops 206952)\n",
      "2020-09-22 21:51:22,599 : INFO : effective_min_count=10 leaves 9389378 word corpus (96% of original 9753977, drops 364599)\n",
      "2020-09-22 21:51:22,699 : INFO : deleting the raw counts dictionary of 232714 items\n",
      "2020-09-22 21:51:22,709 : INFO : sample=0.001 downsamples 58 most-common words\n",
      "2020-09-22 21:51:22,710 : INFO : downsampling leaves estimated 8347815 word corpus (88.9% of prior 9389378)\n",
      "2020-09-22 21:51:22,826 : INFO : estimated required memory for 25762 words and 256 dimensions: 65641576 bytes\n",
      "2020-09-22 21:51:22,828 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  25762\n"
     ]
    }
   ],
   "source": [
    "cbow_model.build_vocab(train_set)\n",
    "print('Vocab size: ', len(cbow_model.wv.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 21:51:34,196 : INFO : training model with 4 workers on 25762 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=8\n",
      "2020-09-22 21:51:35,238 : INFO : EPOCH 1 - PROGRESS: at 9.45% examples, 784475 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:36,245 : INFO : EPOCH 1 - PROGRESS: at 20.20% examples, 838076 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:37,265 : INFO : EPOCH 1 - PROGRESS: at 31.16% examples, 858097 words/s, in_qsize 8, out_qsize 1\n",
      "2020-09-22 21:51:38,268 : INFO : EPOCH 1 - PROGRESS: at 41.69% examples, 863400 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:39,274 : INFO : EPOCH 1 - PROGRESS: at 52.32% examples, 867459 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:40,278 : INFO : EPOCH 1 - PROGRESS: at 63.48% examples, 877554 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:41,289 : INFO : EPOCH 1 - PROGRESS: at 75.58% examples, 895042 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:42,295 : INFO : EPOCH 1 - PROGRESS: at 87.18% examples, 903158 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:43,299 : INFO : EPOCH 1 - PROGRESS: at 97.86% examples, 901223 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:43,472 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:51:43,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:51:43,489 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:51:43,493 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:51:43,495 : INFO : EPOCH - 1 : training on 9753977 raw words (8347304 effective words) took 9.3s, 901401 effective words/s\n",
      "2020-09-22 21:51:44,510 : INFO : EPOCH 2 - PROGRESS: at 10.48% examples, 870408 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:45,512 : INFO : EPOCH 2 - PROGRESS: at 22.35% examples, 930299 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:46,517 : INFO : EPOCH 2 - PROGRESS: at 33.82% examples, 938178 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:47,517 : INFO : EPOCH 2 - PROGRESS: at 45.38% examples, 945057 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:48,525 : INFO : EPOCH 2 - PROGRESS: at 57.34% examples, 954699 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:49,530 : INFO : EPOCH 2 - PROGRESS: at 69.02% examples, 957501 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:50,538 : INFO : EPOCH 2 - PROGRESS: at 79.60% examples, 945525 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:51,544 : INFO : EPOCH 2 - PROGRESS: at 91.60% examples, 951629 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:52,215 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:51:52,222 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:51:52,229 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:51:52,233 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:51:52,234 : INFO : EPOCH - 2 : training on 9753977 raw words (8347889 effective words) took 8.7s, 956550 effective words/s\n",
      "2020-09-22 21:51:53,248 : INFO : EPOCH 3 - PROGRESS: at 11.81% examples, 984030 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:54,256 : INFO : EPOCH 3 - PROGRESS: at 23.89% examples, 992547 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:55,262 : INFO : EPOCH 3 - PROGRESS: at 36.17% examples, 1001598 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:51:56,264 : INFO : EPOCH 3 - PROGRESS: at 48.24% examples, 1003284 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:51:57,269 : INFO : EPOCH 3 - PROGRESS: at 60.42% examples, 1005196 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:58,272 : INFO : EPOCH 3 - PROGRESS: at 72.40% examples, 1003835 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:51:59,279 : INFO : EPOCH 3 - PROGRESS: at 84.52% examples, 1003840 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:00,282 : INFO : EPOCH 3 - PROGRESS: at 96.44% examples, 1001858 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:00,547 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:52:00,555 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:52:00,562 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:52:00,566 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:52:00,567 : INFO : EPOCH - 3 : training on 9753977 raw words (8347470 effective words) took 8.3s, 1003319 effective words/s\n",
      "2020-09-22 21:52:01,589 : INFO : EPOCH 4 - PROGRESS: at 11.81% examples, 975095 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:02,590 : INFO : EPOCH 4 - PROGRESS: at 22.76% examples, 944501 words/s, in_qsize 8, out_qsize 1\n",
      "2020-09-22 21:52:03,599 : INFO : EPOCH 4 - PROGRESS: at 34.95% examples, 965876 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:04,604 : INFO : EPOCH 4 - PROGRESS: at 47.11% examples, 977627 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:05,611 : INFO : EPOCH 4 - PROGRESS: at 58.58% examples, 972436 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:06,616 : INFO : EPOCH 4 - PROGRESS: at 69.43% examples, 960735 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:07,619 : INFO : EPOCH 4 - PROGRESS: at 81.45% examples, 966058 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:08,626 : INFO : EPOCH 4 - PROGRESS: at 93.56% examples, 970607 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:09,132 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:52:09,135 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:52:09,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:52:09,147 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:52:09,148 : INFO : EPOCH - 4 : training on 9753977 raw words (8346070 effective words) took 8.6s, 974126 effective words/s\n",
      "2020-09-22 21:52:10,172 : INFO : EPOCH 5 - PROGRESS: at 11.81% examples, 971981 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:11,183 : INFO : EPOCH 5 - PROGRESS: at 23.99% examples, 989933 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:12,185 : INFO : EPOCH 5 - PROGRESS: at 35.66% examples, 984124 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:13,194 : INFO : EPOCH 5 - PROGRESS: at 47.83% examples, 990225 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:14,201 : INFO : EPOCH 5 - PROGRESS: at 59.90% examples, 992949 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:15,202 : INFO : EPOCH 5 - PROGRESS: at 71.99% examples, 995479 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:16,202 : INFO : EPOCH 5 - PROGRESS: at 84.01% examples, 996255 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:17,211 : INFO : EPOCH 5 - PROGRESS: at 96.24% examples, 997818 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:17,491 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:52:17,499 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:52:17,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:52:17,508 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:52:17,510 : INFO : EPOCH - 5 : training on 9753977 raw words (8348072 effective words) took 8.3s, 999861 effective words/s\n",
      "2020-09-22 21:52:18,525 : INFO : EPOCH 6 - PROGRESS: at 11.81% examples, 982039 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:19,532 : INFO : EPOCH 6 - PROGRESS: at 23.68% examples, 983603 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:20,535 : INFO : EPOCH 6 - PROGRESS: at 35.66% examples, 988605 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:21,535 : INFO : EPOCH 6 - PROGRESS: at 47.32% examples, 985255 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:22,535 : INFO : EPOCH 6 - PROGRESS: at 57.96% examples, 966153 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:23,540 : INFO : EPOCH 6 - PROGRESS: at 69.84% examples, 969827 words/s, in_qsize 7, out_qsize 1\n",
      "2020-09-22 21:52:24,540 : INFO : EPOCH 6 - PROGRESS: at 81.97% examples, 975391 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:25,540 : INFO : EPOCH 6 - PROGRESS: at 93.87% examples, 977284 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 21:52:26,030 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:52:26,040 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:52:26,049 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:52:26,051 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:52:26,052 : INFO : EPOCH - 6 : training on 9753977 raw words (8346132 effective words) took 8.5s, 978619 effective words/s\n",
      "2020-09-22 21:52:27,073 : INFO : EPOCH 7 - PROGRESS: at 12.12% examples, 1005888 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:28,077 : INFO : EPOCH 7 - PROGRESS: at 24.09% examples, 1001281 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:29,083 : INFO : EPOCH 7 - PROGRESS: at 36.27% examples, 1004596 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:30,084 : INFO : EPOCH 7 - PROGRESS: at 47.43% examples, 986320 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:31,089 : INFO : EPOCH 7 - PROGRESS: at 57.55% examples, 957716 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:32,105 : INFO : EPOCH 7 - PROGRESS: at 68.41% examples, 946873 words/s, in_qsize 7, out_qsize 1\n",
      "2020-09-22 21:52:33,109 : INFO : EPOCH 7 - PROGRESS: at 78.47% examples, 930857 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:34,113 : INFO : EPOCH 7 - PROGRESS: at 88.52% examples, 918903 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:35,124 : INFO : EPOCH 7 - PROGRESS: at 99.50% examples, 917401 words/s, in_qsize 3, out_qsize 3\n",
      "2020-09-22 21:52:35,132 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:52:35,135 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:52:35,147 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:52:35,149 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:52:35,150 : INFO : EPOCH - 7 : training on 9753977 raw words (8348302 effective words) took 9.1s, 919307 effective words/s\n",
      "2020-09-22 21:52:36,166 : INFO : EPOCH 8 - PROGRESS: at 11.92% examples, 989531 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:37,170 : INFO : EPOCH 8 - PROGRESS: at 23.89% examples, 992868 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:38,172 : INFO : EPOCH 8 - PROGRESS: at 34.34% examples, 952491 words/s, in_qsize 8, out_qsize 2\n",
      "2020-09-22 21:52:39,172 : INFO : EPOCH 8 - PROGRESS: at 46.19% examples, 962403 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:40,172 : INFO : EPOCH 8 - PROGRESS: at 57.96% examples, 966695 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:41,174 : INFO : EPOCH 8 - PROGRESS: at 69.33% examples, 963685 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:42,179 : INFO : EPOCH 8 - PROGRESS: at 80.94% examples, 963338 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:43,180 : INFO : EPOCH 8 - PROGRESS: at 92.63% examples, 964524 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:43,756 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:52:43,761 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:52:43,772 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:52:43,776 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:52:43,777 : INFO : EPOCH - 8 : training on 9753977 raw words (8347237 effective words) took 8.6s, 969096 effective words/s\n",
      "2020-09-22 21:52:44,797 : INFO : EPOCH 9 - PROGRESS: at 10.99% examples, 909718 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:45,801 : INFO : EPOCH 9 - PROGRESS: at 22.76% examples, 944915 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:46,810 : INFO : EPOCH 9 - PROGRESS: at 34.75% examples, 960322 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:47,811 : INFO : EPOCH 9 - PROGRESS: at 46.30% examples, 961759 words/s, in_qsize 8, out_qsize 1\n",
      "2020-09-22 21:52:48,811 : INFO : EPOCH 9 - PROGRESS: at 57.65% examples, 959368 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:49,815 : INFO : EPOCH 9 - PROGRESS: at 69.02% examples, 957122 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:50,819 : INFO : EPOCH 9 - PROGRESS: at 80.63% examples, 957992 words/s, in_qsize 6, out_qsize 1\n",
      "2020-09-22 21:52:51,823 : INFO : EPOCH 9 - PROGRESS: at 92.32% examples, 959542 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:52,426 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:52:52,432 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:52:52,444 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:52:52,449 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:52:52,451 : INFO : EPOCH - 9 : training on 9753977 raw words (8348050 effective words) took 8.7s, 963892 effective words/s\n",
      "2020-09-22 21:52:53,464 : INFO : EPOCH 10 - PROGRESS: at 11.09% examples, 923109 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:54,473 : INFO : EPOCH 10 - PROGRESS: at 21.74% examples, 902344 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:55,473 : INFO : EPOCH 10 - PROGRESS: at 33.72% examples, 934765 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:56,489 : INFO : EPOCH 10 - PROGRESS: at 45.79% examples, 949845 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:52:57,490 : INFO : EPOCH 10 - PROGRESS: at 57.44% examples, 954707 words/s, in_qsize 6, out_qsize 2\n",
      "2020-09-22 21:52:58,496 : INFO : EPOCH 10 - PROGRESS: at 69.33% examples, 960084 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:52:59,508 : INFO : EPOCH 10 - PROGRESS: at 77.84% examples, 922921 words/s, in_qsize 6, out_qsize 0\n",
      "2020-09-22 21:53:00,509 : INFO : EPOCH 10 - PROGRESS: at 88.82% examples, 921862 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:01,410 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:53:01,418 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:53:01,428 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:53:01,429 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:53:01,430 : INFO : EPOCH - 10 : training on 9753977 raw words (8347877 effective words) took 9.0s, 930961 effective words/s\n",
      "2020-09-22 21:53:02,444 : INFO : EPOCH 11 - PROGRESS: at 11.81% examples, 982646 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:03,444 : INFO : EPOCH 11 - PROGRESS: at 23.38% examples, 974165 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:04,460 : INFO : EPOCH 11 - PROGRESS: at 34.85% examples, 963821 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:05,462 : INFO : EPOCH 11 - PROGRESS: at 46.81% examples, 972562 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:06,466 : INFO : EPOCH 11 - PROGRESS: at 58.98% examples, 981113 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:07,474 : INFO : EPOCH 11 - PROGRESS: at 70.66% examples, 978809 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:08,482 : INFO : EPOCH 11 - PROGRESS: at 82.47% examples, 978378 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:09,482 : INFO : EPOCH 11 - PROGRESS: at 93.97% examples, 975764 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:10,064 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:53:10,076 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:53:10,082 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:53:10,084 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:53:10,085 : INFO : EPOCH - 11 : training on 9753977 raw words (8348410 effective words) took 8.6s, 965864 effective words/s\n",
      "2020-09-22 21:53:11,104 : INFO : EPOCH 12 - PROGRESS: at 11.30% examples, 936544 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:12,117 : INFO : EPOCH 12 - PROGRESS: at 19.48% examples, 805610 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:13,121 : INFO : EPOCH 12 - PROGRESS: at 29.12% examples, 804199 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:14,123 : INFO : EPOCH 12 - PROGRESS: at 39.86% examples, 827096 words/s, in_qsize 6, out_qsize 1\n",
      "2020-09-22 21:53:15,131 : INFO : EPOCH 12 - PROGRESS: at 49.57% examples, 822999 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 21:53:16,134 : INFO : EPOCH 12 - PROGRESS: at 59.29% examples, 820848 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:17,142 : INFO : EPOCH 12 - PROGRESS: at 69.12% examples, 820120 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:18,154 : INFO : EPOCH 12 - PROGRESS: at 79.29% examples, 822203 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:19,155 : INFO : EPOCH 12 - PROGRESS: at 90.68% examples, 836086 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:19,932 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:53:19,939 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:53:19,945 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:53:19,952 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:53:19,953 : INFO : EPOCH - 12 : training on 9753977 raw words (8348569 effective words) took 9.9s, 847179 effective words/s\n",
      "2020-09-22 21:53:20,968 : INFO : EPOCH 13 - PROGRESS: at 11.81% examples, 981841 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:21,970 : INFO : EPOCH 13 - PROGRESS: at 23.68% examples, 986504 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:22,973 : INFO : EPOCH 13 - PROGRESS: at 34.65% examples, 961960 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:23,974 : INFO : EPOCH 13 - PROGRESS: at 45.48% examples, 947592 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:24,979 : INFO : EPOCH 13 - PROGRESS: at 56.41% examples, 940223 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:25,986 : INFO : EPOCH 13 - PROGRESS: at 66.66% examples, 925233 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:26,991 : INFO : EPOCH 13 - PROGRESS: at 77.74% examples, 924292 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:27,993 : INFO : EPOCH 13 - PROGRESS: at 89.33% examples, 929306 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:28,861 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:53:28,863 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:53:28,874 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:53:28,876 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:53:28,878 : INFO : EPOCH - 13 : training on 9753977 raw words (8347203 effective words) took 8.9s, 936751 effective words/s\n",
      "2020-09-22 21:53:29,898 : INFO : EPOCH 14 - PROGRESS: at 11.51% examples, 951523 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:30,899 : INFO : EPOCH 14 - PROGRESS: at 23.48% examples, 975271 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:31,900 : INFO : EPOCH 14 - PROGRESS: at 34.95% examples, 969206 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:32,906 : INFO : EPOCH 14 - PROGRESS: at 46.71% examples, 971529 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:33,910 : INFO : EPOCH 14 - PROGRESS: at 58.27% examples, 969693 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:34,912 : INFO : EPOCH 14 - PROGRESS: at 70.05% examples, 971650 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:35,918 : INFO : EPOCH 14 - PROGRESS: at 81.55% examples, 968999 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:36,920 : INFO : EPOCH 14 - PROGRESS: at 93.35% examples, 970528 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:37,462 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:53:37,467 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:53:37,476 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:53:37,478 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:53:37,478 : INFO : EPOCH - 14 : training on 9753977 raw words (8347028 effective words) took 8.6s, 972047 effective words/s\n",
      "2020-09-22 21:53:38,495 : INFO : EPOCH 15 - PROGRESS: at 11.92% examples, 989220 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:39,503 : INFO : EPOCH 15 - PROGRESS: at 23.58% examples, 978170 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:40,515 : INFO : EPOCH 15 - PROGRESS: at 35.46% examples, 978828 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:41,520 : INFO : EPOCH 15 - PROGRESS: at 47.11% examples, 976978 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:42,529 : INFO : EPOCH 15 - PROGRESS: at 57.24% examples, 949248 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:43,535 : INFO : EPOCH 15 - PROGRESS: at 68.31% examples, 944133 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:44,537 : INFO : EPOCH 15 - PROGRESS: at 79.91% examples, 947144 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:45,540 : INFO : EPOCH 15 - PROGRESS: at 91.60% examples, 950112 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:46,260 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:53:46,267 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:53:46,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:53:46,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:53:46,280 : INFO : EPOCH - 15 : training on 9753977 raw words (8347672 effective words) took 8.8s, 949727 effective words/s\n",
      "2020-09-22 21:53:47,301 : INFO : EPOCH 16 - PROGRESS: at 11.51% examples, 950520 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:48,304 : INFO : EPOCH 16 - PROGRESS: at 23.38% examples, 970470 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:49,308 : INFO : EPOCH 16 - PROGRESS: at 35.25% examples, 976421 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:50,313 : INFO : EPOCH 16 - PROGRESS: at 47.22% examples, 981096 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:51,317 : INFO : EPOCH 16 - PROGRESS: at 59.19% examples, 984465 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:52,322 : INFO : EPOCH 16 - PROGRESS: at 70.86% examples, 982023 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:53,323 : INFO : EPOCH 16 - PROGRESS: at 82.07% examples, 974744 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:54,325 : INFO : EPOCH 16 - PROGRESS: at 93.56% examples, 972269 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:54,844 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:53:54,853 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:53:54,861 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:53:54,863 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:53:54,864 : INFO : EPOCH - 16 : training on 9753977 raw words (8347205 effective words) took 8.6s, 973901 effective words/s\n",
      "2020-09-22 21:53:55,889 : INFO : EPOCH 17 - PROGRESS: at 11.51% examples, 946353 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:53:56,894 : INFO : EPOCH 17 - PROGRESS: at 23.27% examples, 963128 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:57,894 : INFO : EPOCH 17 - PROGRESS: at 33.62% examples, 930169 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:58,895 : INFO : EPOCH 17 - PROGRESS: at 44.45% examples, 923903 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:53:59,897 : INFO : EPOCH 17 - PROGRESS: at 56.00% examples, 932258 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:00,899 : INFO : EPOCH 17 - PROGRESS: at 67.69% examples, 939132 words/s, in_qsize 6, out_qsize 1\n",
      "2020-09-22 21:54:01,902 : INFO : EPOCH 17 - PROGRESS: at 79.80% examples, 948735 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:02,906 : INFO : EPOCH 17 - PROGRESS: at 91.50% examples, 951578 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:03,579 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:54:03,587 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:54:03,596 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:54:03,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:54:03,600 : INFO : EPOCH - 17 : training on 9753977 raw words (8348290 effective words) took 8.7s, 957064 effective words/s\n",
      "2020-09-22 21:54:04,620 : INFO : EPOCH 18 - PROGRESS: at 11.51% examples, 952171 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:05,621 : INFO : EPOCH 18 - PROGRESS: at 23.27% examples, 966981 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 21:54:06,625 : INFO : EPOCH 18 - PROGRESS: at 35.05% examples, 971379 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:07,629 : INFO : EPOCH 18 - PROGRESS: at 46.09% examples, 958656 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:08,645 : INFO : EPOCH 18 - PROGRESS: at 57.24% examples, 950578 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:09,644 : INFO : EPOCH 18 - PROGRESS: at 68.51% examples, 949038 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:10,650 : INFO : EPOCH 18 - PROGRESS: at 78.98% examples, 937358 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:11,654 : INFO : EPOCH 18 - PROGRESS: at 89.85% examples, 933165 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:12,569 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:54:12,571 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:54:12,576 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:54:12,582 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:54:12,583 : INFO : EPOCH - 18 : training on 9753977 raw words (8347573 effective words) took 9.0s, 930592 effective words/s\n",
      "2020-09-22 21:54:13,596 : INFO : EPOCH 19 - PROGRESS: at 10.78% examples, 897791 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:14,603 : INFO : EPOCH 19 - PROGRESS: at 20.20% examples, 839665 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:15,610 : INFO : EPOCH 19 - PROGRESS: at 30.86% examples, 854207 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:16,613 : INFO : EPOCH 19 - PROGRESS: at 42.00% examples, 872799 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:17,614 : INFO : EPOCH 19 - PROGRESS: at 52.84% examples, 879532 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:18,617 : INFO : EPOCH 19 - PROGRESS: at 63.68% examples, 883615 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:19,623 : INFO : EPOCH 19 - PROGRESS: at 74.45% examples, 884938 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:20,626 : INFO : EPOCH 19 - PROGRESS: at 85.54% examples, 889597 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:21,627 : INFO : EPOCH 19 - PROGRESS: at 95.62% examples, 883633 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:21,984 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:54:21,989 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:54:21,998 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:54:22,004 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:54:22,005 : INFO : EPOCH - 19 : training on 9753977 raw words (8347088 effective words) took 9.4s, 887114 effective words/s\n",
      "2020-09-22 21:54:23,022 : INFO : EPOCH 20 - PROGRESS: at 11.40% examples, 946232 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:24,028 : INFO : EPOCH 20 - PROGRESS: at 22.87% examples, 949294 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:25,028 : INFO : EPOCH 20 - PROGRESS: at 34.34% examples, 952197 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:26,033 : INFO : EPOCH 20 - PROGRESS: at 45.79% examples, 952426 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:27,037 : INFO : EPOCH 20 - PROGRESS: at 56.73% examples, 944362 words/s, in_qsize 6, out_qsize 1\n",
      "2020-09-22 21:54:28,048 : INFO : EPOCH 20 - PROGRESS: at 67.58% examples, 936176 words/s, in_qsize 6, out_qsize 1\n",
      "2020-09-22 21:54:29,054 : INFO : EPOCH 20 - PROGRESS: at 78.98% examples, 937367 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:30,056 : INFO : EPOCH 20 - PROGRESS: at 89.13% examples, 925780 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:31,003 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:54:31,012 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:54:31,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:54:31,021 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:54:31,022 : INFO : EPOCH - 20 : training on 9753977 raw words (8345637 effective words) took 9.0s, 926967 effective words/s\n",
      "2020-09-22 21:54:32,040 : INFO : EPOCH 21 - PROGRESS: at 11.61% examples, 961448 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:33,041 : INFO : EPOCH 21 - PROGRESS: at 23.68% examples, 984727 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:34,047 : INFO : EPOCH 21 - PROGRESS: at 35.46% examples, 982888 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:35,050 : INFO : EPOCH 21 - PROGRESS: at 46.81% examples, 973534 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:36,056 : INFO : EPOCH 21 - PROGRESS: at 58.47% examples, 972831 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:37,058 : INFO : EPOCH 21 - PROGRESS: at 69.64% examples, 965934 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:38,060 : INFO : EPOCH 21 - PROGRESS: at 80.84% examples, 960926 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:39,079 : INFO : EPOCH 21 - PROGRESS: at 91.09% examples, 945510 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:39,848 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:54:39,856 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:54:39,865 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:54:39,866 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:54:39,868 : INFO : EPOCH - 21 : training on 9753977 raw words (8348056 effective words) took 8.8s, 945125 effective words/s\n",
      "2020-09-22 21:54:40,884 : INFO : EPOCH 22 - PROGRESS: at 11.71% examples, 971527 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:41,885 : INFO : EPOCH 22 - PROGRESS: at 23.58% examples, 981773 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:42,895 : INFO : EPOCH 22 - PROGRESS: at 35.66% examples, 987571 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:43,895 : INFO : EPOCH 22 - PROGRESS: at 47.11% examples, 980108 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:44,896 : INFO : EPOCH 22 - PROGRESS: at 58.98% examples, 982475 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:45,901 : INFO : EPOCH 22 - PROGRESS: at 70.05% examples, 972080 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:46,903 : INFO : EPOCH 22 - PROGRESS: at 81.24% examples, 966152 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:47,905 : INFO : EPOCH 22 - PROGRESS: at 92.73% examples, 964799 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:48,471 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:54:48,474 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:54:48,484 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:54:48,486 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:54:48,487 : INFO : EPOCH - 22 : training on 9753977 raw words (8347647 effective words) took 8.6s, 969938 effective words/s\n",
      "2020-09-22 21:54:49,503 : INFO : EPOCH 23 - PROGRESS: at 11.51% examples, 954591 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:54:50,510 : INFO : EPOCH 23 - PROGRESS: at 23.27% examples, 965969 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:51,513 : INFO : EPOCH 23 - PROGRESS: at 34.65% examples, 959545 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:52,520 : INFO : EPOCH 23 - PROGRESS: at 45.89% examples, 953016 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:53,528 : INFO : EPOCH 23 - PROGRESS: at 55.80% examples, 927164 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:54,533 : INFO : EPOCH 23 - PROGRESS: at 66.56% examples, 921642 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:55,542 : INFO : EPOCH 23 - PROGRESS: at 77.84% examples, 923185 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:56,545 : INFO : EPOCH 23 - PROGRESS: at 89.64% examples, 930428 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:57,379 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:54:57,386 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:54:57,400 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:54:57,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 21:54:57,404 : INFO : EPOCH - 23 : training on 9753977 raw words (8348793 effective words) took 8.9s, 937470 effective words/s\n",
      "2020-09-22 21:54:58,423 : INFO : EPOCH 24 - PROGRESS: at 10.89% examples, 903107 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:54:59,425 : INFO : EPOCH 24 - PROGRESS: at 22.87% examples, 950674 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:00,426 : INFO : EPOCH 24 - PROGRESS: at 34.44% examples, 955479 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:01,436 : INFO : EPOCH 24 - PROGRESS: at 44.97% examples, 934671 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:02,437 : INFO : EPOCH 24 - PROGRESS: at 55.49% examples, 923897 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:03,444 : INFO : EPOCH 24 - PROGRESS: at 65.64% examples, 910035 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:04,445 : INFO : EPOCH 24 - PROGRESS: at 77.33% examples, 919300 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:05,454 : INFO : EPOCH 24 - PROGRESS: at 88.52% examples, 919894 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:06,402 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:55:06,404 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:55:06,418 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:55:06,420 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:55:06,421 : INFO : EPOCH - 24 : training on 9753977 raw words (8348082 effective words) took 9.0s, 927261 effective words/s\n",
      "2020-09-22 21:55:07,434 : INFO : EPOCH 25 - PROGRESS: at 11.20% examples, 932314 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:08,439 : INFO : EPOCH 25 - PROGRESS: at 22.87% examples, 951225 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:09,439 : INFO : EPOCH 25 - PROGRESS: at 32.60% examples, 905107 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:10,442 : INFO : EPOCH 25 - PROGRESS: at 43.23% examples, 900754 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:55:11,446 : INFO : EPOCH 25 - PROGRESS: at 54.67% examples, 911494 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:12,454 : INFO : EPOCH 25 - PROGRESS: at 65.95% examples, 915198 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:13,456 : INFO : EPOCH 25 - PROGRESS: at 76.40% examples, 909003 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:14,456 : INFO : EPOCH 25 - PROGRESS: at 87.70% examples, 913016 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:15,457 : INFO : EPOCH 25 - PROGRESS: at 99.71% examples, 922461 words/s, in_qsize 3, out_qsize 1\n",
      "2020-09-22 21:55:15,458 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:55:15,467 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:55:15,473 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:55:15,477 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:55:15,477 : INFO : EPOCH - 25 : training on 9753977 raw words (8348534 effective words) took 9.0s, 923075 effective words/s\n",
      "2020-09-22 21:55:16,496 : INFO : EPOCH 26 - PROGRESS: at 11.61% examples, 962235 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:55:17,501 : INFO : EPOCH 26 - PROGRESS: at 22.66% examples, 940467 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:18,513 : INFO : EPOCH 26 - PROGRESS: at 32.50% examples, 897368 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:19,514 : INFO : EPOCH 26 - PROGRESS: at 44.15% examples, 916492 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:20,516 : INFO : EPOCH 26 - PROGRESS: at 55.80% examples, 927862 words/s, in_qsize 7, out_qsize 1\n",
      "2020-09-22 21:55:21,522 : INFO : EPOCH 26 - PROGRESS: at 67.68% examples, 937734 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:22,530 : INFO : EPOCH 26 - PROGRESS: at 79.80% examples, 946755 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:23,539 : INFO : EPOCH 26 - PROGRESS: at 91.30% examples, 947085 words/s, in_qsize 7, out_qsize 1\n",
      "2020-09-22 21:55:24,237 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:55:24,242 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:55:24,249 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:55:24,253 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:55:24,254 : INFO : EPOCH - 26 : training on 9753977 raw words (8348073 effective words) took 8.8s, 952579 effective words/s\n",
      "2020-09-22 21:55:25,271 : INFO : EPOCH 27 - PROGRESS: at 11.30% examples, 938235 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:26,276 : INFO : EPOCH 27 - PROGRESS: at 23.27% examples, 966973 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:55:27,280 : INFO : EPOCH 27 - PROGRESS: at 35.15% examples, 973923 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:28,285 : INFO : EPOCH 27 - PROGRESS: at 46.81% examples, 973223 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:29,294 : INFO : EPOCH 27 - PROGRESS: at 58.58% examples, 973489 words/s, in_qsize 8, out_qsize 1\n",
      "2020-09-22 21:55:30,301 : INFO : EPOCH 27 - PROGRESS: at 70.66% examples, 978307 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:31,309 : INFO : EPOCH 27 - PROGRESS: at 82.58% examples, 979207 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:55:32,312 : INFO : EPOCH 27 - PROGRESS: at 94.49% examples, 980455 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:32,751 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:55:32,758 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:55:32,768 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:55:32,771 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:55:32,772 : INFO : EPOCH - 27 : training on 9753977 raw words (8347622 effective words) took 8.5s, 981536 effective words/s\n",
      "2020-09-22 21:55:33,790 : INFO : EPOCH 28 - PROGRESS: at 10.68% examples, 885916 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:34,790 : INFO : EPOCH 28 - PROGRESS: at 21.54% examples, 896003 words/s, in_qsize 8, out_qsize 1\n",
      "2020-09-22 21:55:35,794 : INFO : EPOCH 28 - PROGRESS: at 33.62% examples, 932603 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:36,804 : INFO : EPOCH 28 - PROGRESS: at 45.79% examples, 951703 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:37,805 : INFO : EPOCH 28 - PROGRESS: at 57.65% examples, 959597 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:55:38,810 : INFO : EPOCH 28 - PROGRESS: at 69.94% examples, 970011 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:55:39,819 : INFO : EPOCH 28 - PROGRESS: at 81.97% examples, 973236 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:40,827 : INFO : EPOCH 28 - PROGRESS: at 93.45% examples, 970308 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:41,439 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:55:41,444 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:55:41,456 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:55:41,460 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:55:41,461 : INFO : EPOCH - 28 : training on 9753977 raw words (8347841 effective words) took 8.7s, 962292 effective words/s\n",
      "2020-09-22 21:55:42,480 : INFO : EPOCH 29 - PROGRESS: at 11.30% examples, 936476 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:43,488 : INFO : EPOCH 29 - PROGRESS: at 22.55% examples, 934696 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:44,499 : INFO : EPOCH 29 - PROGRESS: at 33.93% examples, 936248 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:55:45,503 : INFO : EPOCH 29 - PROGRESS: at 45.48% examples, 942865 words/s, in_qsize 8, out_qsize 1\n",
      "2020-09-22 21:55:46,504 : INFO : EPOCH 29 - PROGRESS: at 56.73% examples, 942391 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:47,506 : INFO : EPOCH 29 - PROGRESS: at 67.99% examples, 941901 words/s, in_qsize 6, out_qsize 1\n",
      "2020-09-22 21:55:48,507 : INFO : EPOCH 29 - PROGRESS: at 80.22% examples, 952646 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:49,519 : INFO : EPOCH 29 - PROGRESS: at 91.50% examples, 949694 words/s, in_qsize 6, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 21:55:50,408 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:55:50,415 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:55:50,420 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:55:50,422 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:55:50,424 : INFO : EPOCH - 29 : training on 9753977 raw words (8348049 effective words) took 9.0s, 932734 effective words/s\n",
      "2020-09-22 21:55:51,448 : INFO : EPOCH 30 - PROGRESS: at 10.58% examples, 873667 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:52,463 : INFO : EPOCH 30 - PROGRESS: at 22.15% examples, 913016 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:55:53,465 : INFO : EPOCH 30 - PROGRESS: at 33.62% examples, 927259 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:54,470 : INFO : EPOCH 30 - PROGRESS: at 45.69% examples, 946536 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:55,476 : INFO : EPOCH 30 - PROGRESS: at 57.03% examples, 946077 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:56,477 : INFO : EPOCH 30 - PROGRESS: at 68.71% examples, 950782 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:57,486 : INFO : EPOCH 30 - PROGRESS: at 79.80% examples, 945783 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:55:58,491 : INFO : EPOCH 30 - PROGRESS: at 91.40% examples, 947711 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:55:59,219 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:55:59,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:55:59,229 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:55:59,234 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:55:59,235 : INFO : EPOCH - 30 : training on 9753977 raw words (8348019 effective words) took 8.8s, 949013 effective words/s\n",
      "2020-09-22 21:56:00,261 : INFO : EPOCH 31 - PROGRESS: at 10.58% examples, 871315 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:01,262 : INFO : EPOCH 31 - PROGRESS: at 19.48% examples, 807716 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:02,268 : INFO : EPOCH 31 - PROGRESS: at 30.45% examples, 841771 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:03,277 : INFO : EPOCH 31 - PROGRESS: at 39.96% examples, 828207 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:04,282 : INFO : EPOCH 31 - PROGRESS: at 48.75% examples, 809107 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:05,291 : INFO : EPOCH 31 - PROGRESS: at 59.90% examples, 828345 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:06,292 : INFO : EPOCH 31 - PROGRESS: at 70.05% examples, 830849 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:07,296 : INFO : EPOCH 31 - PROGRESS: at 82.07% examples, 851583 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:08,303 : INFO : EPOCH 31 - PROGRESS: at 93.45% examples, 861643 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:08,813 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:56:08,815 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:56:08,822 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:56:08,830 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:56:08,832 : INFO : EPOCH - 31 : training on 9753977 raw words (8347297 effective words) took 9.6s, 871072 effective words/s\n",
      "2020-09-22 21:56:09,849 : INFO : EPOCH 32 - PROGRESS: at 10.89% examples, 905730 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:56:10,855 : INFO : EPOCH 32 - PROGRESS: at 22.97% examples, 954161 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:11,856 : INFO : EPOCH 32 - PROGRESS: at 34.54% examples, 957924 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:12,857 : INFO : EPOCH 32 - PROGRESS: at 46.61% examples, 970556 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:13,857 : INFO : EPOCH 32 - PROGRESS: at 57.03% examples, 951029 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:14,862 : INFO : EPOCH 32 - PROGRESS: at 66.87% examples, 928656 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-22 21:56:15,873 : INFO : EPOCH 32 - PROGRESS: at 79.08% examples, 939984 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-22 21:56:16,875 : INFO : EPOCH 32 - PROGRESS: at 90.26% examples, 938741 words/s, in_qsize 6, out_qsize 1\n",
      "2020-09-22 21:56:17,692 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-22 21:56:17,702 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-22 21:56:17,706 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-22 21:56:17,711 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-22 21:56:17,711 : INFO : EPOCH - 32 : training on 9753977 raw words (8347368 effective words) took 8.9s, 941667 effective words/s\n",
      "2020-09-22 21:56:17,712 : INFO : training on a 312127264 raw words (267124459 effective words) took 283.5s, 942189 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(267124459, 312127264)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cbow_model.train(train_set, total_examples=len(train_set),epochs=model_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fantastic', 0.7758585214614868),\n",
       " ('wonderful', 0.7226381301879883),\n",
       " ('good', 0.6906130313873291),\n",
       " ('awesome', 0.6826470494270325),\n",
       " ('amazing', 0.6501271724700928),\n",
       " ('fabulous', 0.6395999193191528),\n",
       " ('excellent', 0.6273428201675415),\n",
       " ('fab', 0.6175312995910645),\n",
       " ('nice', 0.5801827907562256),\n",
       " ('amaze', 0.5740725994110107)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most similar words\n",
    "\n",
    "cbow_model.wv.most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of array:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.44429505,  0.23518018,  0.06412721,  1.2245287 ,  1.0157188 ,\n",
       "       -0.3377532 , -1.207964  , -0.25429347,  0.9659266 , -0.5499973 ,\n",
       "       -1.0439688 ,  1.5073498 ,  0.9097087 ,  1.4931809 , -0.82146996,\n",
       "       -0.30336332, -1.6721662 , -1.0133312 ,  0.95516783, -0.945025  ,\n",
       "       -0.22023506,  1.2074769 ,  0.7996962 , -0.79653096,  0.6997143 ,\n",
       "       -0.36372596, -0.08584122, -0.45974258,  0.34157288,  0.9178934 ,\n",
       "        1.8157425 ,  0.54358864, -0.5472745 ,  0.6433603 ,  0.4254519 ,\n",
       "        0.9968094 , -0.9540379 ,  1.2603736 ,  1.382922  , -0.75961983,\n",
       "       -1.2407627 , -0.652299  , -0.90325624, -0.33656508, -0.13726255,\n",
       "       -0.70761377, -1.4578966 ,  0.630882  , -2.774012  , -1.031076  ,\n",
       "        0.69106287,  1.3046227 , -0.09744342, -0.13449739, -0.49499324,\n",
       "       -0.19178785, -1.2465953 ,  0.44321957,  0.4177703 ,  2.0692544 ,\n",
       "       -0.51653224,  0.11155434,  0.67808074,  1.2080457 ,  0.9004645 ,\n",
       "        1.2117167 ,  0.8216612 ,  0.11337686,  1.2734159 ,  0.9943214 ,\n",
       "       -2.120547  ,  0.91318965,  0.54682934, -1.6216669 , -1.4184915 ,\n",
       "       -0.23234728, -0.06819919, -0.58965653,  0.8157927 ,  0.09875819,\n",
       "        0.06779445, -0.19327243,  1.5703048 ,  0.47703505, -0.9503153 ,\n",
       "       -1.0766743 , -0.24984476,  1.1562082 ,  1.1606472 ,  1.1802766 ,\n",
       "       -0.6365217 , -1.3202963 , -0.03939469, -0.16587844, -1.1703703 ,\n",
       "        0.20039067, -0.82525855,  0.3821709 ,  0.04077233, -0.71370876,\n",
       "       -1.198887  ,  0.62131786, -0.43718165, -0.9885607 , -1.3923533 ,\n",
       "       -0.07114001, -0.01556184,  0.77824354,  1.2801142 ,  0.17490596,\n",
       "       -0.6364821 , -0.14256166,  0.7106099 ,  1.1751326 ,  0.2581702 ,\n",
       "       -0.31465265, -0.4496725 ,  0.10384853, -0.6459756 ,  0.04988305,\n",
       "       -1.4186887 , -1.0182142 , -0.27212185, -0.16041327, -0.5333205 ,\n",
       "        0.08497153,  2.2139647 , -0.08247083, -0.07967417, -0.6684321 ,\n",
       "       -0.7411422 , -0.1133072 ,  0.93441063,  1.0814643 ,  0.5251267 ,\n",
       "       -0.07702292,  1.2497549 ,  0.30315697, -0.94994885, -0.15417676,\n",
       "        0.7816651 ,  1.117283  ,  1.0062727 ,  0.3633944 ,  0.39682588,\n",
       "        0.4048874 , -0.9087725 ,  0.17115706, -0.27792302,  0.7849472 ,\n",
       "       -0.39325467, -0.11377314,  0.513171  ,  0.24748316,  1.6186069 ,\n",
       "       -0.08103693, -0.627394  , -0.00894927, -1.1361744 ,  0.92542297,\n",
       "       -0.8863243 , -0.5856476 , -0.3248462 ,  0.4499253 , -1.0127302 ,\n",
       "       -0.5270207 ,  0.07290668,  0.6214325 ,  0.5204873 ,  0.11070704,\n",
       "       -0.4698535 ,  1.4553348 ,  1.0484339 ,  1.5914718 , -0.8116307 ,\n",
       "        0.06948481,  0.624041  , -0.46638328, -0.8767258 ,  1.2390479 ,\n",
       "       -2.1644561 , -1.7202184 ,  0.2000719 , -0.39834404, -0.82180786,\n",
       "       -1.0153419 ,  1.7259516 , -0.7133504 , -0.08586155, -1.1651962 ,\n",
       "        0.71440226,  0.18437243, -0.14942844, -0.19034119, -0.76441634,\n",
       "       -1.6848704 ,  0.05104151,  1.1321046 ,  0.877816  ,  0.69168544,\n",
       "       -0.6275966 , -0.21036308, -1.4760349 , -0.22958693,  0.90460294,\n",
       "       -0.39207903,  1.3171601 , -0.02285626,  0.18919964,  0.47155443,\n",
       "       -0.6295116 ,  0.40089568, -0.56013125,  0.9795822 , -0.12894592,\n",
       "       -0.84072226, -0.9015508 , -0.6210216 ,  0.08601564,  1.1750275 ,\n",
       "        0.37195244, -0.3942386 , -0.01039182, -0.6031794 , -0.2998871 ,\n",
       "        0.6093762 , -0.42507067, -0.4692    ,  1.0628614 ,  0.5808834 ,\n",
       "       -0.6440159 ,  0.2040601 , -0.4954977 ,  1.3672762 , -0.13888709,\n",
       "       -0.5865392 , -0.14323477,  1.8183708 ,  0.10619894, -0.74851996,\n",
       "       -0.05921908,  0.8057511 , -1.1971607 ,  0.02864073,  0.8695075 ,\n",
       "       -1.3317491 ,  0.00936641,  0.458681  ,  0.3227419 , -1.8683472 ,\n",
       "       -0.25723433,  0.4994219 ,  0.53125745, -0.3369279 ,  0.50935024,\n",
       "        0.6114905 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Length of array: ', len(cbow_model[\"love\"]))\n",
    "cbow_model[\"love\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 21:58:42,379 : INFO : saving Word2Vec object under cbow_model.bin, separately None\n",
      "2020-09-22 21:58:42,380 : INFO : not storing attribute vectors_norm\n",
      "2020-09-22 21:58:42,382 : INFO : not storing attribute cum_table\n",
      "2020-09-22 21:58:43,354 : INFO : saved cbow_model.bin\n"
     ]
    }
   ],
   "source": [
    "# saving our cbow model\n",
    "\n",
    "cbow_model.save('cbow_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 21:59:42,248 : INFO : loading Word2Vec object from cbow_model.bin\n",
      "2020-09-22 21:59:42,717 : INFO : loading wv recursively from cbow_model.bin.wv.* with mmap=None\n",
      "2020-09-22 21:59:42,718 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-09-22 21:59:42,719 : INFO : loading vocabulary recursively from cbow_model.bin.vocabulary.* with mmap=None\n",
      "2020-09-22 21:59:42,720 : INFO : loading trainables recursively from cbow_model.bin.trainables.* with mmap=None\n",
      "2020-09-22 21:59:42,721 : INFO : setting ignored attribute cum_table to None\n",
      "2020-09-22 21:59:42,722 : INFO : loaded cbow_model.bin\n"
     ]
    }
   ],
   "source": [
    "# loading our cbow model\n",
    "\n",
    "Model = Word2Vec.load('cbow_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fantastic', 0.7758585214614868),\n",
       " ('wonderful', 0.7226381301879883),\n",
       " ('good', 0.6906130313873291),\n",
       " ('awesome', 0.6826470494270325),\n",
       " ('amazing', 0.6501271724700928),\n",
       " ('fabulous', 0.6395999193191528),\n",
       " ('excellent', 0.6273428201675415),\n",
       " ('fab', 0.6175312995910645),\n",
       " ('nice', 0.5801827907562256),\n",
       " ('amaze', 0.5740725994110107)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.wv.most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature</th>\n",
       "      <th>hash</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1374558</td>\n",
       "      <td>4</td>\n",
       "      <td>with ya quot i'd like a palm pre touchstone ch...</td>\n",
       "      <td>[pre, launch]</td>\n",
       "      <td>[ya, quot, 'd, like, palm, pre, touchstone, ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1389115</td>\n",
       "      <td>4</td>\n",
       "      <td>felt the earthquake this afternoon it seems to...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[felt, earthquake, afternoon, seem, epicenter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1137831</td>\n",
       "      <td>4</td>\n",
       "      <td>ruffles on shirts are like so in me likey</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ruffle, shirt, like, likey]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790714</td>\n",
       "      <td>0</td>\n",
       "      <td>pretty bad night into a crappy morning fml if ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[pretty, bad, night, crappy, morning, fml, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1117911</td>\n",
       "      <td>4</td>\n",
       "      <td>yeah what a clear view</td>\n",
       "      <td>[]</td>\n",
       "      <td>[yeah, clear, view]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259178</td>\n",
       "      <td>0</td>\n",
       "      <td>this song's middle change just doesn't want to...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[song, 's, middle, change, n't, want, born, ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1414414</td>\n",
       "      <td>4</td>\n",
       "      <td>good luck with that</td>\n",
       "      <td>[]</td>\n",
       "      <td>[good, luck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131932</td>\n",
       "      <td>0</td>\n",
       "      <td>i rather average 32370</td>\n",
       "      <td>[]</td>\n",
       "      <td>[rather, average, 32370]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671155</td>\n",
       "      <td>0</td>\n",
       "      <td>pickin up waitin on 2 hurry up i odeeee missed...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[pickin, waitin, 2, hurry, odeeee, miss, dem, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121958</td>\n",
       "      <td>0</td>\n",
       "      <td>home studying for maths wooot im so going to f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[home, study, math, wooot, im, go, fail, shit]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            feature  \\\n",
       "1374558      4  with ya quot i'd like a palm pre touchstone ch...   \n",
       "1389115      4  felt the earthquake this afternoon it seems to...   \n",
       "1137831      4          ruffles on shirts are like so in me likey   \n",
       "790714       0  pretty bad night into a crappy morning fml if ...   \n",
       "1117911      4                             yeah what a clear view   \n",
       "...        ...                                                ...   \n",
       "259178       0  this song's middle change just doesn't want to...   \n",
       "1414414      4                                good luck with that   \n",
       "131932       0                             i rather average 32370   \n",
       "671155       0  pickin up waitin on 2 hurry up i odeeee missed...   \n",
       "121958       0  home studying for maths wooot im so going to f...   \n",
       "\n",
       "                  hash                                             tokens  \n",
       "1374558  [pre, launch]  [ya, quot, 'd, like, palm, pre, touchstone, ch...  \n",
       "1389115             []     [felt, earthquake, afternoon, seem, epicenter]  \n",
       "1137831             []                       [ruffle, shirt, like, likey]  \n",
       "790714              []  [pretty, bad, night, crappy, morning, fml, but...  \n",
       "1117911             []                                [yeah, clear, view]  \n",
       "...                ...                                                ...  \n",
       "259178              []  [song, 's, middle, change, n't, want, born, ar...  \n",
       "1414414             []                                       [good, luck]  \n",
       "131932              []                           [rather, average, 32370]  \n",
       "671155              []  [pickin, waitin, 2, hurry, odeeee, miss, dem, ...  \n",
       "121958              []     [home, study, math, wooot, im, go, fail, shit]  \n",
       "\n",
       "[1280000 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 258269\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Use tokenizer to vectorize our tweets - map each tweet as sequence of integers of where text occurs\n",
    "    EX. tokenizer.fit_on_texts(\"The earth is an awesome place live\")\n",
    "        ->     [[1,2,3,4,5,6,7]] where 3:\"is\" , 6:\"place\", etc.\n",
    "'''\n",
    "\n",
    "# create vocabulary index dictionary - on word freq\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train['feature'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Total words\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374558    with ya quot i'd like a palm pre touchstone ch...\n",
       "1389115    felt the earthquake this afternoon it seems to...\n",
       "1137831            ruffles on shirts are like so in me likey\n",
       "790714     pretty bad night into a crappy morning fml if ...\n",
       "1117911                               yeah what a clear view\n",
       "                                 ...                        \n",
       "259178     this song's middle change just doesn't want to...\n",
       "1414414                                  good luck with that\n",
       "131932                                i rather average 32370\n",
       "671155     pickin up waitin on 2 hurry up i odeeee missed...\n",
       "121958     home studying for maths wooot im so going to f...\n",
       "Name: feature, Length: 1280000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0      0      0 ...    701   1269   2375]\n",
      " [     0      0      0 ...     23      3  32888]\n",
      " [     0      0      0 ...     10     14   5671]\n",
      " ...\n",
      " [     0      0      0 ...    792   4423 258268]\n",
      " [     0      0      0 ...      2     22   1104]\n",
      " [     0      0      0 ...    602     26    383]]\n",
      "Shape:  (1280000, 300)\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# transform tweets from training data to a sequence of integers based on our created dictionary\n",
    "\n",
    "encoded_train = tokenizer.texts_to_sequences(df_train['feature'])\n",
    "encoded_test = tokenizer.texts_to_sequences(df_test['feature'])\n",
    "\n",
    "# pad documents to a consistent array length of 300\n",
    "\n",
    "max_length = 300 # pad all x vectors to be uniform 300 length\n",
    "\n",
    "x_training = pad_sequences(encoded_train, maxlen=max_length, padding='pre') # prepadding adds 0s before sequence up to length 300\n",
    "x_testing = pad_sequences(encoded_test, maxlen=max_length, padding='pre')\n",
    "\n",
    "# x_training is 1280000 x 300 matrix, with 1280000 rows (training samples), each sample padded to a length 300 tokenized vector\n",
    "\n",
    "print(x_training) \n",
    "print('Shape: ', x_training.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedded Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.0113766  -0.01582396  0.08579604 ...  0.48666427  0.15633571\n",
      "   0.31762081]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "Shape:  (258269, 256)\n",
      "Parameters:  66116864\n",
      "Wall time: 225 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "For each word in our features, we first encode it to vector by position through encoding tokenizer,\n",
    "then map it to the word vector we have trained using Model (W2V)\n",
    "\n",
    "Model (W2V cbow):\n",
    "- has 25762 unique words\n",
    "    - from training set (tweet lists that are tokenized, lemmatized, remove stop words)\n",
    "- each word as 256 length array of floats\n",
    "- training words with 'distance' to 256 neighbor words\n",
    "\n",
    "\n",
    "Encoding Tokenizer:\n",
    "- has 258269 words\n",
    "    - from training set (raw string of tweet)\n",
    "- each word as 300 length array of ints  \n",
    "- our training features that are processed for learning\n",
    "\n",
    "Created an embedding matrix that we will use as matrix of weights for our embedding layer, initialized by our W2V model\n",
    "'''\n",
    "\n",
    "# now apply the trained w2v model onto our training set\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, model_size)) # shape: 258269 x 256  \n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in Model.wv.vocab:\n",
    "        embedding_vector = Model.wv[word] # 256 length vector for each word in Model\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "'''\n",
    "embedding matrix:shape:\n",
    "\n",
    "shape: 258269 x 256  \n",
    "\n",
    "array(\n",
    "        [\n",
    "\n",
    "            [Model[1]],\n",
    "            [Model[2]],\n",
    "            ...\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "where\n",
    "Model[2]:\n",
    "\n",
    "array([len(256)]) that represent 256 'distances' to neighbors of 'to' (tokenizer.word_index['to'] = 2)\n",
    "\n",
    "'''\n",
    "\n",
    "print(embedding_matrix)\n",
    "print('Shape: ', embedding_matrix.shape)\n",
    "print('Parameters: ', embedding_matrix.shape[0] * embedding_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Shape:  (1280000, 1)\n",
      "Wall time: 598 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# mapper for sentiment labels\n",
    "\n",
    "sent_map = {0:'Negative', 2: 'Neutral', 4:'Positive'}\n",
    "\n",
    "train_labels = df_train['label'].tolist()\n",
    "test_labels = df_test['label'].tolist()\n",
    "\n",
    "# relabelling our labels using sklearn.preprocessing's labelencoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels)\n",
    "\n",
    "'''\n",
    "Labels will be from 0-1\n",
    "    - {0:'Negative', 1:'Positive'}\n",
    "    - neutral not present in dataset when we trained LabelEncoder()\n",
    "    \n",
    "Labels will be transformed from column of df_train\n",
    "    => np.array where (1280000, 1) for y_training\n",
    "    => basically to retain original shape of 1280000 rows, 1 column in numpy.array form\n",
    "\n",
    "''' \n",
    "\n",
    "y_training = label_encoder.transform(train_labels)\n",
    "y_test = label_encoder.transform(test_labels)\n",
    "\n",
    "y_training = y_training.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "# reshape to training to be (1280000, 1), where we have one column and 1280000 rows to match our 1280000 training sample\n",
    "\n",
    "print(y_training)\n",
    "print('Shape: ', y_training.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, LSTM\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 256)          66116864  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 66,314,113\n",
      "Trainable params: 197,249\n",
      "Non-trainable params: 66,116,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Keras Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "emb_layer = Embedding(vocab_size, model_size, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model.add(emb_layer)\n",
    "model.add(Dropout(0.3))\n",
    "# LSTM cell\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 22:12:13,524 : WARNING : From C:\\Users\\jymas\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# configure model for training\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/20200922-224048\n"
     ]
    }
   ],
   "source": [
    "# setting parameters\n",
    "\n",
    "N_EPOCHS = 8 # iterations to train\n",
    "BATCHES = 1024 # samples/gradient update\n",
    "VAL_SPLIT = 0.1 # set aside 10% for validation before testing\n",
    "\n",
    "LOG_DIR = 'logs/{}'.format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# callback monitoring\n",
    "# possibly include remotemonitor for eventual streaming, modelcheckpoint for saving\n",
    "\n",
    "CALLBACKS = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0), # reduce learning rate once val_loss stops improving\n",
    "            EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5), # stopping training when val_accuracy stops improving\n",
    "           TensorBoard(log_dir=LOG_DIR, histogram_freq=1)]\n",
    "print(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1152000 samples, validate on 128000 samples\n",
      "Epoch 1/8\n",
      "1152000/1152000 [==============================] - 1385s 1ms/step - loss: 0.4679 - accuracy: 0.7754 - val_loss: 0.4457 - val_accuracy: 0.7914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5e93119623a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVAL_SPLIT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                          callbacks=CALLBACKS)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    214\u001b[0m                         \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    287\u001b[0m                     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m                     \u001b[0msummary_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = model.fit(x_training,\n",
    "                          y_training,\n",
    "                          epochs=N_EPOCHS,\n",
    "                          batch_size=BATCHES,\n",
    "                          validation_split=VAL_SPLIT,\n",
    "                          verbose=1,\n",
    "                         callbacks=CALLBACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "n_classes = 2\n",
    "batch_size = 128\n",
    "\n",
    "chunk_size = 256 # should be length of the tweet\n",
    "n_chunks = None\n",
    "\n",
    "rnn_size = 128\n",
    "\n",
    "def recurrent_neural_network(x):\n",
    "    \n",
    "    # creating embedding layer\n",
    "    emb_layer = tf.convert_to_tensor(embedding_matrix, np.float32)\n",
    "    \n",
    "    layer = {'weights':tf.Variable(emb_layer, tf.random_normal([n_classes]))\n",
    "            'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    x = tf.transpose(x, [1,0,2])\n",
    "    x = tf.reshape(x, [-1, chunk_size])\n",
    "    x = tf.split(0, n_chunks, x)\n",
    "    \n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(rnn_size)\n",
    "    \n",
    "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    \n",
    "    # (matmul of final output with weights) + biases\n",
    "    output = tf.matmul(outputs[-1], layer['weights']) + layer['biases']\n",
    "    \n",
    "    return output\n",
    "\n",
    "#def train_neural_network(x):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "neptune": {
   "notebookId": "79056051-d37b-46d1-bdb2-571cbb7ba7bc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
